{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rajab\\documents\\workspace\\pinn_turbulentchannelflow\\.conda\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-win_amd64.whl (2496.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-win_amd64.whl (6.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 pillow-11.0.0 sympy-1.13.1 torch-2.6.0+cu126 torchaudio-2.6.0+cu126 torchvision-0.21.0+cu126\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ED63CE4290>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /whl/cu126/torch/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ED63CE50D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /whl/cu126/torch/\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relying on environment variable.\n",
      "DeepXDE Backend requested: pytorch\n",
      "DeepXDE Backend actual: pytorch\n",
      "CUDA available. Setting default device to CUDA.\n",
      "PyTorch CUDA device detected by DDE: 0\n",
      "PyTorch default dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DDE_BACKEND\"] = \"pytorch\"\n",
    "try:\n",
    "  import torch\n",
    "except ImportError:\n",
    "  %pip install torch -q\n",
    "try:\n",
    "  import deepxde\n",
    "except ImportError:\n",
    "  %pip install deepxde -q\n",
    "try:\n",
    "  import pandas\n",
    "except ImportError:\n",
    "  %pip install pandas -q\n",
    "try:\n",
    "  import matplotlib\n",
    "except ImportError:\n",
    "  %pip install matplotlib -q\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import deepxde as dde\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import re # <<<--- IMPORT REGEX MODULE\n",
    "\n",
    "\n",
    "\n",
    "# <<<--- Attempt to explicitly set backend (ignore error if attribute doesn't exist) ---\n",
    "try:\n",
    "    dde.config.set_default_backend(\"pytorch\")\n",
    "    print(\"Attempted to explicitly set DeepXDE backend to PyTorch.\")\n",
    "except AttributeError:\n",
    "    print(f\"Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relying on environment variable.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not explicitly set backend via dde.config: {e}\")\n",
    "# --- END OF ADDED CODE ---\n",
    "\n",
    "print(f\"DeepXDE Backend requested: {os.environ.get('DDE_BACKEND', 'Not Set')}\") # Use .get for safety\n",
    "# Ensure dde.backend is accessed after potential import\n",
    "if \"deepxde\" in sys.modules:\n",
    "    print(f\"DeepXDE Backend actual: {dde.backend.backend_name}\")\n",
    "\n",
    "    # Set up device (check AFTER backend is confirmed)\n",
    "    if dde.backend.backend_name == \"pytorch\":\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"CUDA available. Setting default device to CUDA.\")\n",
    "            try:\n",
    "                # Let DeepXDE manage device placement where possible\n",
    "                # torch.set_default_device(\"cuda\") # This might conflict with DDE sometimes\n",
    "                # Ensure default dtype is float32 for consistency\n",
    "                torch.set_default_dtype(torch.float32)\n",
    "                # Get the device DDE will use\n",
    "                current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "                print(f\"PyTorch CUDA device detected by DDE: {current_device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error during PyTorch device setup: {e}\")\n",
    "        else:\n",
    "            print(\"CUDA not available. Using CPU.\")\n",
    "            try:\n",
    "                # torch.set_default_device(\"cpu\")\n",
    "                torch.set_default_dtype(torch.float32)\n",
    "                print(f\"PyTorch default device set to: cpu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to set default PyTorch device to CPU: {e}\")\n",
    "\n",
    "        print(f\"PyTorch default dtype: {torch.get_default_dtype()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Backend is '{dde.backend.backend_name}', not PyTorch. Skipping PyTorch device setup.\")\n",
    "else:\n",
    "    print(\"Error: deepxde module not imported correctly.\")\n",
    "\n",
    "\n",
    "class PlotterConfig:\n",
    "    \"\"\"Stores configuration parameters specifically for plotting.\"\"\"\n",
    "    NX_PRED = 200\n",
    "    NY_PRED = 100\n",
    "    CMAP_VELOCITY = 'viridis'\n",
    "    CMAP_PRESSURE = 'coolwarm'\n",
    "    CMAP_TURBULENCE = 'plasma'\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Stores configuration parameters for the simulation.\"\"\"\n",
    "    DRIVE_MOUNT_POINT = '/content/drive'\n",
    "    # Adjust path if necessary\n",
    "    GDRIVE_BASE_FOLDER = 'PINN_RANS_ChannelFlow'\n",
    "    OUTPUT_DIR = GDRIVE_BASE_FOLDER\n",
    "    MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_checkpoints\")\n",
    "    LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
    "    PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "    DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
    "    LOG_FILE = os.path.join(LOG_DIR, \"training_log.log\")\n",
    "    REFERENCE_DATA_FILE = os.path.join(DATA_DIR, \"reference_output_data.csv\")\n",
    "\n",
    "    # <<<--- CHECKPOINT_FILENAME_BASE: Just the base name ---\n",
    "    CHECKPOINT_FILENAME_BASE  = \"rans_channel_wf\"\n",
    "\n",
    "    NU = 0.0002\n",
    "    RHO = 1.0\n",
    "    MU = RHO * NU\n",
    "    U_INLET = 1.0\n",
    "    H = 2.0\n",
    "    CHANNEL_HALF_HEIGHT = H / 2.0\n",
    "    L = 10.0\n",
    "    RE_H = U_INLET * H / NU\n",
    "    EPS_SMALL = 1e-10\n",
    "\n",
    "    CMU = 0.09\n",
    "    CEPS1 = 1.44\n",
    "    CEPS2 = 1.92\n",
    "    SIGMA_K = 1.0\n",
    "    SIGMA_EPS = 1.3\n",
    "\n",
    "    KAPPA = 0.41\n",
    "    E_WALL = 9.8\n",
    "    Y_P = 0.04\n",
    "    RE_TAU_TARGET = 350\n",
    "    U_TAU_TARGET = RE_TAU_TARGET * NU / CHANNEL_HALF_HEIGHT\n",
    "    YP_PLUS_TARGET = Y_P * U_TAU_TARGET / NU\n",
    "    U_TARGET_WF = (U_TAU_TARGET / KAPPA) * np.log(max(E_WALL * YP_PLUS_TARGET, EPS_SMALL))\n",
    "    K_TARGET_WF = U_TAU_TARGET**2 / np.sqrt(CMU)\n",
    "    EPS_TARGET_WF = U_TAU_TARGET**3 / max(KAPPA * Y_P, EPS_SMALL)\n",
    "\n",
    "    TURBULENCE_INTENSITY = 0.05\n",
    "    MIXING_LENGTH_SCALE = 0.07 * CHANNEL_HALF_HEIGHT\n",
    "    K_INLET = 1.5 * (U_INLET * TURBULENCE_INTENSITY)**2\n",
    "    EPS_INLET = (CMU**0.75) * (K_INLET**1.5) / MIXING_LENGTH_SCALE\n",
    "    K_INLET_TRANSFORMED = np.log(max(K_INLET, EPS_SMALL))\n",
    "    EPS_INLET_TRANSFORMED = np.log(max(EPS_INLET, EPS_SMALL))\n",
    "    K_TARGET_WF_TRANSFORMED = np.log(max(K_TARGET_WF, EPS_SMALL))\n",
    "    EPS_TARGET_WF_TRANSFORMED = np.log(max(EPS_TARGET_WF, EPS_SMALL))\n",
    "\n",
    "    GEOM = dde.geometry.Rectangle(xmin=[0, -CHANNEL_HALF_HEIGHT], xmax=[L, CHANNEL_HALF_HEIGHT])\n",
    "\n",
    "    NUM_LAYERS = 8\n",
    "    NUM_NEURONS = 64\n",
    "    ACTIVATION = \"tanh\"\n",
    "    INITIALIZER = \"Glorot normal\"\n",
    "    NETWORK_INPUTS = 2\n",
    "    NETWORK_OUTPUTS = 5\n",
    "\n",
    "    NUM_DOMAIN_POINTS = 20000\n",
    "    NUM_BOUNDARY_POINTS = 4000\n",
    "    NUM_TEST_POINTS = 5000\n",
    "    NUM_WF_POINTS_PER_WALL = 200\n",
    "    LEARNING_RATE_ADAM = 1e-3\n",
    "    ADAM_ITERATIONS = 50000\n",
    "    LBFGS_ITERATIONS = 20000\n",
    "    PDE_WEIGHTS = [1, 1, 1, 1, 1]\n",
    "    BC_WEIGHTS = [10, 10, 10, 10, 10, 10, 10, 20, 20, 20]\n",
    "    LOSS_WEIGHTS = PDE_WEIGHTS + BC_WEIGHTS\n",
    "    SAVE_INTERVAL = 1000 # Checkpoint saving interval\n",
    "    DISPLAY_EVERY = 1000 # Loss display interval\n",
    "\n",
    "\n",
    "# Instantiate config\n",
    "cfg = Config()\n",
    "plotter_cfg = PlotterConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relying on environment variable.\n",
      "DeepXDE Backend requested: pytorch\n",
      "DeepXDE Backend actual: pytorch\n",
      "CUDA available.\n",
      "PyTorch CUDA device detected by DDE: 0 (NVIDIA GeForce RTX 3050 Laptop GPU)\n",
      "PyTorch version: 2.6.0+cu126\n",
      "Number of GPUs: 1\n",
      "PyTorch default dtype: torch.float32\n",
      "2025-04-14 23:38:14 [INFO] Logging configured.\n",
      "2025-04-14 23:38:14 [INFO] ============================================================\n",
      "2025-04-14 23:38:14 [INFO]  PINN RANS k-epsilon Channel Flow Simulation Start \n",
      "2025-04-14 23:38:14 [INFO] ============================================================\n",
      "2025-04-14 23:38:14 [INFO] ==================================================\n",
      "2025-04-14 23:38:14 [INFO] Simulation Configuration:\n",
      "2025-04-14 23:38:14 [INFO]   Output Directory: PINN_RANS_ChannelFlow\n",
      "2025-04-14 23:38:14 [INFO]   Re_H: 10000\n",
      "2025-04-14 23:38:14 [INFO]   Wall Function y_p: 0.04 (Target y+: 14.00)\n",
      "2025-04-14 23:38:14 [INFO]   Network: 8 layers, 64 neurons\n",
      "2025-04-14 23:38:14 [INFO]   Inlet k (log): -5.5860, Inlet eps (log): -7.5257\n",
      "2025-04-14 23:38:14 [INFO]   Target WF U: 0.8402, k (log): -4.1145, eps (log): -3.8673\n",
      "2025-04-14 23:38:14 [INFO]   Adam Iterations: 50000, LR: 0.001\n",
      "2025-04-14 23:38:14 [INFO]   L-BFGS Iterations: 20000\n",
      "2025-04-14 23:38:14 [INFO]   Checkpoint Interval: 1000\n",
      "2025-04-14 23:38:14 [INFO]   Reference Data File (CSV): PINN_RANS_ChannelFlow\\data\\reference_output_data.csv\n",
      "2025-04-14 23:38:14 [INFO] Plotter Configuration:\n",
      "2025-04-14 23:38:14 [INFO]   Prediction Grid Nx: 200, Ny: 100\n",
      "2025-04-14 23:38:14 [INFO] ==================================================\n",
      "2025-04-14 23:38:14 [INFO] Generated 400 anchor points for wall functions.\n",
      "2025-04-14 23:38:15 [INFO] Defined 10 boundary conditions.\n",
      "2025-04-14 23:38:15 [INFO] Using 400 anchor points for wall functions.\n",
      "2025-04-14 23:38:15 [INFO] Building the PINN model...\n",
      "Warning: 5000 points required, but 5088 points sampled.\n",
      "2025-04-14 23:38:15 [INFO] Model built successfully.\n",
      "2025-04-14 23:38:15 [INFO] Checkpoint filename base: PINN_RANS_ChannelFlow\\model_checkpoints\\rans_channel_wf-\n",
      "2025-04-14 23:38:15 [INFO] Searching for checkpoints in: PINN_RANS_ChannelFlow\\model_checkpoints\n",
      "2025-04-14 23:38:15 [INFO] Using pattern: ^rans_channel_wf-(\\d+)\\.pt$\n",
      "2025-04-14 23:38:15 [INFO] Found latest valid checkpoint: PINN_RANS_ChannelFlow\\model_checkpoints\\rans_channel_wf-54000.pt at step 54000\n",
      "2025-04-14 23:38:15 [INFO] Restored step (54000) >= Adam iterations (50000). Will compile with L-BFGS initially.\n",
      "2025-04-14 23:38:15 [INFO] Compiling model with L-BFGS optimizer initially.\n",
      "Compiling model...\n",
      "'compile' took 3.180720 s\n",
      "\n",
      "2025-04-14 23:38:19 [INFO] Model compiled with L-BFGS.\n",
      "2025-04-14 23:38:19 [INFO] Explicitly restoring model state from: PINN_RANS_ChannelFlow\\model_checkpoints\\rans_channel_wf-54000.pt\n",
      "Restoring model from PINN_RANS_ChannelFlow\\model_checkpoints\\rans_channel_wf-54000.pt ...\n",
      "\n",
      "2025-04-14 23:38:19 [INFO] Model state restored. Internal DDE step count *after* restore: 0\n",
      "2025-04-14 23:38:19 [WARNING] Mismatch between expected restored step (54000) and DDE internal step (0) after restore! Forcing DDE step.\n",
      "2025-04-14 23:38:19 [INFO] Manually setting internal step count to the restored step: 54000\n",
      "2025-04-14 23:38:19 [INFO] Internal step count after manual setting: 54000\n",
      "2025-04-14 23:38:19 [INFO] Skipping Adam phase as initial compilation was L-BFGS.\n",
      "2025-04-14 23:38:19 [INFO] Proceeding to L-BFGS phase (current step 54000). Target iterations for this phase: 20000\n",
      "2025-04-14 23:38:19 [INFO] Optimizer is already L-BFGS (likely resumed). No recompilation needed.\n",
      "Training model...\n",
      "\n",
      "Step      Train loss                                                                                                                                                Test loss                                                                                                                                                 Test metric\n",
      "54000     [3.58e-03, 3.35e-03, 1.04e-03, 8.78e-04, 1.07e-03, 4.89e-03, 7.23e-05, 5.67e-04, 7.07e-04, 2.26e-05, 5.99e-03, 1.99e-04, 3.27e-05, 3.59e-04, 2.76e-04]    [2.06e-03, 2.34e-03, 4.10e-04, 1.46e-04, 2.02e-04, 4.89e-03, 7.23e-05, 5.67e-04, 7.07e-04, 2.26e-05, 5.99e-03, 1.99e-04, 3.27e-05, 3.59e-04, 2.76e-04]    []  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1840\u001b[39m\n\u001b[32m   1838\u001b[39m trainer.build_model(bcs, anchor_points)\n\u001b[32m   1839\u001b[39m \u001b[38;5;66;03m# Execute the updated training method\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m model_trained, history_trained, state_trained = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;66;03m# Check if training outputs seem valid\u001b[39;00m\n\u001b[32m   1843\u001b[39m \u001b[38;5;66;03m# Basic check: ensure they are not None\u001b[39;00m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_trained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m history_trained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m state_trained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1845\u001b[39m      \u001b[38;5;66;03m# Could add more checks, e.g., on final loss\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 830\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    826\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel re-compiled with L-BFGS. Step count after compile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model.train_state.step\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.model.train_state\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# Use the same CUSTOM checkpointer instance for L-BFGS saves\u001b[39;00m\n\u001b[32m    829\u001b[39m \u001b[38;5;66;03m# The iterations here are L-BFGS specific iterations, not added to the global step counter in the same way Adam does.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28mself\u001b[39m.losshistory, \u001b[38;5;28mself\u001b[39m.train_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlbfgs_iters_to_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Let L-BFGS run its course\u001b[39;49;00m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDISPLAY_EVERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Can use display_every for LBFGS too\u001b[39;49;00m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustom_checkpointer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass the custom callback instance\u001b[39;49;00m\n\u001b[32m    834\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m lbfgs_time = time.time() - lbfgs_start_time\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.losshistory \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.losshistory, \u001b[33m'\u001b[39m\u001b[33mloss_train\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.losshistory.loss_train:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\deepxde\\utils\\internal.py:22\u001b[39m, in \u001b[36mtiming.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     21\u001b[39m     ts = timeit.default_timer()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     te = timeit.default_timer()\n\u001b[32m     24\u001b[39m     verbose = kwargs.get(\u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\deepxde\\model.py:682\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs, verbose)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_name == \u001b[33m\"\u001b[39m\u001b[33mL-BFGS\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_pytorch_lbfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_name == \u001b[33m\"\u001b[39m\u001b[33mNNCG\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    684\u001b[39m         \u001b[38;5;28mself\u001b[39m._train_sgd(iterations, display_every, verbose=verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\deepxde\\model.py:807\u001b[39m, in \u001b[36mModel._train_pytorch_lbfgs\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks.on_batch_begin()\n\u001b[32m    804\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state.set_data_train(\n\u001b[32m    805\u001b[39m     *\u001b[38;5;28mself\u001b[39m.data.train_next_batch(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    806\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    813\u001b[39m n_iter = \u001b[38;5;28mself\u001b[39m.opt.state_dict()[\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mn_iter\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prev_n_iter == n_iter - \u001b[32m1\u001b[39m:\n\u001b[32m    815\u001b[39m     \u001b[38;5;66;03m# Converged\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\deepxde\\model.py:595\u001b[39m, in \u001b[36mModel._train_step\u001b[39m\u001b[34m(self, inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_step(inputs, targets, auxiliary_vars)\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state = \u001b[38;5;28mself\u001b[39m.train_step(\n\u001b[32m    599\u001b[39m         \u001b[38;5;28mself\u001b[39m.params, \u001b[38;5;28mself\u001b[39m.opt_state, inputs, targets\n\u001b[32m    600\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\deepxde\\model.py:377\u001b[39m, in \u001b[36mModel._compile_pytorch.<locals>.train_step\u001b[39m\u001b[34m(inputs, targets, auxiliary_vars)\u001b[39m\n\u001b[32m    374\u001b[39m     total_loss.backward()\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    379\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\optim\\lbfgs.py:444\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m    442\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._directional_evaluate(closure, x, t, d)\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     loss, flat_grad, t, ls_func_evals = \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m    448\u001b[39m opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\optim\\lbfgs.py:48\u001b[39m, in \u001b[36m_strong_wolfe\u001b[39m\u001b[34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[39m\n\u001b[32m     46\u001b[39m g = g.clone(memory_format=torch.contiguous_format)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m f_new, g_new = \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m ls_func_evals = \u001b[32m1\u001b[39m\n\u001b[32m     50\u001b[39m gtd_new = g_new.dot(d)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\optim\\lbfgs.py:442\u001b[39m, in \u001b[36mLBFGS.step.<locals>.obj_func\u001b[39m\u001b[34m(x, t, d)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj_func\u001b[39m(x, t, d):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\optim\\lbfgs.py:296\u001b[39m, in \u001b[36mLBFGS._directional_evaluate\u001b[39m\u001b[34m(self, closure, x, t, d)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_grad(t, d)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     loss = \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[32m    297\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_param(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajab\\Documents\\workspace\\PINN_TurbulentChannelFlow\\.conda\\Lib\\site-packages\\torch\\utils\\_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DDE_BACKEND\"] = \"pytorch\"\n",
    "try:\n",
    "  import torch\n",
    "except ImportError:\n",
    "  # If running locally and don't have torch, install it:\n",
    "  # pip install torch\n",
    "  print(\"PyTorch not found. Please install it.\")\n",
    "  exit()\n",
    "try:\n",
    "  import deepxde\n",
    "except ImportError:\n",
    "  # If running locally and don't have deepxde, install it:\n",
    "  # pip install deepxde\n",
    "  print(\"DeepXDE not found. Please install it.\")\n",
    "  exit()\n",
    "try:\n",
    "  import pandas\n",
    "except ImportError:\n",
    "  # If running locally and don't have pandas, install it:\n",
    "  # pip install pandas\n",
    "  print(\"Pandas not found. Please install it.\")\n",
    "  exit()\n",
    "try:\n",
    "  import matplotlib\n",
    "except ImportError:\n",
    "  # If running locally and don't have matplotlib, install it:\n",
    "  # pip install matplotlib\n",
    "  print(\"Matplotlib not found. Please install it.\")\n",
    "  exit()\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import deepxde as dde\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import re # Regex module for checkpoint parsing\n",
    "\n",
    "\n",
    "# --- Attempt to explicitly set backend ---\n",
    "try:\n",
    "    dde.config.set_default_backend(\"pytorch\")\n",
    "    print(\"Attempted to explicitly set DeepXDE backend to PyTorch.\")\n",
    "except AttributeError:\n",
    "    print(f\"Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relying on environment variable.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not explicitly set backend via dde.config: {e}\")\n",
    "\n",
    "print(f\"DeepXDE Backend requested: {os.environ.get('DDE_BACKEND', 'Not Set')}\")\n",
    "\n",
    "# --- Check actual backend and setup device/dtype ---\n",
    "if \"deepxde\" in sys.modules:\n",
    "    print(f\"DeepXDE Backend actual: {dde.backend.backend_name}\")\n",
    "    if dde.backend.backend_name == \"pytorch\":\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"CUDA available.\")\n",
    "            try:\n",
    "                torch.set_default_dtype(torch.float32)\n",
    "                current_device = torch.cuda.current_device()\n",
    "                print(f\"PyTorch CUDA device detected by DDE: {current_device} ({torch.cuda.get_device_name(current_device)})\")\n",
    "                print(f\"PyTorch version: {torch.__version__}\")\n",
    "                print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error during PyTorch device setup: {e}\")\n",
    "        else:\n",
    "            print(\"CUDA not available. Using CPU.\")\n",
    "            try:\n",
    "                torch.set_default_dtype(torch.float32)\n",
    "                print(f\"PyTorch default device set to: cpu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to set default PyTorch device to CPU: {e}\")\n",
    "        print(f\"PyTorch default dtype: {torch.get_default_dtype()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Backend is '{dde.backend.backend_name}', not PyTorch. PyTorch-specific device setup skipped.\")\n",
    "else:\n",
    "    print(\"Error: deepxde module not imported correctly.\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ===== Configuration Classes =====\n",
    "# =============================\n",
    "\n",
    "class PlotterConfig:\n",
    "    \"\"\"Stores configuration parameters specifically for plotting.\"\"\"\n",
    "    NX_PRED = 200\n",
    "    NY_PRED = 100\n",
    "    CMAP_VELOCITY = 'viridis'\n",
    "    CMAP_PRESSURE = 'coolwarm'\n",
    "    CMAP_TURBULENCE = 'plasma'\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Stores configuration parameters for the simulation.\"\"\"\n",
    "    DRIVE_MOUNT_POINT = '/content/drive' # Specific to Google Colab\n",
    "    GDRIVE_BASE_FOLDER = 'PINN_RANS_ChannelFlow' # Base folder name on Drive or local\n",
    "    OUTPUT_DIR = GDRIVE_BASE_FOLDER # Default output dir (can be overwritten if not on Colab)\n",
    "    MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_checkpoints\")\n",
    "    LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
    "    PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "    DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
    "    LOG_FILE = os.path.join(LOG_DIR, \"training_log.log\")\n",
    "    REFERENCE_DATA_FILE = os.path.join(DATA_DIR, \"reference_output_data.csv\")\n",
    "\n",
    "    # Checkpoint filename base (without step number or extension)\n",
    "    CHECKPOINT_FILENAME_BASE  = \"rans_channel_wf\"\n",
    "\n",
    "    # --- Fluid and Geometry Parameters ---\n",
    "    NU = 0.0002 # Kinematic viscosity\n",
    "    RHO = 1.0 # Density (often set to 1 for incompressible flow)\n",
    "    MU = RHO * NU # Dynamic viscosity\n",
    "    U_INLET = 1.0 # Inlet velocity\n",
    "    H = 2.0 # Full channel height\n",
    "    CHANNEL_HALF_HEIGHT = H / 2.0\n",
    "    L = 10.0 # Channel length\n",
    "    RE_H = U_INLET * H / NU # Reynolds number based on full height\n",
    "    EPS_SMALL = 1e-10 # Small epsilon for numerical stability (avoid log(0), division by zero)\n",
    "\n",
    "    # --- k-epsilon Model Constants ---\n",
    "    CMU = 0.09\n",
    "    CEPS1 = 1.44\n",
    "    CEPS2 = 1.92\n",
    "    SIGMA_K = 1.0\n",
    "    SIGMA_EPS = 1.3\n",
    "    KAPPA = 0.41 # Von Karman constant\n",
    "\n",
    "    # --- Wall Function Parameters ---\n",
    "    E_WALL = 9.8 # Log-law constant for smooth walls\n",
    "    Y_P = 0.04 # Distance from wall for applying wall functions (y_p)\n",
    "    # Target values for deriving wall function BCs (can be based on desired Re_tau)\n",
    "    RE_TAU_TARGET = 350 # Target friction Reynolds number\n",
    "    U_TAU_TARGET = RE_TAU_TARGET * NU / CHANNEL_HALF_HEIGHT # Target friction velocity\n",
    "    YP_PLUS_TARGET = Y_P * U_TAU_TARGET / NU # Target y+ at y_p\n",
    "    # Target values at y_p based on log-law and turbulence equilibrium\n",
    "    U_TARGET_WF = (U_TAU_TARGET / KAPPA) * np.log(max(E_WALL * YP_PLUS_TARGET, EPS_SMALL))\n",
    "    K_TARGET_WF = U_TAU_TARGET**2 / np.sqrt(CMU)\n",
    "    EPS_TARGET_WF = U_TAU_TARGET**3 / max(KAPPA * Y_P, EPS_SMALL)\n",
    "\n",
    "    # --- Inlet Turbulence Parameters ---\n",
    "    TURBULENCE_INTENSITY = 0.05 # Typical value for channel flow inlet\n",
    "    MIXING_LENGTH_SCALE = 0.07 * CHANNEL_HALF_HEIGHT # Estimate based on boundary layer thickness\n",
    "    # Inlet k and epsilon based on intensity and length scale\n",
    "    K_INLET = 1.5 * (U_INLET * TURBULENCE_INTENSITY)**2\n",
    "    EPS_INLET = (CMU**0.75) * (K_INLET**1.5) / MIXING_LENGTH_SCALE\n",
    "    # Transformed values (log) for network prediction/BCs\n",
    "    K_INLET_TRANSFORMED = np.log(max(K_INLET, EPS_SMALL))\n",
    "    EPS_INLET_TRANSFORMED = np.log(max(EPS_INLET, EPS_SMALL))\n",
    "    K_TARGET_WF_TRANSFORMED = np.log(max(K_TARGET_WF, EPS_SMALL))\n",
    "    EPS_TARGET_WF_TRANSFORMED = np.log(max(EPS_TARGET_WF, EPS_SMALL))\n",
    "\n",
    "    # --- Domain Geometry ---\n",
    "    GEOM = dde.geometry.Rectangle(xmin=[0, -CHANNEL_HALF_HEIGHT], xmax=[L, CHANNEL_HALF_HEIGHT])\n",
    "\n",
    "    # --- Network Architecture ---\n",
    "    NUM_LAYERS = 8\n",
    "    NUM_NEURONS = 64\n",
    "    ACTIVATION = \"tanh\"\n",
    "    INITIALIZER = \"Glorot normal\"\n",
    "    NETWORK_INPUTS = 2 # x, y\n",
    "    NETWORK_OUTPUTS = 5 # u, v, p', log(k), log(eps)\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    NUM_DOMAIN_POINTS = 20000\n",
    "    NUM_BOUNDARY_POINTS = 4000 # For physical boundaries (inlet, outlet, walls)\n",
    "    NUM_TEST_POINTS = 5000 # For evaluating PDE residuals during training/testing\n",
    "    NUM_WF_POINTS_PER_WALL = 200 # Anchor points for wall function BCs (per wall)\n",
    "    LEARNING_RATE_ADAM = 1e-3\n",
    "    ADAM_ITERATIONS = 50000 # Number of iterations for Adam optimizer\n",
    "    LBFGS_ITERATIONS = 20000 # Max iterations for L-BFGS optimizer\n",
    "    # Loss weights: [PDE_cont, PDE_mom_x, PDE_mom_y, PDE_k, PDE_eps, BC_u_in, BC_v_in, BC_k_in, BC_eps_in, BC_p_out, BC_u_wall, BC_v_wall, BC_u_wf, BC_k_wf, BC_eps_wf]\n",
    "    PDE_WEIGHTS = [1, 1, 1, 1, 1] # Weights for the 5 PDE residuals\n",
    "    BC_WEIGHTS = [10, 10, 10, 10, 10, 10, 10, 20, 20, 20] # Weights for the 10 BCs (adjust as needed)\n",
    "    LOSS_WEIGHTS = PDE_WEIGHTS + BC_WEIGHTS\n",
    "    SAVE_INTERVAL = 1000 # Checkpoint saving interval (steps)\n",
    "    DISPLAY_EVERY = 1000 # Loss display interval (steps)\n",
    "\n",
    "\n",
    "# Instantiate config objects\n",
    "cfg = Config()\n",
    "plotter_cfg = PlotterConfig()\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# ===== Custom Checkpoint Callback Class ========\n",
    "# ==============================================\n",
    "class CustomModelCheckpoint(dde.callbacks.Callback):\n",
    "    \"\"\"Custom checkpoint callback that saves based on global step.\"\"\"\n",
    "    def __init__(self, filepath_base, period, verbose=1):\n",
    "        super().__init__()\n",
    "        self.filepath_base = filepath_base # e.g., /path/to/model_checkpoints/rans_channel_wf-\n",
    "        self.period = period\n",
    "        self.verbose = verbose\n",
    "        self._saved_steps = set() # Tracks steps saved in the current trainer.train() call\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Check step at the end of each epoch.\"\"\"\n",
    "        self._save_checkpoint()\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"Internal method to check step and save.\"\"\"\n",
    "        if not hasattr(self, 'model') or not self.model or not hasattr(self.model, 'train_state') or not self.model.train_state:\n",
    "            return # Model not ready\n",
    "\n",
    "        if dde.backend.backend_name != \"pytorch\":\n",
    "             logging.warning(\"CustomModelCheckpoint requires PyTorch backend for model.save behavior.\")\n",
    "             return\n",
    "\n",
    "        step = self.model.train_state.step\n",
    "\n",
    "        if step > 0 and step % self.period == 0 and step not in self._saved_steps:\n",
    "            filepath = f\"{self.filepath_base}{step}.pt\" # Construct filename using global step\n",
    "            if self.verbose > 0:\n",
    "                logging.info(f\"Step {step}: saving model to {filepath} ...\")\n",
    "            try:\n",
    "                # Use DeepXDE's save method which handles backend specifics\n",
    "                self.model.save(filepath, verbose=0)\n",
    "                self._saved_steps.add(step) # Mark this step as saved for this run\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving checkpoint at step {step} to {filepath}: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================\n",
    "# ===== END: Custom Checkpoint Callback ========\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ===== Utility Functions =====\n",
    "# ==========================\n",
    "def setup_logging(log_file):\n",
    "    \"\"\"Configures logging to file and console.\"\"\"\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    ensure_dir(log_dir)\n",
    "    root_logger = logging.getLogger()\n",
    "    # Clear existing handlers to avoid duplicate messages\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "    log_formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    root_logger.setLevel(logging.INFO) # Set root logger level\n",
    "\n",
    "    # File handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(log_formatter)\n",
    "    root_logger.addHandler(file_handler)\n",
    "\n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    logging.info(\"Logging configured.\")\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    \"\"\"Creates a directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        logging.info(f\"Created directory: {directory}\")\n",
    "\n",
    "def mount_drive(mount_point):\n",
    "    \"\"\"Mounts Google Drive if running in Colab.\"\"\"\n",
    "    if 'google.colab' in sys.modules:\n",
    "        if not os.path.exists(os.path.join(mount_point, 'MyDrive')):\n",
    "            try:\n",
    "                from google.colab import drive\n",
    "                logging.info(f\"Mounting Google Drive at {mount_point}...\")\n",
    "                drive.mount(mount_point, force_remount=True)\n",
    "                logging.info(\"Google Drive mounted successfully.\")\n",
    "                # Verify base folder access after mount\n",
    "                gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER)\n",
    "                cfg.OUTPUT_DIR = gdrive_output_path # IMPORTANT: Update config path\n",
    "                cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
    "                cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
    "                cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
    "                cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
    "                cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
    "                cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
    "                logging.info(f\"Output paths updated to Google Drive: {cfg.OUTPUT_DIR}\")\n",
    "                ensure_dir(cfg.OUTPUT_DIR)\n",
    "                if os.path.exists(cfg.OUTPUT_DIR):\n",
    "                    logging.info(f\"Base folder exists: {cfg.OUTPUT_DIR}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Configured base folder NOT found after mount: {cfg.OUTPUT_DIR}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error mounting Google Drive or accessing path: {e}\")\n",
    "                # Fallback to local directory if mount fails\n",
    "                logging.warning(\"Falling back to local directory structure.\")\n",
    "                cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER # Use base folder name locally\n",
    "                cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
    "                cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
    "                cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
    "                cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
    "                cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
    "                cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
    "        else:\n",
    "            logging.info(\"Google Drive already mounted.\")\n",
    "            # Still update paths if already mounted\n",
    "            gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER)\n",
    "            cfg.OUTPUT_DIR = gdrive_output_path # IMPORTANT: Update config path\n",
    "            cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
    "            cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
    "            cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
    "            cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
    "            cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
    "            cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
    "            logging.info(f\"Output paths point to Google Drive: {cfg.OUTPUT_DIR}\")\n",
    "    else:\n",
    "        logging.info(\"Not running in Google Colab. Using local directory structure.\")\n",
    "        # Ensure local paths are based on the script location or CWD\n",
    "        cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER # Use base folder name locally\n",
    "        cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
    "        cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
    "        cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
    "        cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
    "        cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
    "        cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
    "\n",
    "\n",
    "def setup_output_directories(config):\n",
    "    \"\"\"Creates all necessary output directories.\"\"\"\n",
    "    logging.info(\"Setting up output directories...\")\n",
    "    ensure_dir(config.OUTPUT_DIR)\n",
    "    ensure_dir(config.MODEL_DIR)\n",
    "    ensure_dir(config.LOG_DIR)\n",
    "    ensure_dir(config.PLOT_DIR)\n",
    "    ensure_dir(config.DATA_DIR)\n",
    "    logging.info(\"Output directories verified/created.\")\n",
    "\n",
    "def log_configuration(config, plotter_config):\n",
    "    \"\"\"Logs the simulation and plotter configuration.\"\"\"\n",
    "    logging.info(\"=\" * 50)\n",
    "    logging.info(\"Simulation Configuration:\")\n",
    "    logging.info(f\"  Output Directory: {config.OUTPUT_DIR}\")\n",
    "    logging.info(f\"  Re_H: {config.RE_H:.0f}\")\n",
    "    logging.info(f\"  Wall Function y_p: {config.Y_P} (Target y+: {config.YP_PLUS_TARGET:.2f})\")\n",
    "    logging.info(f\"  Network: {config.NUM_LAYERS} layers, {config.NUM_NEURONS} neurons\")\n",
    "    logging.info(f\"  Inlet k (log): {config.K_INLET_TRANSFORMED:.4f}, Inlet eps (log): {config.EPS_INLET_TRANSFORMED:.4f}\")\n",
    "    logging.info(f\"  Target WF U: {config.U_TARGET_WF:.4f}, k (log): {config.K_TARGET_WF_TRANSFORMED:.4f}, eps (log): {config.EPS_TARGET_WF_TRANSFORMED:.4f}\")\n",
    "    logging.info(f\"  Adam Iterations: {config.ADAM_ITERATIONS}, LR: {config.LEARNING_RATE_ADAM}\")\n",
    "    logging.info(f\"  L-BFGS Iterations: {config.LBFGS_ITERATIONS}\")\n",
    "    logging.info(f\"  Checkpoint Interval: {config.SAVE_INTERVAL}\")\n",
    "    logging.info(f\"  Reference Data File (CSV): {config.REFERENCE_DATA_FILE}\")\n",
    "    logging.info(\"Plotter Configuration:\")\n",
    "    logging.info(f\"  Prediction Grid Nx: {plotter_config.NX_PRED}, Ny: {plotter_config.NY_PRED}\")\n",
    "    logging.info(\"=\" * 50)\n",
    "# --- End Utility Functions ---\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ===== PDE System Definition =====\n",
    "# ===============================\n",
    "def pde(x, y, config): # Pass config explicitly\n",
    "    \"\"\"Defines the RANS k-epsilon PDE system.\"\"\"\n",
    "    if dde.backend.backend_name != \"pytorch\":\n",
    "        logging.warning(\"PDE function relies on PyTorch autograd for transformations. Backend mismatch.\")\n",
    "\n",
    "    nu = config.NU; Cmu = config.CMU; Ceps1 = config.CEPS1; Ceps2 = config.CEPS2\n",
    "    sigma_k = config.SIGMA_K; sigma_eps = config.SIGMA_EPS; eps_small = config.EPS_SMALL\n",
    "\n",
    "    # Network outputs: u, v, p', log(k), log(eps)\n",
    "    u, v, p_prime, k_raw, eps_raw = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4], y[:, 4:5]\n",
    "\n",
    "    # --- Apply inverse transformation and enforce positivity ---\n",
    "    # Add eps_small *before* exp for k_raw, eps_raw if they can be very negative\n",
    "    # k = dde.backend.exp(k_raw + eps_small) # Alternative if log(k) can be very negative\n",
    "    k = dde.backend.exp(k_raw) + eps_small\n",
    "    eps = dde.backend.exp(eps_raw) + eps_small\n",
    "\n",
    "    # --- Calculate Gradients using PyTorch Autograd ---\n",
    "    # Use autograd for reliability with transformed variables k, eps\n",
    "    try:\n",
    "        # Gradients of primitive variables (u, v, p') directly from network output 'y'\n",
    "        u_x = dde.grad.jacobian(y, x, i=0, j=0); u_y = dde.grad.jacobian(y, x, i=0, j=1)\n",
    "        v_x = dde.grad.jacobian(y, x, i=1, j=0); v_y = dde.grad.jacobian(y, x, i=1, j=1)\n",
    "        p_prime_x = dde.grad.jacobian(y, x, i=2, j=0); p_prime_y = dde.grad.jacobian(y, x, i=2, j=1)\n",
    "\n",
    "        u_xx = dde.grad.hessian(y, x, component=0, i=0, j=0); u_yy = dde.grad.hessian(y, x, component=0, i=1, j=1)\n",
    "        v_xx = dde.grad.hessian(y, x, component=1, i=0, j=0); v_yy = dde.grad.hessian(y, x, component=1, i=1, j=1)\n",
    "        # Mixed derivatives (needed for momentum diffusion terms)\n",
    "        u_xy = dde.grad.hessian(y, x, component=0, i=0, j=1); v_xy = dde.grad.hessian(y, x, component=1, i=0, j=1) # or j=0, i=1\n",
    "\n",
    "        # Gradients of transformed k, eps using torch.autograd\n",
    "        # Ensure x requires grad if DeepXDE doesn't handle it automatically in this context\n",
    "        if isinstance(x, torch.Tensor) and not x.requires_grad:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        grad_k = torch.autograd.grad(k, x, grad_outputs=torch.ones_like(k), create_graph=True)[0]\n",
    "        k_x, k_y = grad_k[:, 0:1], grad_k[:, 1:2]\n",
    "        grad_eps = torch.autograd.grad(eps, x, grad_outputs=torch.ones_like(eps), create_graph=True)[0]\n",
    "        eps_x, eps_y = grad_eps[:, 0:1], grad_eps[:, 1:2]\n",
    "\n",
    "        # Hessians of transformed k, eps using torch.autograd\n",
    "        grad_kx = torch.autograd.grad(k_x, x, grad_outputs=torch.ones_like(k_x), create_graph=True)[0]\n",
    "        k_xx = grad_kx[:, 0:1]\n",
    "        grad_ky = torch.autograd.grad(k_y, x, grad_outputs=torch.ones_like(k_y), create_graph=True)[0]\n",
    "        k_yy = grad_ky[:, 1:2]\n",
    "\n",
    "        grad_epsx = torch.autograd.grad(eps_x, x, grad_outputs=torch.ones_like(eps_x), create_graph=True)[0]\n",
    "        eps_xx = grad_epsx[:, 0:1]\n",
    "        grad_epsy = torch.autograd.grad(eps_y, x, grad_outputs=torch.ones_like(eps_y), create_graph=True)[0]\n",
    "        eps_yy = grad_epsy[:, 1:2]\n",
    "\n",
    "    except Exception as grad_e:\n",
    "        logging.error(f\"Error calculating gradients in PDE function: {grad_e}\", exc_info=True)\n",
    "        # Return tensors of zeros with the correct shape and device\n",
    "        zero_tensor = torch.zeros_like(y[:, 0:1])\n",
    "        return [zero_tensor] * 5 # Match the number of expected PDE residual outputs\n",
    "\n",
    "    # --- Turbulent Viscosity ---\n",
    "    # Use the transformed k, eps which are guaranteed positive\n",
    "    k_safe = k\n",
    "    eps_safe = eps # eps already has eps_small added\n",
    "    # Add eps_small to denominator for extra safety, although eps_safe should be positive\n",
    "    nu_t = Cmu * dde.backend.square(k_safe) / (eps_safe + eps_small) # Eq: nu_t = Cmu * k^2 / eps\n",
    "    nu_eff = nu + nu_t # Effective viscosity\n",
    "\n",
    "    # --- Gradients of nu_eff (Needed for diffusion terms in momentum eqns) ---\n",
    "    # Using chain rule: d(nu_eff)/dx = d(nu_t)/dk * dk/dx + d(nu_t)/deps * deps/dx\n",
    "    dnut_dk = 2.0 * Cmu * k_safe / (eps_safe + eps_small)\n",
    "    dnut_deps = -Cmu * dde.backend.square(k_safe) / dde.backend.square(eps_safe + eps_small)\n",
    "\n",
    "    nu_eff_x = dnut_dk * k_x + dnut_deps * eps_x\n",
    "    nu_eff_y = dnut_dk * k_y + dnut_deps * eps_y\n",
    "\n",
    "    # --- RANS Equation Residuals ---\n",
    "\n",
    "    # 1. Continuity Equation: d(u)/dx + d(v)/dy = 0\n",
    "    eq_continuity = u_x + v_y\n",
    "\n",
    "    # 2. X-Momentum Equation:\n",
    "    # d(u)/dt + u*du/dx + v*du/dy = -dp'/dx + d/dx[nu_eff * (2*du/dx)] + d/dy[nu_eff * (du/dy + dv/dx)]\n",
    "    # Steady state: u*du/dx + v*du/dy + dp'/dx - [d/dx(...) + d/dy(...)] = 0\n",
    "    adv_u = u * u_x + v * u_y # Advection\n",
    "    # Diffusion terms (expanded using product rule)\n",
    "    # d/dx[nu_eff * (2*du/dx)] = d(nu_eff)/dx * (2*du/dx) + nu_eff * (2*d^2u/dx^2)\n",
    "    diff_u_term1 = nu_eff_x * (2 * u_x) + nu_eff * (2 * u_xx)\n",
    "    # d/dy[nu_eff * (du/dy + dv/dx)] = d(nu_eff)/dy * (du/dy + dv/dx) + nu_eff * (d^2u/dy^2 + d^2v/dxdy)\n",
    "    diff_u_term2 = nu_eff_y * (u_y + v_x) + nu_eff * (u_yy + v_xy) # Assuming v_xy = d^2v/dxdy\n",
    "    eq_mom_x = adv_u + p_prime_x - (diff_u_term1 + diff_u_term2)\n",
    "\n",
    "    # 3. Y-Momentum Equation:\n",
    "    # d(v)/dt + u*dv/dx + v*dv/dy = -dp'/dy + d/dx[nu_eff * (dv/dx + du/dy)] + d/dy[nu_eff * (2*dv/dy)]\n",
    "    # Steady state: u*dv/dx + v*dv/dy + dp'/dy - [d/dx(...) + d/dy(...)] = 0\n",
    "    adv_v = u * v_x + v * v_y # Advection\n",
    "    # Diffusion terms (expanded)\n",
    "    # d/dx[nu_eff * (dv/dx + du/dy)] = d(nu_eff)/dx * (dv/dx + du/dy) + nu_eff * (d^2v/dx^2 + d^2u/dxdy)\n",
    "    diff_v_term1 = nu_eff_x * (v_x + u_y) + nu_eff * (v_xx + u_xy) # Assuming u_xy = d^2u/dxdy\n",
    "    # d/dy[nu_eff * (2*dv/dy)] = d(nu_eff)/dy * (2*dv/dy) + nu_eff * (2*d^2v/dy^2)\n",
    "    diff_v_term2 = nu_eff_y * (2 * v_y) + nu_eff * (2 * v_yy)\n",
    "    eq_mom_y = adv_v + p_prime_y - (diff_v_term1 + diff_v_term2)\n",
    "\n",
    "    # --- Turbulence Model Equations ---\n",
    "\n",
    "    # Production term P_k = nu_t * S^2, where S is the modulus of the mean strain rate tensor\n",
    "    # S^2 = 2*(du/dx)^2 + 2*(dv/dy)^2 + (du/dy + dv/dx)^2  (for 2D)\n",
    "    S_squared = 2 * (dde.backend.square(u_x) + dde.backend.square(v_y)) + dde.backend.square(u_y + v_x)\n",
    "    P_k = nu_t * S_squared\n",
    "\n",
    "    # 4. k-Equation:\n",
    "    # d(k)/dt + u*dk/dx + v*dk/dy = d/dx[(nu + nu_t/sigma_k)*dk/dx] + d/dy[(nu + nu_t/sigma_k)*dk/dy] + P_k - eps\n",
    "    # Steady state: u*dk/dx + v*dk/dy - [Diffusion] - P_k + eps = 0\n",
    "    adv_k = u * k_x + v * k_y # Advection\n",
    "    # Diffusion term: div[ (nu + nu_t/sigma_k) * grad(k) ]\n",
    "    diffusivity_k = nu + nu_t / sigma_k\n",
    "    # Gradient of diffusivity: d(diff_k)/dx = (1/sigma_k) * d(nu_t)/dx\n",
    "    d_diffk_dx = (1 / sigma_k) * nu_eff_x # nu_eff_x contains gradients of nu_t\n",
    "    d_diffk_dy = (1 / sigma_k) * nu_eff_y # nu_eff_y contains gradients of nu_t\n",
    "    laplacian_k = k_xx + k_yy\n",
    "    # Expand divergence using product rule: div(D*grad(k)) = grad(D).grad(k) + D*laplacian(k)\n",
    "    diffusion_k = d_diffk_dx * k_x + d_diffk_dy * k_y + diffusivity_k * laplacian_k\n",
    "    # Use eps_safe which is guaranteed positive\n",
    "    eq_k = adv_k - diffusion_k - P_k + eps_safe\n",
    "\n",
    "    # 5. -Equation:\n",
    "    # d(eps)/dt + u*deps/dx + v*deps/dy = d/dx[(nu + nu_t/sigma_eps)*deps/dx] + d/dy[(nu + nu_t/sigma_eps)*deps/dy] + Ceps1*(eps/k)*P_k - Ceps2*(eps^2/k)\n",
    "    # Steady state: u*deps/dx + v*deps/dy - [Diffusion] - Source + Sink = 0\n",
    "    adv_eps = u * eps_x + v * eps_y # Advection\n",
    "    # Diffusion term: div[ (nu + nu_t/sigma_eps) * grad(eps) ]\n",
    "    diffusivity_eps = nu + nu_t / sigma_eps\n",
    "    d_diffeps_dx = (1 / sigma_eps) * nu_eff_x # Gradients of nu_t\n",
    "    d_diffeps_dy = (1 / sigma_eps) * nu_eff_y\n",
    "    laplacian_eps = eps_xx + eps_yy\n",
    "    diffusion_eps = d_diffeps_dx * eps_x + d_diffeps_dy * eps_y + diffusivity_eps * laplacian_eps\n",
    "    # Source/Sink terms (use safe, positive k and eps)\n",
    "    # Add eps_small to denominators for robustness, even if k_safe/eps_safe have it\n",
    "    source_eps = Ceps1 * (eps_safe / (k_safe + eps_small)) * P_k\n",
    "    sink_eps = Ceps2 * (dde.backend.square(eps_safe) / (k_safe + eps_small))\n",
    "    eq_eps = adv_eps - diffusion_eps - source_eps + sink_eps\n",
    "\n",
    "    # Return the residuals of the 5 equations\n",
    "    return [eq_continuity, eq_mom_x, eq_mom_y, eq_k, eq_eps]\n",
    "# --- End PDE function ---\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ===== Boundary Conditions =====\n",
    "# =============================\n",
    "def get_boundary_conditions(config):\n",
    "    \"\"\"Defines all boundary conditions for the channel flow problem.\"\"\"\n",
    "    geom = config.GEOM\n",
    "    h = config.CHANNEL_HALF_HEIGHT\n",
    "    L = config.L\n",
    "    y_p = config.Y_P # Wall function distance\n",
    "    n_wf_points = config.NUM_WF_POINTS_PER_WALL\n",
    "\n",
    "    # --- Boundary Definition Functions ---\n",
    "    def boundary_inlet(x, on_boundary):\n",
    "        return on_boundary and np.isclose(x[0], 0)\n",
    "\n",
    "    def boundary_outlet(x, on_boundary):\n",
    "        return on_boundary and np.isclose(x[0], L)\n",
    "\n",
    "    def boundary_bottom_wall_physical(x, on_boundary):\n",
    "        # Physical wall at y = -h\n",
    "        return on_boundary and np.isclose(x[1], -h)\n",
    "\n",
    "    def boundary_top_wall_physical(x, on_boundary):\n",
    "        # Physical wall at y = +h\n",
    "        return on_boundary and np.isclose(x[1], h)\n",
    "\n",
    "    def boundary_walls_physical(x, on_boundary):\n",
    "        # Combined physical walls\n",
    "        return boundary_bottom_wall_physical(x, on_boundary) or boundary_top_wall_physical(x, on_boundary)\n",
    "\n",
    "    # --- Inlet BCs (Dirichlet) ---\n",
    "    # u = U_INLET, v = 0, k = k_inlet_transformed, eps = eps_inlet_transformed\n",
    "    bc_u_inlet = dde.DirichletBC(geom, lambda x: config.U_INLET, boundary_inlet, component=0) # component=0 -> u\n",
    "    bc_v_inlet = dde.DirichletBC(geom, lambda x: 0, boundary_inlet, component=1) # component=1 -> v\n",
    "    bc_k_inlet = dde.DirichletBC(geom, lambda x: config.K_INLET_TRANSFORMED, boundary_inlet, component=3) # component=3 -> log(k)\n",
    "    bc_eps_inlet = dde.DirichletBC(geom, lambda x: config.EPS_INLET_TRANSFORMED, boundary_inlet, component=4) # component=4 -> log(eps)\n",
    "\n",
    "    # --- Outlet BC (Dirichlet) ---\n",
    "    # p' = 0 (gauge pressure relative to outlet)\n",
    "    bc_p_outlet = dde.DirichletBC(geom, lambda x: 0, boundary_outlet, component=2) # component=2 -> p'\n",
    "\n",
    "    # --- Physical Wall BCs (No-slip) ---\n",
    "    # u = 0, v = 0\n",
    "    bc_u_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=0) # u=0 on walls\n",
    "    bc_v_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=1) # v=0 on walls\n",
    "\n",
    "    # --- Wall Function BCs (PointSetBC near walls) ---\n",
    "    # Define anchor points slightly away from the walls at y = +/- (h - y_p)\n",
    "    # Avoid placing points exactly at inlet/outlet for stability\n",
    "    x_wf_coords = np.linspace(0 + L * 0.01, L - L * 0.01, n_wf_points)[:, None]\n",
    "    points_bottom_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, -h + y_p)))\n",
    "    points_top_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, h - y_p)))\n",
    "    anchor_points_wf = np.vstack((points_bottom_wf, points_top_wf))\n",
    "    logging.info(f\"Generated {anchor_points_wf.shape[0]} anchor points for wall functions.\")\n",
    "\n",
    "    # Target values at these anchor points (using pre-calculated config values)\n",
    "    U_target_vals = np.full((anchor_points_wf.shape[0], 1), config.U_TARGET_WF)\n",
    "    k_target_vals = np.full((anchor_points_wf.shape[0], 1), config.K_TARGET_WF_TRANSFORMED)\n",
    "    eps_target_vals = np.full((anchor_points_wf.shape[0], 1), config.EPS_TARGET_WF_TRANSFORMED)\n",
    "\n",
    "    # Define PointSetBCs for u, k (log), eps (log) at the anchor points\n",
    "    bc_u_wf = dde.PointSetBC(anchor_points_wf, U_target_vals, component=0) # Target u at WF points\n",
    "    bc_k_wf = dde.PointSetBC(anchor_points_wf, k_target_vals, component=3) # Target log(k) at WF points\n",
    "    bc_eps_wf = dde.PointSetBC(anchor_points_wf, eps_target_vals, component=4) # Target log(eps) at WF points\n",
    "\n",
    "    # --- Collect all BCs ---\n",
    "    all_bcs = [\n",
    "        bc_u_inlet, bc_v_inlet, bc_k_inlet, bc_eps_inlet, # Inlet (4 BCs)\n",
    "        bc_p_outlet, # Outlet (1 BC)\n",
    "        bc_u_walls, bc_v_walls, # Physical Walls (2 BCs)\n",
    "        bc_u_wf, bc_k_wf, bc_eps_wf # Wall Functions (3 BCs)\n",
    "    ]\n",
    "    # Note: The anchor_points_wf array is also needed by the Data object later.\n",
    "    return all_bcs, anchor_points_wf\n",
    "# --- End Boundary Conditions ---\n",
    "\n",
    "\n",
    "# =======================\n",
    "# ===== Trainer Class =====\n",
    "# =======================\n",
    "class Trainer:\n",
    "    \"\"\"Handles the setup, training, and checkpointing of the PINN model.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.losshistory = None\n",
    "        self.train_state = None\n",
    "        self.pde = pde # Assign the PDE function\n",
    "\n",
    "    def build_model(self, bcs, anchor_points):\n",
    "        \"\"\"Builds the DeepXDE model including network and data.\"\"\"\n",
    "        logging.info(\"Building the PINN model...\")\n",
    "        if dde.backend.backend_name != \"pytorch\":\n",
    "             raise RuntimeError(\"This code relies on the PyTorch backend.\")\n",
    "\n",
    "        # Define the neural network\n",
    "        net = dde.maps.FNN(\n",
    "            layer_sizes=[self.config.NETWORK_INPUTS] + [self.config.NUM_NEURONS] * self.config.NUM_LAYERS + [self.config.NETWORK_OUTPUTS],\n",
    "            activation=self.config.ACTIVATION,\n",
    "            kernel_initializer=self.config.INITIALIZER\n",
    "        )\n",
    "\n",
    "        # Wrap PDE to include config\n",
    "        pde_with_config = lambda x, y: self.pde(x, y, config=self.config)\n",
    "\n",
    "        # Define the PDE data object\n",
    "        data = dde.data.PDE(\n",
    "            geometry=self.config.GEOM,\n",
    "            pde=pde_with_config,\n",
    "            bcs=bcs, # List of boundary conditions\n",
    "            num_domain=self.config.NUM_DOMAIN_POINTS,\n",
    "            num_boundary=self.config.NUM_BOUNDARY_POINTS, # Sample points on physical boundaries\n",
    "            num_test=self.config.NUM_TEST_POINTS, # Points for testing PDE residual during training\n",
    "            anchors=anchor_points # Provide wall function anchor points here\n",
    "        )\n",
    "        self.model = dde.Model(data, net)\n",
    "        logging.info(\"Model built successfully.\")\n",
    "\n",
    "    # ==============================================================\n",
    "    # ========    UPDATED Trainer.train Method            ==========\n",
    "    # ==============================================================\n",
    "    def train(self):\n",
    "        \"\"\"Compiles and trains the model, handles checkpointing and optimizer switching.\"\"\"\n",
    "        if self.model is None:\n",
    "            logging.error(\"Model not built. Call build_model first.\")\n",
    "            return None, None, None\n",
    "\n",
    "        # --- Define Checkpoint Paths and Callback ---\n",
    "        filepath_base = os.path.join(self.config.MODEL_DIR, f\"{self.config.CHECKPOINT_FILENAME_BASE}-\")\n",
    "        logging.info(f\"Checkpoint filename base: {filepath_base}\")\n",
    "        custom_checkpointer = CustomModelCheckpoint(\n",
    "            filepath_base=filepath_base,\n",
    "            period=self.config.SAVE_INTERVAL,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Check for Latest Checkpoint ---\n",
    "        latest_checkpoint = None\n",
    "        restored_step = 0\n",
    "        if os.path.exists(self.config.MODEL_DIR):\n",
    "            try:\n",
    "                filename_pattern = re.compile(rf\"^{re.escape(self.config.CHECKPOINT_FILENAME_BASE)}-(\\d+)\\.pt$\")\n",
    "                checkpoint_files = []\n",
    "                logging.info(f\"Searching for checkpoints in: {self.config.MODEL_DIR}\")\n",
    "                logging.info(f\"Using pattern: {filename_pattern.pattern}\")\n",
    "                for f in os.listdir(self.config.MODEL_DIR):\n",
    "                    match = filename_pattern.match(f)\n",
    "                    if match:\n",
    "                        step_num = int(match.group(1))\n",
    "                        full_path = os.path.join(self.config.MODEL_DIR, f)\n",
    "                        checkpoint_files.append((step_num, full_path))\n",
    "                if checkpoint_files:\n",
    "                    checkpoint_files.sort(key=lambda item: item[0])\n",
    "                    restored_step, latest_checkpoint = checkpoint_files[-1]\n",
    "                    logging.info(f\"Found latest valid checkpoint: {latest_checkpoint} at step {restored_step}\")\n",
    "                else:\n",
    "                    logging.info(\"No valid checkpoints found matching the pattern.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error finding/parsing checkpoint filenames: {e}\", exc_info=True)\n",
    "                latest_checkpoint = None; restored_step = 0\n",
    "\n",
    "        restore_path = latest_checkpoint if (latest_checkpoint and os.path.isfile(latest_checkpoint)) else None\n",
    "\n",
    "        # --- Determine Initial Optimizer Based on Restored Step ---\n",
    "        initial_optimizer = \"adam\"\n",
    "        initial_lr = self.config.LEARNING_RATE_ADAM\n",
    "        if restore_path and restored_step >= self.config.ADAM_ITERATIONS:\n",
    "            # If restoring from a step within or after the L-BFGS phase, compile L-BFGS first.\n",
    "            initial_optimizer = \"L-BFGS\"\n",
    "            initial_lr = None # L-BFGS doesn't use LR in compile signature\n",
    "            logging.info(f\"Restored step ({restored_step}) >= Adam iterations ({self.config.ADAM_ITERATIONS}). Will compile with L-BFGS initially.\")\n",
    "        else:\n",
    "            # Start with Adam (from scratch or resuming within Adam phase).\n",
    "            logging.info(f\"Restored step ({restored_step}) < Adam iterations ({self.config.ADAM_ITERATIONS}) or no checkpoint. Will compile with Adam initially.\")\n",
    "\n",
    "        # --- Compile with the Determined Initial Optimizer ---\n",
    "        logging.info(f\"Compiling model with {initial_optimizer} optimizer initially.\")\n",
    "        if dde.backend.backend_name != \"pytorch\":\n",
    "             raise RuntimeError(\"Cannot compile model, backend is not PyTorch.\")\n",
    "\n",
    "        compile_args = {\"optimizer\": initial_optimizer, \"loss_weights\": self.config.LOSS_WEIGHTS}\n",
    "        if initial_lr is not None:\n",
    "            compile_args[\"lr\"] = initial_lr\n",
    "        try:\n",
    "            self.model.compile(**compile_args)\n",
    "            logging.info(f\"Model compiled with {initial_optimizer}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to compile model with {initial_optimizer}: {e}\", exc_info=True)\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "        # --- Explicit Restore (if applicable) ---\n",
    "        if restore_path:\n",
    "            try:\n",
    "                logging.info(f\"Explicitly restoring model state from: {restore_path}\")\n",
    "                # Restore should work because the compiled optimizer matches the saved state\n",
    "                self.model.restore(restore_path, verbose=1)\n",
    "\n",
    "                current_step_after_restore = self.model.train_state.step if self.model.train_state else -1\n",
    "                logging.info(f\"Model state restored. Internal DDE step count *after* restore: {current_step_after_restore}\")\n",
    "\n",
    "                # Force set the step counter based on the filename step for consistency\n",
    "                if self.model.train_state:\n",
    "                    if current_step_after_restore != restored_step:\n",
    "                         logging.warning(f\"Mismatch between expected restored step ({restored_step}) and DDE internal step ({current_step_after_restore}) after restore! Forcing DDE step.\")\n",
    "                    # else:\n",
    "                         # logging.info(\"DDE internal step matches restored step.\") # Can be verbose\n",
    "                    logging.info(f\"Manually setting internal step count to the restored step: {restored_step}\")\n",
    "                    self.model.train_state.step = restored_step\n",
    "                    current_step_after_manual_set = self.model.train_state.step\n",
    "                    logging.info(f\"Internal step count after manual setting: {current_step_after_manual_set}\")\n",
    "                else:\n",
    "                    # This should not happen if restore was successful with a valid state\n",
    "                    logging.error(\"Cannot manually set step count: model.train_state is None after restore. Restore might have failed silently.\")\n",
    "                    self.model = None # Indicate failure\n",
    "                    return None, None, None # Stop if restore fails\n",
    "\n",
    "            except Exception as e:\n",
    "                # The specific KeyError: 'step' should be avoided by compiling correctly first.\n",
    "                # Other errors (file corruption, etc.) might still occur.\n",
    "                logging.error(f\"Failed during explicit model restore: {e}\", exc_info=True)\n",
    "                self.model = None # Indicate failure\n",
    "                return None, None, None # Stop if restore fails\n",
    "        else:\n",
    "            logging.info(\"No suitable checkpoint found. Starting training from scratch.\")\n",
    "            restored_step = 0 # Ensure this is 0 if starting fresh\n",
    "            if self.model.train_state:\n",
    "                self.model.train_state.step = 0 # Ensure internal step is 0 for new training\n",
    "            else:\n",
    "                 # This can happen if the initial compile failed\n",
    "                 logging.error(\"model.train_state is None when starting from scratch. Compile likely failed earlier.\")\n",
    "                 return None, None, None\n",
    "\n",
    "\n",
    "        # --- Adam Training Phase ---\n",
    "        run_adam_phase = False\n",
    "        adam_iters_to_run = 0\n",
    "        # Use the step count that's been synchronized after potential restore\n",
    "        current_step_after_restore_or_init = self.model.train_state.step if self.model.train_state else restored_step\n",
    "\n",
    "        if initial_optimizer == \"adam\":\n",
    "            # Only run Adam if we started with Adam and haven't finished its iterations\n",
    "            if current_step_after_restore_or_init < self.config.ADAM_ITERATIONS:\n",
    "                run_adam_phase = True\n",
    "                adam_iters_to_run = self.config.ADAM_ITERATIONS - current_step_after_restore_or_init\n",
    "                logging.info(f\"Starting Adam training for remaining {adam_iters_to_run} iterations...\")\n",
    "            else:\n",
    "                logging.info(f\"Adam phase already completed (current step {current_step_after_restore_or_init} >= {self.config.ADAM_ITERATIONS}).\")\n",
    "        else:\n",
    "            # Adam is skipped if we restored directly into the L-BFGS phase\n",
    "            logging.info(\"Skipping Adam phase as initial compilation was L-BFGS.\")\n",
    "\n",
    "        adam_start_time = time.time()\n",
    "        if run_adam_phase and adam_iters_to_run > 0:\n",
    "            try:\n",
    "                # Ensure model is compiled with Adam before training Adam\n",
    "                if self.model.opt_name != \"adam\": # Safety check\n",
    "                     logging.warning(\"Model optimizer is not Adam before Adam training. Recompiling.\")\n",
    "                     self.model.compile(\"adam\", lr=self.config.LEARNING_RATE_ADAM, loss_weights=self.config.LOSS_WEIGHTS)\n",
    "\n",
    "                self.losshistory, self.train_state = self.model.train(\n",
    "                    iterations=adam_iters_to_run,\n",
    "                    display_every=self.config.DISPLAY_EVERY,\n",
    "                    callbacks=[custom_checkpointer] # Use the custom callback\n",
    "                )\n",
    "                adam_time = time.time() - adam_start_time\n",
    "                # Check if losshistory is valid before accessing attributes\n",
    "                if self.losshistory and hasattr(self.losshistory, 'loss_train') and self.losshistory.loss_train:\n",
    "                    final_loss = self.losshistory.loss_train[-1]\n",
    "                    current_step = self.model.train_state.step if self.model.train_state else -1\n",
    "                    logging.info(f\"Adam training ({adam_iters_to_run} iterations) finished in {adam_time:.2f}s. Final loss: {final_loss}. Current step: {current_step}\")\n",
    "                else:\n",
    "                    logging.error(\"Adam training finished but loss history is empty/invalid.\")\n",
    "                    # Allow proceeding to L-BFGS maybe, but log error. Could also return here.\n",
    "                    # return self.model, self.losshistory, self.train_state\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error occurred during Adam training: {e}\", exc_info=True)\n",
    "                return self.model, self.losshistory, self.train_state # Exit on error\n",
    "\n",
    "        # --- L-BFGS Training Phase ---\n",
    "        run_lbfgs_phase = False\n",
    "        lbfgs_iters_to_run = 0\n",
    "        needs_lbfgs_compile = False\n",
    "\n",
    "        # Get the step count *after* any Adam training that might have occurred\n",
    "        current_step_after_adam = self.model.train_state.step if self.model.train_state else current_step_after_restore_or_init\n",
    "\n",
    "        if self.config.LBFGS_ITERATIONS > 0:\n",
    "            # Rough target for total steps if both phases run fully\n",
    "            total_target_steps = self.config.ADAM_ITERATIONS + self.config.LBFGS_ITERATIONS\n",
    "\n",
    "            if current_step_after_adam < self.config.ADAM_ITERATIONS:\n",
    "                 # Still in Adam phase according to step count, L-BFGS should not run yet.\n",
    "                 logging.info(f\"Skipping L-BFGS: Current step {current_step_after_adam} is less than Adam target {self.config.ADAM_ITERATIONS}.\")\n",
    "            elif current_step_after_adam >= total_target_steps:\n",
    "                 # Already past the combined target steps based on initial config.\n",
    "                 # Note: If LBFGS runs for its full iteration count regardless of starting step,\n",
    "                 # the final step might exceed this simplistic target.\n",
    "                 logging.info(f\"Skipping L-BFGS: Current step {current_step_after_adam} meets or exceeds nominal total target steps {total_target_steps}.\")\n",
    "            else:\n",
    "                 # We are at or past the Adam iterations, L-BFGS is configured, and potentially haven't finished.\n",
    "                 run_lbfgs_phase = True\n",
    "                 # Simplification: Run the full configured L-BFGS iterations when entering this phase.\n",
    "                 # More complex logic could try to track remaining L-BFGS iterations, but it's tricky.\n",
    "                 lbfgs_iters_to_run = self.config.LBFGS_ITERATIONS\n",
    "                 logging.info(f\"Proceeding to L-BFGS phase (current step {current_step_after_adam}). Target iterations for this phase: {lbfgs_iters_to_run}\")\n",
    "\n",
    "                 # Check if we need to compile L-BFGS (i.e., if the *current* optimizer is Adam)\n",
    "                 if self.model.opt_name == \"adam\": # Check the actual current optimizer\n",
    "                     needs_lbfgs_compile = True\n",
    "                     logging.info(\"Switching optimizer: Will compile for L-BFGS.\")\n",
    "                 elif self.model.opt_name == \"L-BFGS\":\n",
    "                      logging.info(\"Optimizer is already L-BFGS (likely resumed). No recompilation needed.\")\n",
    "                 else:\n",
    "                      logging.warning(f\"Unexpected optimizer '{self.model.opt_name}' before L-BFGS phase.\")\n",
    "                      # Force compile just in case.\n",
    "                      needs_lbfgs_compile = True\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.info(\"L-BFGS iterations set to 0 in config, skipping L-BFGS training.\")\n",
    "\n",
    "\n",
    "        if run_lbfgs_phase and lbfgs_iters_to_run > 0:\n",
    "             if self.model is not None and self.model.net is not None:\n",
    "                 lbfgs_start_time = time.time()\n",
    "                 try:\n",
    "                     # Re-compile for L-BFGS only if necessary\n",
    "                     if needs_lbfgs_compile:\n",
    "                         self.model.compile(\"L-BFGS\", loss_weights=self.config.LOSS_WEIGHTS)\n",
    "                         logging.info(f\"Model re-compiled with L-BFGS. Step count after compile: {self.model.train_state.step if self.model.train_state else 'N/A'}\")\n",
    "\n",
    "                     # Use the same CUSTOM checkpointer instance for L-BFGS saves\n",
    "                     # The iterations here are L-BFGS specific iterations, not added to the global step counter in the same way Adam does.\n",
    "                     self.losshistory, self.train_state = self.model.train(\n",
    "                         iterations=lbfgs_iters_to_run, # Let L-BFGS run its course\n",
    "                         display_every=self.config.DISPLAY_EVERY, # Can use display_every for LBFGS too\n",
    "                         callbacks=[custom_checkpointer] # Pass the custom callback instance\n",
    "                     )\n",
    "                     lbfgs_time = time.time() - lbfgs_start_time\n",
    "                     if self.losshistory and hasattr(self.losshistory, 'loss_train') and self.losshistory.loss_train:\n",
    "                         final_loss = self.losshistory.loss_train[-1]\n",
    "                         # The global step counter might not increase by LBFGS_ITERATIONS here,\n",
    "                         # it might just reflect the step where L-BFGS started or finished.\n",
    "                         # Checkpoint saving relies on the step counter updated by the callback mechanism.\n",
    "                         current_step = self.model.train_state.step if self.model.train_state else -1\n",
    "                         logging.info(f\"L-BFGS training (max {lbfgs_iters_to_run} internal iterations) finished in {lbfgs_time:.2f}s. Final loss: {final_loss}. Final global step recorded: {current_step}\")\n",
    "                     else:\n",
    "                         logging.error(\"L-BFGS training finished but loss history is invalid.\")\n",
    "                         # Decide whether to continue based on Adam success\n",
    "\n",
    "                 except Exception as e:\n",
    "                     logging.error(f\"Error during L-BFGS compilation or training: {e}\", exc_info=True)\n",
    "                     # Decide whether to continue based on Adam success\n",
    "             else:\n",
    "                 logging.warning(\"Skipping L-BFGS training because the model state is not valid (e.g., restore failed or Adam failed).\")\n",
    "\n",
    "        # --- Post-training Validation (Optional) ---\n",
    "        if self.model and self.model.net:\n",
    "            try:\n",
    "                self._post_training_checks()\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Error during post-training checks: {e}\", exc_info=True)\n",
    "\n",
    "        logging.info(\"Training sequence complete.\")\n",
    "        # Make sure to return the potentially updated model, losshistory, train_state\n",
    "        return self.model, self.losshistory, self.train_state\n",
    "    # ==============================================================\n",
    "    #========== END OF UPDATED Trainer.train Method      ===========\n",
    "    # ==============================================================\n",
    "\n",
    "\n",
    "    # --- Optional Post Training Checks ---\n",
    "    def _post_training_checks(self):\n",
    "        \"\"\"Perform physics validation checks compatible with PyTorch backend.\"\"\"\n",
    "        logging.info(\"Performing PyTorch-based post-training checks...\")\n",
    "        if dde.backend.backend_name != \"pytorch\":\n",
    "             logging.warning(\"Post-training checks skipped, requires PyTorch backend.\")\n",
    "             return\n",
    "        if self.model is None or self.model.net is None:\n",
    "             logging.error(\"Model or network not available for post-training checks.\")\n",
    "             return\n",
    "        try:\n",
    "            # Example check:\n",
    "            self._check_turbulence_production()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during post-training checks: {e}\", exc_info=True)\n",
    "\n",
    "    def _check_turbulence_production(self):\n",
    "        \"\"\"Ensure turbulence production term P_k >= 0 using PyTorch autograd.\"\"\"\n",
    "        logging.info(\"Checking turbulence production term Pk...\")\n",
    "        if not hasattr(self.model.data, 'test_x') or self.model.data.test_x is None or len(self.model.data.test_x) == 0:\n",
    "            logging.warning(\"Test points not available, using training points for P_k check.\")\n",
    "            if not hasattr(self.model.data, 'train_x') or self.model.data.train_x is None or len(self.model.data.train_x) == 0:\n",
    "                 logging.error(\"No points available (train or test) for P_k check.\")\n",
    "                 return\n",
    "            X = self.model.data.train_x # Fallback to training points\n",
    "        else:\n",
    "             X = self.model.data.test_x # Prefer test points\n",
    "\n",
    "        if self.model.net is None or not list(self.model.net.parameters()):\n",
    "             logging.error(\"Model network not available or has no parameters for P_k check.\")\n",
    "             return\n",
    "\n",
    "        # Get device from model parameters\n",
    "        try:\n",
    "            device = next(self.model.net.parameters()).device\n",
    "        except StopIteration:\n",
    "             logging.error(\"Model network has no parameters.\")\n",
    "             return\n",
    "\n",
    "        x_tensor = torch.tensor(X, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "        # Forward pass with gradient tracking\n",
    "        y_tensor = self.model.net(x_tensor)\n",
    "        if y_tensor.shape[1] != self.config.NETWORK_OUTPUTS:\n",
    "             logging.error(f\"Network output has unexpected shape {y_tensor.shape} for P_k check.\")\n",
    "             return\n",
    "\n",
    "        u = y_tensor[:, 0:1]; v = y_tensor[:, 1:2]\n",
    "        k_raw = y_tensor[:, 3:4]; eps_raw = y_tensor[:, 4:5]\n",
    "\n",
    "        # Use the same transformation as in PDE\n",
    "        k = torch.exp(k_raw) + self.config.EPS_SMALL\n",
    "        eps = torch.exp(eps_raw) + self.config.EPS_SMALL\n",
    "\n",
    "        # Compute gradients using PyTorch autograd\n",
    "        try:\n",
    "            u_grad = torch.autograd.grad(u, x_tensor, grad_outputs=torch.ones_like(u), create_graph=False)[0] # No graph needed here\n",
    "            u_x, u_y = u_grad[:, 0:1], u_grad[:, 1:2]\n",
    "            v_grad = torch.autograd.grad(v, x_tensor, grad_outputs=torch.ones_like(v), create_graph=False)[0]\n",
    "            v_x, v_y = v_grad[:, 0:1], v_grad[:, 1:2]\n",
    "        except Exception as grad_e:\n",
    "             logging.error(f\"Error computing velocity gradients for P_k check: {grad_e}\", exc_info=True)\n",
    "             return\n",
    "\n",
    "        # Compute nu_t using safe k, eps from transformation\n",
    "        k_safe_check = k\n",
    "        eps_safe_check = eps\n",
    "        nu_t_check = self.config.CMU * torch.square(k_safe_check) / torch.maximum(eps_safe_check, torch.tensor(self.config.EPS_SMALL, device=device))\n",
    "\n",
    "        # Strain rate tensor squared (S^2)\n",
    "        S_squared = 2*(torch.square(u_x) + torch.square(v_y)) + torch.square(u_y + v_x)\n",
    "        # Production term P_k = nu_t * S^2\n",
    "        P_k = nu_t_check * S_squared\n",
    "\n",
    "        # Check for negative production (detach before converting to numpy/item)\n",
    "        try:\n",
    "            P_k_detached = P_k.detach()\n",
    "            min_Pk = torch.min(P_k_detached).item()\n",
    "            max_Pk = torch.max(P_k_detached).item()\n",
    "            num_negative = torch.sum(P_k_detached < 0).item()\n",
    "            if min_Pk < -self.config.EPS_SMALL * 10: # Allow small numerical errors\n",
    "                logging.warning(f\"Negative turbulence production detected! Min P_k = {min_Pk:.3e}. ({num_negative}/{len(P_k)} points < 0)\")\n",
    "            else:\n",
    "                logging.info(f\"Turbulence production check passed (min P_k = {min_Pk:.3e}, max P_k = {max_Pk:.3e})\")\n",
    "        except Exception as check_e:\n",
    "             logging.error(f\"Error checking P_k value: {check_e}\", exc_info=True)\n",
    "# --- End Trainer Class ---\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ===== Plotter Class =====\n",
    "# ========================\n",
    "class Plotter:\n",
    "    \"\"\"Handles post-processing and plotting of simulation results.\"\"\"\n",
    "    def __init__(self, config, plotter_config, model, losshistory, train_state):\n",
    "        self.config = config\n",
    "        self.plotter_config = plotter_config\n",
    "        self.model = model\n",
    "        self.losshistory = losshistory\n",
    "        self.train_state = train_state\n",
    "        self.ref_data_path = config.REFERENCE_DATA_FILE\n",
    "        self.plots_dir = config.PLOT_DIR\n",
    "        self.ref_data = None\n",
    "        self.has_ref_data = False\n",
    "        self.ref_data_utau = None # Estimated friction velocity from reference data\n",
    "        self.pinn_data_utau = None # Estimated friction velocity from PINN data\n",
    "\n",
    "        # Predicted fields (initialized to None)\n",
    "        self.X_grid, self.Y_grid = None, None\n",
    "        self.u_pred, self.v_pred, self.p_prime_pred = None, None, None\n",
    "        self.k_pred, self.eps_pred, self.nu_t_pred = None, None, None\n",
    "        self.p_pred = None # Kinematic pressure\n",
    "\n",
    "        os.makedirs(self.plots_dir, exist_ok=True)\n",
    "        logging.info(f\"Plotter initialized. Plots will be saved in: {self.plots_dir}\")\n",
    "        if self.ref_data_path:\n",
    "             if os.path.exists(self.ref_data_path):\n",
    "                 logging.info(f\"Reference CSV data path found: {self.ref_data_path}\")\n",
    "             else:\n",
    "                 logging.warning(f\"Reference CSV file not found: {self.ref_data_path}. Comparisons will be skipped.\")\n",
    "        else:\n",
    "             logging.info(\"No reference data path provided in config. Comparisons will be skipped.\")\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"Plots and saves the training loss history.\"\"\"\n",
    "        if self.losshistory and self.train_state:\n",
    "            logging.info(\"Saving loss history plot...\")\n",
    "            try:\n",
    "                os.makedirs(self.plots_dir, exist_ok=True)\n",
    "                # Use isplot=False to prevent showing plot in non-interactive envs\n",
    "                dde.saveplot(self.losshistory, self.train_state, issave=True, isplot=False, output_dir=self.plots_dir)\n",
    "                # Rename default 'loss.png' for clarity\n",
    "                default_loss_file = os.path.join(self.plots_dir, \"loss.png\")\n",
    "                target_loss_file = os.path.join(self.plots_dir, \"training_loss_history.png\")\n",
    "                if os.path.exists(default_loss_file):\n",
    "                    try:\n",
    "                        # Use os.replace for atomic rename if possible, fallback to rename\n",
    "                        os.replace(default_loss_file, target_loss_file)\n",
    "                    except OSError: # Fallback for cross-device links etc.\n",
    "                        os.rename(default_loss_file, target_loss_file)\n",
    "                    logging.info(f\"Loss history plot saved as '{target_loss_file}'.\")\n",
    "                else:\n",
    "                     # Check if the loss file was created with a different name pattern potentially\n",
    "                     # List files in the plots_dir and check for recently created png files\n",
    "                     potential_files = [f for f in os.listdir(self.plots_dir) if f.endswith('.png')]\n",
    "                     if potential_files:\n",
    "                          logging.warning(f\"dde.saveplot might not have produced 'loss.png'. Found: {potential_files}. Check DeepXDE version/behavior.\")\n",
    "                     else:\n",
    "                          logging.warning(\"dde.saveplot did not produce 'loss.png'. Check file permissions or DeepXDE version.\")\n",
    "            except ImportError:\n",
    "                logging.error(\"Matplotlib might be needed by dde.saveplot but is not installed.\")\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Could not save loss history plot: {e}\", exc_info=True)\n",
    "        else:\n",
    "            logging.warning(\"Loss history or train state not available, skipping loss plot.\")\n",
    "\n",
    "    def load_reference_data(self):\n",
    "        \"\"\"Loads and preprocesses reference data from a CSV file.\"\"\"\n",
    "        if not self.ref_data_path:\n",
    "            self.has_ref_data = False\n",
    "            return\n",
    "        if not os.path.exists(self.ref_data_path):\n",
    "            logging.warning(f\"Reference CSV file not found: '{self.ref_data_path}'. Skipping.\")\n",
    "            self.has_ref_data = False\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Loading reference data from: {self.ref_data_path}\")\n",
    "        try:\n",
    "            df_ref = pd.read_csv(self.ref_data_path)\n",
    "            logging.info(f\"Loaded reference data: {df_ref.shape[0]} rows, {df_ref.shape[1]} cols. Columns: {df_ref.columns.tolist()}\")\n",
    "\n",
    "            # --- Data Filtering (Optional) ---\n",
    "            # Filter for latest time step if applicable\n",
    "            if 'Time' in df_ref.columns:\n",
    "                df_ref = df_ref[df_ref['Time'] == df_ref['Time'].max()].copy()\n",
    "                logging.info(f\"Filtered for latest time: {df_ref.shape[0]} rows remaining.\")\n",
    "            elif 'TimeStep' in df_ref.columns:\n",
    "                df_ref = df_ref[df_ref['TimeStep'] == df_ref['TimeStep'].max()].copy()\n",
    "                logging.info(f\"Filtered for latest timestep: {df_ref.shape[0]} rows remaining.\")\n",
    "\n",
    "            # Identify coordinate columns (handle variations in naming)\n",
    "            x_col, y_col, z_col = None, None, None\n",
    "            coord_map = {'Points:0':'x', 'x-coordinate':'x', 'x':'x',\n",
    "                         'Points:1':'y', 'y-coordinate':'y', 'y':'y',\n",
    "                         'Points:2':'z', 'z-coordinate':'z', 'z':'z'}\n",
    "            for col in df_ref.columns:\n",
    "                mapped_coord = coord_map.get(col.lower().strip()) # Use lower case and strip spaces\n",
    "                if mapped_coord == 'x': x_col = col\n",
    "                if mapped_coord == 'y': y_col = col\n",
    "                if mapped_coord == 'z': z_col = col\n",
    "            if not x_col or not y_col:\n",
    "                 raise ValueError(f\"Could not definitively identify x/y coordinates in reference columns: {df_ref.columns.tolist()}\")\n",
    "            logging.info(f\"Identified coordinate columns: x='{x_col}', y='{y_col}'\" + (f\", z='{z_col}'\" if z_col else \"\"))\n",
    "\n",
    "            # Filter for specific Z-plane if data is 3D\n",
    "            if z_col and len(df_ref[z_col].unique()) > 1:\n",
    "                target_z = 0.0 # Target the center plane\n",
    "                unique_z = df_ref[z_col].unique()\n",
    "                nearest_z = unique_z[np.argmin(np.abs(unique_z - target_z))]\n",
    "                df_ref = df_ref[np.isclose(df_ref[z_col], nearest_z)].copy()\n",
    "                logging.info(f\"Filtered for z-plane near {target_z} (actual: {nearest_z:.4f}): {df_ref.shape[0]} rows remaining.\")\n",
    "\n",
    "            # --- Variable Renaming (Handle variations) ---\n",
    "            var_map = { # Map potential CSV column names to consistent internal names\n",
    "                'U:0':'u_ref', 'U_x':'u_ref', 'Velocity:0':'u_ref', 'velocity_x':'u_ref', 'u':'u_ref',\n",
    "                'U:1':'v_ref', 'U_y':'v_ref', 'Velocity:1':'v_ref', 'velocity_y':'v_ref', 'v':'v_ref',\n",
    "                'p':'p_ref', 'pressure':'p_ref', 'kinematic_pressure':'p_ref', # Assuming kinematic pressure if 'p'\n",
    "                'k':'k_ref', 'turbulentKineticEnergy':'k_ref', 'tke':'k_ref',\n",
    "                'epsilon':'eps_ref', 'turbulenceDissipationRate':'eps_ref', 'epsilon_dissipation_rate':'eps_ref', 'dissipation':'eps_ref',\n",
    "                'nut':'nut_ref', 'turbulentViscosity':'nut_ref', 'nuTilda':'nut_ref', 'eddy_viscosity':'nut_ref'\n",
    "            }\n",
    "            rename_dict = {}\n",
    "            for col in df_ref.columns:\n",
    "                 mapped_var = var_map.get(col.lower().strip()) # Case-insensitive mapping\n",
    "                 if mapped_var:\n",
    "                      rename_dict[col] = mapped_var\n",
    "\n",
    "            # Add coordinate renaming\n",
    "            rename_dict[x_col] = 'x'\n",
    "            rename_dict[y_col] = 'y'\n",
    "            if z_col: rename_dict[z_col] = 'z'\n",
    "\n",
    "            df_ref.rename(columns=rename_dict, inplace=True)\n",
    "            logging.info(f\"Renamed reference columns based on mapping: {rename_dict}\")\n",
    "\n",
    "            # --- Check for Required Columns ---\n",
    "            required_cols = ['x', 'y', 'u_ref', 'p_ref', 'k_ref', 'eps_ref']\n",
    "            missing_cols = [col for col in required_cols if col not in df_ref.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns after renaming in reference data: {missing_cols}. Available: {df_ref.columns.tolist()}\")\n",
    "\n",
    "            # Keep only necessary columns + optional ones if present\n",
    "            cols_to_keep = ['x', 'y'] + [col for col in ['u_ref', 'v_ref', 'p_ref', 'k_ref', 'eps_ref', 'nut_ref'] if col in df_ref.columns]\n",
    "            if z_col and 'z' in df_ref.columns: cols_to_keep.append('z')\n",
    "            df_ref = df_ref[cols_to_keep]\n",
    "\n",
    "            # Sort and reset index\n",
    "            df_ref.sort_values(by=['x', 'y'], inplace=True)\n",
    "            df_ref.reset_index(drop=True, inplace=True)\n",
    "            self.ref_data = df_ref\n",
    "            self.has_ref_data = True\n",
    "            logging.info(f\"Successfully loaded and preprocessed reference CSV data. Final columns: {df_ref.columns.tolist()}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # Already logged warning, just pass\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading or processing reference CSV: {e}\", exc_info=True)\n",
    "            self.ref_data = None\n",
    "            self.has_ref_data = False\n",
    "\n",
    "    def predict_pinn_fields(self):\n",
    "        \"\"\"Predicts flow fields using the trained PINN model on a grid.\"\"\"\n",
    "        if self.model is None or self.model.net is None:\n",
    "             logging.error(\"PINN Model or network not available for prediction.\")\n",
    "             return False\n",
    "\n",
    "        logging.info(\"Predicting PINN flow fields on evaluation grid...\")\n",
    "        nx = self.plotter_config.NX_PRED\n",
    "        ny = self.plotter_config.NY_PRED\n",
    "        x_coords = np.linspace(0, self.config.L, nx)\n",
    "        y_coords = np.linspace(-self.config.CHANNEL_HALF_HEIGHT, self.config.CHANNEL_HALF_HEIGHT, ny)\n",
    "        self.X_grid, self.Y_grid = np.meshgrid(x_coords, y_coords)\n",
    "        # Create prediction points (N, 2) array\n",
    "        pred_points = np.vstack((np.ravel(self.X_grid), np.ravel(self.Y_grid))).T\n",
    "\n",
    "        try:\n",
    "            # Use model.predict for inference\n",
    "            # Ensure input is float32, as model was likely trained with it\n",
    "            pred_points_tensor = torch.tensor(pred_points, dtype=torch.float32)\n",
    "            predictions_raw = self.model.predict(pred_points_tensor.cpu().numpy()) # Predict expects numpy\n",
    "\n",
    "            if predictions_raw is None or predictions_raw.shape[1] != self.config.NETWORK_OUTPUTS:\n",
    "                logging.error(f\"Prediction shape mismatch. Expected {self.config.NETWORK_OUTPUTS} outputs, got shape {predictions_raw.shape if predictions_raw is not None else 'None'}.\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during PINN prediction: {e}\", exc_info=True)\n",
    "            return False\n",
    "\n",
    "        # --- Process Raw Predictions ---\n",
    "        # Reshape predictions back to grid format (ny, nx)\n",
    "        self.u_pred = predictions_raw[:, 0].reshape(ny, nx)\n",
    "        self.v_pred = predictions_raw[:, 1].reshape(ny, nx)\n",
    "        self.p_prime_pred = predictions_raw[:, 2].reshape(ny, nx)\n",
    "        k_raw_pred = predictions_raw[:, 3].reshape(ny, nx)\n",
    "        eps_raw_pred = predictions_raw[:, 4].reshape(ny, nx)\n",
    "\n",
    "        # Apply inverse transform (exp) and add epsilon for positivity\n",
    "        self.k_pred = np.exp(k_raw_pred) + self.config.EPS_SMALL\n",
    "        self.eps_pred = np.exp(eps_raw_pred) + self.config.EPS_SMALL\n",
    "\n",
    "        # Calculate kinematic pressure p = p' - (2/3)*k (assuming isotropic normal stress contribution)\n",
    "        # Note: For plotting/comparison, often p' itself is used, or a relative pressure.\n",
    "        # This definition assumes p' = p_kinematic + (2/3)k\n",
    "        self.p_pred = self.p_prime_pred - (2.0 / 3.0) * self.k_pred # Definition check needed\n",
    "\n",
    "        # Calculate turbulent viscosity nu_t = Cmu * k^2 / eps\n",
    "        # Use maximum with small number in denominator for robustness\n",
    "        self.nu_t_pred = self.config.CMU * np.square(self.k_pred) / np.maximum(self.eps_pred, self.config.EPS_SMALL**2)\n",
    "\n",
    "        logging.info(\"PINN field prediction and processing complete.\")\n",
    "        return True\n",
    "\n",
    "    def plot_contour_fields(self):\n",
    "        \"\"\"Plots contour fields of the predicted PINN variables.\"\"\"\n",
    "        if self.u_pred is None:\n",
    "            logging.warning(\"PINN data unavailable for plotting. Run predict_pinn_fields first. Skipping contours.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Generating PINN contour plots...\")\n",
    "        plt.figure(figsize=(18, 12)) # Adjust figure size as needed\n",
    "\n",
    "        cmap_vel = self.plotter_config.CMAP_VELOCITY\n",
    "        cmap_p = self.plotter_config.CMAP_PRESSURE\n",
    "        cmap_turb = self.plotter_config.CMAP_TURBULENCE\n",
    "\n",
    "        # Helper function for plotting individual contours\n",
    "        def plot_contour(subplot_idx, data, title, label, cmap='viridis', is_log=False):\n",
    "            plt.subplot(2, 3, subplot_idx)\n",
    "            plot_data = data\n",
    "            cbar_label = label\n",
    "            levels = 50 # Number of contour levels\n",
    "\n",
    "            # Optional log scale for positive quantities like k, eps, nut\n",
    "            if is_log:\n",
    "                # Avoid log(0) or log(negative) issues\n",
    "                min_positive = np.min(data[data > self.config.EPS_SMALL]) if np.any(data > self.config.EPS_SMALL) else self.config.EPS_SMALL\n",
    "                # Floor values at a small fraction of min_positive before taking log10\n",
    "                plot_data = np.log10(np.maximum(data, min_positive * 0.1))\n",
    "                cbar_label = f'log10({label})'\n",
    "\n",
    "            try:\n",
    "                cf = plt.contourf(self.X_grid, self.Y_grid, plot_data, levels=levels, cmap=cmap, extend='both')\n",
    "                plt.colorbar(cf, label=cbar_label)\n",
    "                plt.title(title)\n",
    "                plt.xlabel('x (m)')\n",
    "                plt.ylabel('y (m)')\n",
    "                plt.gca().set_aspect('equal', adjustable='box') # Make aspect ratio equal\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error plotting contour for {title}: {e}\")\n",
    "\n",
    "        # Plot individual fields\n",
    "        plot_contour(1, self.u_pred, 'PINN Streamwise Velocity (u)', 'u (m/s)', cmap=cmap_vel)\n",
    "        plot_contour(2, self.v_pred, 'PINN Transverse Velocity (v)', 'v (m/s)', cmap=cmap_vel)\n",
    "        plot_contour(3, self.p_pred, \"PINN Kinematic Pressure (p)\", 'p/rho (m^2/s^2)', cmap=cmap_p) # Using calculated p\n",
    "        # plot_contour(3, self.p_prime_pred, \"PINN Fluctuation Pressure (p')\", 'p\\'/rho (m^2/s^2)', cmap=cmap_p) # Alternative: plot p'\n",
    "\n",
    "        plot_contour(4, self.k_pred, 'PINN Turbulent Kinetic Energy (k)', 'k (m^2/s^2)', cmap=cmap_turb) # Linear scale k\n",
    "        # plot_contour(4, self.k_pred, 'PINN Turbulent Kinetic Energy (k)', 'k (m^2/s^2)', cmap=cmap_turb, is_log=True) # Log scale k\n",
    "\n",
    "        plot_contour(5, self.eps_pred, 'PINN Dissipation Rate (epsilon)', 'eps (m^2/s^3)', cmap=cmap_turb) # Linear scale eps\n",
    "        # plot_contour(5, self.eps_pred, 'PINN Dissipation Rate (epsilon)', 'eps (m^2/s^3)', cmap=cmap_turb, is_log=True) # Log scale eps\n",
    "\n",
    "        # Plot eddy viscosity ratio nu_t / nu\n",
    "        eddy_viscosity_ratio = self.nu_t_pred / self.config.NU\n",
    "        plot_contour(6, eddy_viscosity_ratio, 'PINN Eddy Viscosity Ratio', 'nu_t / nu', cmap=cmap_turb) # Linear scale ratio\n",
    "        # plot_contour(6, eddy_viscosity_ratio, 'PINN Eddy Viscosity Ratio', 'nu_t / nu', cmap=cmap_turb, is_log=True) # Log scale ratio\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.plots_dir, \"pinn_field_contours.png\")\n",
    "        try:\n",
    "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close() # Close figure to free memory\n",
    "            logging.info(f\"PINN contour field plots saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save contour plot: {e}\")\n",
    "\n",
    "    def _estimate_utau(self, data_source='pinn', x_slice_loc=None):\n",
    "        \"\"\"Estimates friction velocity u_tau from data near the wall.\"\"\"\n",
    "        if x_slice_loc is None:\n",
    "            # Choose a location away from inlet/outlet, e.g., 80% downstream\n",
    "            x_slice_loc = self.config.L * 0.8\n",
    "\n",
    "        h = self.config.CHANNEL_HALF_HEIGHT\n",
    "        y_p = self.config.Y_P # Use the wall function distance y_p\n",
    "        nu = self.config.NU\n",
    "\n",
    "        # Define two points near the wall (e.g., slightly inside and outside y_p) for gradient calc\n",
    "        # Ensure points are within the channel bounds [-h, h]\n",
    "        y_eval_1 = h - y_p * 1.1 # Slightly further from wall than y_p\n",
    "        y_eval_2 = h - y_p * 0.9 # Slightly closer to wall than y_p\n",
    "\n",
    "        # Clamp points to be within the domain bounds\n",
    "        y_eval_1 = max(y_eval_1, -h + self.config.EPS_SMALL * 1.1)\n",
    "        y_eval_2 = min(y_eval_2, h - self.config.EPS_SMALL * 0.9)\n",
    "        # Ensure y_eval_2 > y_eval_1\n",
    "        if y_eval_2 <= y_eval_1:\n",
    "             y_eval_1 = h - y_p - self.config.EPS_SMALL\n",
    "             y_eval_2 = h - y_p + self.config.EPS_SMALL\n",
    "             y_eval_1 = max(y_eval_1, -h + self.config.EPS_SMALL * 1.1)\n",
    "             y_eval_2 = min(y_eval_2, h - self.config.EPS_SMALL * 0.9)\n",
    "             if y_eval_2 <= y_eval_1: # Failsafe\n",
    "                  logging.error(\"Cannot define distinct points near y_p for u_tau estimation.\")\n",
    "                  return None\n",
    "\n",
    "        eval_points = np.array([[x_slice_loc, y_eval_1], [x_slice_loc, y_eval_2]])\n",
    "        # Average distance from the *nearest* wall (top wall in this case)\n",
    "        y_dist_wall_avg = h - (y_eval_1 + y_eval_2) / 2.0\n",
    "\n",
    "        u1, k1, eps1, u2, k2, eps2 = None, None, None, None, None, None\n",
    "\n",
    "        try:\n",
    "            # Get u, k, eps at the two evaluation points\n",
    "            if data_source == 'pinn':\n",
    "                if self.model is None: return None\n",
    "                pred_raw = self.model.predict(eval_points)\n",
    "                if pred_raw is None or pred_raw.shape[0] < 2: return None\n",
    "                # Extract and transform\n",
    "                u1, u2 = pred_raw[:, 0]\n",
    "                k1_raw, k2_raw = pred_raw[:, 3]\n",
    "                eps1_raw, eps2_raw = pred_raw[:, 4]\n",
    "                k1 = np.exp(k1_raw) + self.config.EPS_SMALL\n",
    "                k2 = np.exp(k2_raw) + self.config.EPS_SMALL\n",
    "                eps1 = np.exp(eps1_raw) + self.config.EPS_SMALL\n",
    "                eps2 = np.exp(eps2_raw) + self.config.EPS_SMALL\n",
    "\n",
    "            elif data_source == 'reference' and self.has_ref_data:\n",
    "                if self.ref_data is None: return None\n",
    "                points_ref = self.ref_data[['x', 'y']].values\n",
    "                req_cols = ['u_ref', 'k_ref', 'eps_ref']\n",
    "                if not all(col in self.ref_data.columns for col in req_cols):\n",
    "                    logging.warning(f\"Reference data missing required columns {req_cols} for u_tau estimation.\")\n",
    "                    return None\n",
    "                # Interpolate required values\n",
    "                u_ref = self.ref_data['u_ref'].values\n",
    "                k_ref = self.ref_data['k_ref'].values\n",
    "                eps_ref = self.ref_data['eps_ref'].values\n",
    "                interp_u = griddata(points_ref, u_ref, eval_points, method='linear')\n",
    "                interp_k = griddata(points_ref, k_ref, eval_points, method='linear')\n",
    "                interp_eps = griddata(points_ref, eps_ref, eval_points, method='linear')\n",
    "\n",
    "                # Handle potential NaN from linear interpolation (e.g., if eval_points are outside convex hull)\n",
    "                nan_mask = np.isnan(interp_u) | np.isnan(interp_k) | np.isnan(interp_eps)\n",
    "                if np.any(nan_mask):\n",
    "                    logging.warning(f\"Linear interpolation failed for u_tau estimation ({data_source}) at x={x_slice_loc:.2f}. Trying 'nearest'.\")\n",
    "                    interp_u[nan_mask] = griddata(points_ref, u_ref, eval_points[nan_mask], method='nearest')\n",
    "                    interp_k[nan_mask] = griddata(points_ref, k_ref, eval_points[nan_mask], method='nearest')\n",
    "                    interp_eps[nan_mask] = griddata(points_ref, eps_ref, eval_points[nan_mask], method='nearest')\n",
    "                    # Check again if nearest neighbor failed\n",
    "                    if np.isnan(interp_u).any():\n",
    "                         logging.error(f\"Nearest neighbor interpolation also failed for u_tau estimation ({data_source}) at x={x_slice_loc:.2f}.\")\n",
    "                         return None\n",
    "                u1, u2 = interp_u\n",
    "                k1, k2 = interp_k\n",
    "                eps1, eps2 = interp_eps # Already physical values from CSV\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"Invalid data_source '{data_source}' or missing data for u_tau estimation.\")\n",
    "                return None\n",
    "\n",
    "            # Estimate gradient du/dy\n",
    "            # Need absolute value since y_eval_2 > y_eval_1 but corresponds to smaller wall distance\n",
    "            du_dy_eval = (u2 - u1) / (y_eval_2 - y_eval_1) # Should be positive near top wall\n",
    "\n",
    "            # Estimate effective viscosity at the average location\n",
    "            k_avg = (k1 + k2) / 2.0\n",
    "            eps_avg = (eps1 + eps2) / 2.0\n",
    "            nu_t_avg = self.config.CMU * k_avg**2 / max(eps_avg, self.config.EPS_SMALL**2)\n",
    "            nu_eff_avg = nu + nu_t_avg\n",
    "\n",
    "            # Estimate wall shear stress tau_w = rho * nu_eff * |du/dy| (near wall)\n",
    "            # Using absolute value of gradient for robustness\n",
    "            tau_w_grad = self.config.RHO * nu_eff_avg * abs(du_dy_eval)\n",
    "            # Estimate u_tau = sqrt(tau_w / rho)\n",
    "            u_tau_grad = np.sqrt(max(tau_w_grad / self.config.RHO, self.config.EPS_SMALL)) # Ensure positive arg\n",
    "\n",
    "            # Optional: Refine using log-law if in log region\n",
    "            u_avg = (u1 + u2) / 2.0\n",
    "            y_plus_est = y_dist_wall_avg * u_tau_grad / nu\n",
    "            u_tau_estimated = u_tau_grad # Default estimate\n",
    "\n",
    "            # If y+ is sufficiently large, blend with log-law estimate\n",
    "            if y_plus_est > 11: # Heuristic threshold for log-law applicability\n",
    "                 try:\n",
    "                    # Estimate u_tau from U = u_tau/kappa * ln(E*y+)\n",
    "                    log_arg = max(self.config.E_WALL * y_plus_est, self.config.EPS_SMALL)\n",
    "                    denominator = (1 / self.config.KAPPA) * np.log(log_arg)\n",
    "                    if abs(denominator) > self.config.EPS_SMALL:\n",
    "                         u_tau_log = u_avg / denominator\n",
    "                         # Simple average blend (could use weighting)\n",
    "                         u_tau_estimated = (u_tau_grad + u_tau_log) / 2.0\n",
    "                 except Exception as log_e:\n",
    "                     logging.warning(f\"Could not apply log-law refinement for u_tau: {log_e}\")\n",
    "                     pass # Use gradient estimate if log-law fails\n",
    "\n",
    "            logging.info(f\"Estimated u_tau ({data_source}) at x={x_slice_loc:.2f} m: {u_tau_estimated:.4f} m/s (y+ ~ {y_plus_est:.1f})\")\n",
    "            return u_tau_estimated\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error estimating u_tau for {data_source} at x={x_slice_loc:.2f}: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def plot_profile_comparison(self):\n",
    "        \"\"\"Plots profiles of PINN vs Reference data at a channel cross-section.\"\"\"\n",
    "        if self.u_pred is None:\n",
    "            logging.warning(\"PINN data unavailable for plotting. Skipping profile comparison.\")\n",
    "            return\n",
    "        if not self.has_ref_data:\n",
    "            logging.warning(\"Reference CSV data not loaded or failed processing. Skipping profile comparison.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Generating profile comparison plots...\")\n",
    "        # Define slice location (e.g., channel midpoint)\n",
    "        x_slice_loc = self.config.L / 2.0\n",
    "        ny_pinn = self.plotter_config.NY_PRED # Number of points in y from PINN grid\n",
    "\n",
    "        # Find the closest x-coordinate in the PINN grid\n",
    "        y_coords_pinn = self.Y_grid[:, 0] # y-coordinates from PINN grid\n",
    "        x_coords_pinn = self.X_grid[0, :] # x-coordinates from PINN grid\n",
    "        x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc))\n",
    "        actual_x_pinn = x_coords_pinn[x_slice_idx_pinn] # Actual x used for slicing\n",
    "\n",
    "        # Extract PINN data slice at the chosen x-index\n",
    "        pinn_slice = {\n",
    "            'y': y_coords_pinn,\n",
    "            'u': self.u_pred[:, x_slice_idx_pinn],\n",
    "            'v': self.v_pred[:, x_slice_idx_pinn],\n",
    "            'p': self.p_pred[:, x_slice_idx_pinn], # Use calculated kinematic p\n",
    "            'k': self.k_pred[:, x_slice_idx_pinn],\n",
    "            'eps': self.eps_pred[:, x_slice_idx_pinn],\n",
    "            'nut': self.nu_t_pred[:, x_slice_idx_pinn]\n",
    "        }\n",
    "\n",
    "        # --- Interpolate Reference Data onto PINN y-coordinates at the same x ---\n",
    "        ref_slice = {'y': y_coords_pinn} # Initialize dict for interpolated ref data\n",
    "        try:\n",
    "            if self.ref_data is None: raise ValueError(\"Reference data frame is None.\")\n",
    "\n",
    "            # Points from reference data (x, y)\n",
    "            points_ref = self.ref_data[['x', 'y']].values\n",
    "            # Target points for interpolation (same x, PINN y-coords)\n",
    "            target_points = np.vstack((np.full(ny_pinn, actual_x_pinn), y_coords_pinn)).T\n",
    "\n",
    "            logging.info(f\"Interpolating reference data onto {ny_pinn} points at x={actual_x_pinn:.2f}...\")\n",
    "\n",
    "            # Interpolate each variable present in the reference data\n",
    "            variables_to_interpolate = [\n",
    "                ('u_ref', 'u'), ('v_ref', 'v'), ('p_ref', 'p'),\n",
    "                ('k_ref', 'k'), ('eps_ref', 'eps'), ('nut_ref', 'nut')\n",
    "            ]\n",
    "            for var_ref, var_pinn in variables_to_interpolate:\n",
    "                if var_ref in self.ref_data.columns:\n",
    "                    values_ref = self.ref_data[var_ref].values\n",
    "                    # Linear interpolation\n",
    "                    interp_values = griddata(points_ref, values_ref, target_points, method='linear')\n",
    "                    # Handle NaNs with nearest neighbor fallback\n",
    "                    nan_mask = np.isnan(interp_values)\n",
    "                    if np.any(nan_mask):\n",
    "                        interp_nearest = griddata(points_ref, values_ref, target_points[nan_mask], method='nearest')\n",
    "                        interp_values[nan_mask] = interp_nearest\n",
    "                        if np.any(np.isnan(interp_values)): # Check if nearest also failed\n",
    "                             logging.warning(f\"Interpolation (linear & nearest) failed for '{var_ref}' at some points.\")\n",
    "                    ref_slice[var_pinn] = interp_values\n",
    "                else:\n",
    "                    logging.warning(f\"Reference variable '{var_ref}' not found in CSV data. Skipping interpolation.\")\n",
    "                    ref_slice[var_pinn] = np.full(ny_pinn, np.nan) # Fill with NaN if missing\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error interpolating reference data for profiles: {e}\", exc_info=True)\n",
    "            # If interpolation fails, mark ref data as unavailable for this plot\n",
    "            self.has_ref_data = False # Prevent plotting potentially bad data\n",
    "\n",
    "        # --- Create Plots ---\n",
    "        plt.figure(figsize=(15, 12)) # Adjust size\n",
    "        plot_idx = 1\n",
    "        h = self.config.CHANNEL_HALF_HEIGHT\n",
    "        plot_vars = [ # Variables to plot, their names, and units\n",
    "            ('u', 'Velocity u', 'm/s'),\n",
    "            ('v', 'Velocity v', 'm/s'),\n",
    "            ('p', 'Kinematic Pressure p', 'm^2/s^2'),\n",
    "            ('k', 'TKE k', 'm^2/s^2'),\n",
    "            ('eps', 'Dissipation eps', 'm^2/s^3'),\n",
    "            ('nut', 'Eddy Viscosity nu_t', 'm^2/s')\n",
    "        ]\n",
    "\n",
    "        for key, name, unit in plot_vars:\n",
    "            plt.subplot(3, 2, plot_idx) # Arrange plots in 3 rows, 2 columns\n",
    "\n",
    "            # Plot PINN data\n",
    "            plt.plot(pinn_slice[key], pinn_slice['y'] / h, 'r-', linewidth=2, label='PINN')\n",
    "\n",
    "            # Plot Reference data if available and valid\n",
    "            if self.has_ref_data and key in ref_slice and not np.all(np.isnan(ref_slice[key])):\n",
    "                plt.plot(ref_slice[key], ref_slice['y'] / h, 'b--', linewidth=1.5, label='Reference (CSV)')\n",
    "\n",
    "            plt.xlabel(f'{name} ({unit})')\n",
    "            plt.ylabel('y/h') # Normalize y by half-height\n",
    "            plt.title(f'{name} Profile at x={actual_x_pinn:.2f}m')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=':')\n",
    "\n",
    "            # Use log scale for x-axis for turbulence quantities (k, eps, nut) if values are positive\n",
    "            if key in ['k', 'eps', 'nut']:\n",
    "                 try:\n",
    "                     # Check if all plotted values are sufficiently positive\n",
    "                     min_val_for_log = self.config.EPS_SMALL / 10.0\n",
    "                     pinn_positive = np.all(pinn_slice[key] > min_val_for_log)\n",
    "                     ref_positive = True # Assume true if no ref data plotted\n",
    "                     if self.has_ref_data and key in ref_slice and not np.all(np.isnan(ref_slice[key])):\n",
    "                          ref_positive = np.all(ref_slice[key][~np.isnan(ref_slice[key])] > min_val_for_log)\n",
    "\n",
    "                     if pinn_positive and ref_positive:\n",
    "                          plt.semilogx()\n",
    "                          plt.grid(True, which='both', linestyle=':') # Add minor grid for log scale\n",
    "                 except Exception as log_e:\n",
    "                      logging.warning(f\"Could not apply log scale for {key}: {log_e}\")\n",
    "\n",
    "            plot_idx += 1\n",
    "\n",
    "        plt.suptitle(f'Profile Comparison at x  {x_slice_loc:.2f} m', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.94]) # Adjust layout to prevent title overlap\n",
    "\n",
    "        save_path = os.path.join(self.plots_dir, \"profile_comparison_pinn_vs_csv.png\")\n",
    "        try:\n",
    "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Profile comparison plot saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save profile comparison plot: {e}\")\n",
    "\n",
    "    def plot_wall_unit_comparison(self):\n",
    "        \"\"\"Plots profiles in wall units (y+, U+, k+, eps+) vs reference/theory.\"\"\"\n",
    "        if self.u_pred is None:\n",
    "            logging.warning(\"PINN data unavailable for plotting. Skipping wall unit plots.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Generating wall unit comparison plots...\")\n",
    "\n",
    "        # --- Estimate Friction Velocity (u_tau) ---\n",
    "        # Use a location away from inlet/outlet for better u_tau estimate\n",
    "        x_slice_loc_utau = self.config.L * 0.8\n",
    "        self.pinn_data_utau = self._estimate_utau(data_source='pinn', x_slice_loc=x_slice_loc_utau)\n",
    "        if self.has_ref_data:\n",
    "            self.ref_data_utau = self._estimate_utau(data_source='reference', x_slice_loc=x_slice_loc_utau)\n",
    "        else:\n",
    "            self.ref_data_utau = None\n",
    "\n",
    "        # Proceed only if PINN u_tau could be estimated\n",
    "        if not self.pinn_data_utau:\n",
    "            logging.error(\"Could not estimate PINN u_tau. Skipping wall unit plots.\")\n",
    "            return\n",
    "        if self.has_ref_data and not self.ref_data_utau:\n",
    "            logging.warning(\"Could not estimate reference u_tau. Plotting PINN wall units only vs theory.\")\n",
    "\n",
    "        # --- Prepare Data for Wall Units ---\n",
    "        nu = self.config.NU\n",
    "        h = self.config.CHANNEL_HALF_HEIGHT\n",
    "        kappa = self.config.KAPPA\n",
    "        E_wall_plot = self.config.E_WALL # Use config value for consistency\n",
    "        Cmu = self.config.CMU\n",
    "\n",
    "        # Use the same x-slice as the profile plots for consistency, or define a new one\n",
    "        x_slice_loc_plot = self.config.L / 2.0\n",
    "        y_coords_pinn = self.Y_grid[:, 0]\n",
    "        x_coords_pinn = self.X_grid[0, :]\n",
    "        x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc_plot))\n",
    "        actual_x_pinn = x_coords_pinn[x_slice_idx_pinn]\n",
    "\n",
    "        # Extract PINN data near the top wall (y >= 0)\n",
    "        # Could average top/bottom walls or plot separately\n",
    "        wall_indices_pinn = y_coords_pinn >= 0 # Indices for top half\n",
    "        y_wall_pinn = y_coords_pinn[wall_indices_pinn]\n",
    "        # Distance from the nearest wall (top wall: h - y)\n",
    "        y_dist_wall_pinn = np.maximum(h - y_wall_pinn, self.config.EPS_SMALL * h) # Avoid zero distance\n",
    "\n",
    "        # Get corresponding u, k, eps from the PINN slice\n",
    "        u_wall_pinn = self.u_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
    "        k_wall_pinn = self.k_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
    "        eps_wall_pinn = self.eps_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
    "\n",
    "        # Calculate wall units for PINN data\n",
    "        utau_pinn_safe = max(self.pinn_data_utau, self.config.EPS_SMALL) # Avoid division by zero\n",
    "        y_plus_pinn = y_dist_wall_pinn * utau_pinn_safe / nu\n",
    "        u_plus_pinn = u_wall_pinn / utau_pinn_safe\n",
    "        # k+ = k / u_tau^2\n",
    "        k_plus_pinn = k_wall_pinn / max(utau_pinn_safe**2, self.config.EPS_SMALL)\n",
    "        # eps+ = eps * nu / u_tau^4\n",
    "        eps_plus_pinn = eps_wall_pinn * nu / max(utau_pinn_safe**4, self.config.EPS_SMALL)\n",
    "\n",
    "        # Sort PINN data by y+ for plotting lines correctly\n",
    "        sort_idx_pinn = np.argsort(y_plus_pinn)\n",
    "        y_plus_pinn = y_plus_pinn[sort_idx_pinn]\n",
    "        u_plus_pinn = u_plus_pinn[sort_idx_pinn]\n",
    "        k_plus_pinn = k_plus_pinn[sort_idx_pinn]\n",
    "        eps_plus_pinn = eps_plus_pinn[sort_idx_pinn]\n",
    "\n",
    "        # --- Prepare Reference Data (if available and u_tau estimated) ---\n",
    "        y_plus_ref, u_plus_ref, k_plus_ref, eps_plus_ref = None, None, None, None\n",
    "        if self.has_ref_data and self.ref_data_utau:\n",
    "            try:\n",
    "                # Filter reference data near the chosen x-slice and top wall (y>=0)\n",
    "                # Use a tolerance for x matching\n",
    "                ref_wall_data = self.ref_data[\n",
    "                    (np.isclose(self.ref_data['x'], actual_x_pinn, rtol=0.05, atol=0.1)) &\n",
    "                    (self.ref_data['y'] >= -self.config.EPS_SMALL) # Include y=0\n",
    "                ].copy()\n",
    "\n",
    "                if not ref_wall_data.empty:\n",
    "                    y_dist_wall_ref = np.maximum(h - ref_wall_data['y'].values, self.config.EPS_SMALL * h)\n",
    "                    utau_ref_safe = max(self.ref_data_utau, self.config.EPS_SMALL)\n",
    "                    y_plus_ref = y_dist_wall_ref * utau_ref_safe / nu\n",
    "                    # Check if required columns exist before accessing\n",
    "                    if 'u_ref' in ref_wall_data.columns: u_plus_ref = ref_wall_data['u_ref'].values / utau_ref_safe\n",
    "                    if 'k_ref' in ref_wall_data.columns: k_plus_ref = ref_wall_data['k_ref'].values / max(utau_ref_safe**2, self.config.EPS_SMALL)\n",
    "                    if 'eps_ref' in ref_wall_data.columns: eps_plus_ref = ref_wall_data['eps_ref'].values * nu / max(utau_ref_safe**4, self.config.EPS_SMALL)\n",
    "\n",
    "                    # Sort reference data by y+\n",
    "                    sort_idx_ref = np.argsort(y_plus_ref)\n",
    "                    y_plus_ref = y_plus_ref[sort_idx_ref]\n",
    "                    if u_plus_ref is not None: u_plus_ref = u_plus_ref[sort_idx_ref]\n",
    "                    if k_plus_ref is not None: k_plus_ref = k_plus_ref[sort_idx_ref]\n",
    "                    if eps_plus_ref is not None: eps_plus_ref = eps_plus_ref[sort_idx_ref]\n",
    "                    logging.info(f\"Processed {len(y_plus_ref)} reference points for wall unit comparison.\")\n",
    "                else:\n",
    "                    logging.warning(f\"No reference data found near x={actual_x_pinn:.2f}, y>=0 for wall unit plots.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing reference data for wall units: {e}\", exc_info=True)\n",
    "                # Prevent plotting bad ref data\n",
    "                y_plus_ref, u_plus_ref, k_plus_ref, eps_plus_ref = None, None, None, None\n",
    "\n",
    "        # --- Create Wall Unit Plots ---\n",
    "        plt.figure(figsize=(18, 6)) # Figure for U+, k+, eps+\n",
    "\n",
    "        # Determine plot limits dynamically\n",
    "        y_plus_max_plot = 1.1 * max(\n",
    "            np.max(y_plus_pinn) if len(y_plus_pinn) > 0 else 100,\n",
    "            np.max(y_plus_ref) if y_plus_ref is not None and len(y_plus_ref) > 0 else 100,\n",
    "            self.config.YP_PLUS_TARGET * 1.5 # Ensure target y+ is visible\n",
    "        )\n",
    "        u_plus_max_plot = 1.1 * max(\n",
    "             np.max(u_plus_pinn) if len(u_plus_pinn)>0 else 25,\n",
    "             np.max(u_plus_ref) if u_plus_ref is not None and len(u_plus_ref)>0 else 25\n",
    "        )\n",
    "        k_plus_max_plot = 1.1 * max(\n",
    "             np.max(k_plus_pinn) if len(k_plus_pinn)>0 else 5,\n",
    "             np.max(k_plus_ref) if k_plus_ref is not None and len(k_plus_ref)>0 else 5,\n",
    "             (self.config.K_TARGET_WF / max(self.pinn_data_utau**2, 1e-9)) * 1.2 if self.pinn_data_utau else 5 # Include target k+ if estimable\n",
    "        )\n",
    "        # Determine eps+ limits carefully (can vary wildly)\n",
    "        min_eps_plus_data = min(\n",
    "            np.min(eps_plus_pinn[eps_plus_pinn > 0]) if np.any(eps_plus_pinn > 0) else 1e-4,\n",
    "            np.min(eps_plus_ref[eps_plus_ref > 0]) if eps_plus_ref is not None and np.any(eps_plus_ref > 0) else 1e-4\n",
    "        )\n",
    "        max_eps_plus_data = max(\n",
    "            np.max(eps_plus_pinn) if len(eps_plus_pinn)>0 else 1,\n",
    "            np.max(eps_plus_ref) if eps_plus_ref is not None and len(eps_plus_ref)>0 else 1\n",
    "        )\n",
    "\n",
    "\n",
    "        # 1. U+ vs y+ plot\n",
    "        ax1 = plt.subplot(1, 3, 1)\n",
    "        ax1.semilogx(y_plus_pinn, u_plus_pinn, 'r.', ms=4, label=f'PINN ($u_\\\\tau \\\\approx {self.pinn_data_utau:.3f}$)')\n",
    "        if y_plus_ref is not None and u_plus_ref is not None:\n",
    "            ax1.semilogx(y_plus_ref, u_plus_ref, 'bo', mfc='none', ms=5, label=f'Reference ($u_\\\\tau \\\\approx {self.ref_data_utau:.3f}$)' if self.ref_data_utau else 'Reference (u_tau N/A)')\n",
    "        # Theoretical laws\n",
    "        y_plus_theory_log = np.logspace(np.log10(max(11, 1)), np.log10(y_plus_max_plot*1.1), 100) # Extend slightly beyond max y+\n",
    "        u_plus_loglaw = (1 / kappa) * np.log(y_plus_theory_log) + E_wall_plot\n",
    "        y_plus_theory_vis = np.linspace(0.1, 20, 50) # Viscous sublayer range\n",
    "        u_plus_viscous = y_plus_theory_vis # U+ = y+\n",
    "        ax1.semilogx(y_plus_theory_log, u_plus_loglaw, 'k:', lw=1.5, label=f'Log-Law ($\\\\kappa={kappa}, E={E_wall_plot}$)')\n",
    "        ax1.semilogx(y_plus_theory_vis, u_plus_viscous, 'k--', lw=1.5, label='Viscous ($U^+=y^+$)')\n",
    "        ax1.set_xlabel('$y^+$')\n",
    "        ax1.set_ylabel('$U^+$')\n",
    "        ax1.set_title(f'$U^+$ vs $y^+$ (x={actual_x_pinn:.2f}m)')\n",
    "        ax1.legend(fontsize=9)\n",
    "        ax1.grid(True, which='both', ls=':')\n",
    "        ax1.set_ylim(bottom=0, top=u_plus_max_plot)\n",
    "        ax1.set_xlim(left=0.1, right=y_plus_max_plot) # Start x-axis slightly > 0 for log scale\n",
    "\n",
    "\n",
    "        # 2. k+ vs y+ plot\n",
    "        ax2 = plt.subplot(1, 3, 2)\n",
    "        ax2.semilogx(y_plus_pinn, k_plus_pinn, 'r.', ms=4, label='PINN')\n",
    "        if y_plus_ref is not None and k_plus_ref is not None:\n",
    "            ax2.semilogx(y_plus_ref, k_plus_ref, 'bo', mfc='none', ms=5, label='Reference')\n",
    "        # Add line for target y+ location\n",
    "        ax2.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'Target $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
    "        ax2.set_xlabel('$y^+$')\n",
    "        ax2.set_ylabel('$k^+$')\n",
    "        ax2.set_title('$k^+$ vs $y^+$')\n",
    "        ax2.legend(fontsize=9)\n",
    "        ax2.grid(True, which='both', ls=':')\n",
    "        ax2.set_ylim(bottom=0, top=k_plus_max_plot) # k+ starts from 0\n",
    "        ax2.set_xlim(left=0.1, right=y_plus_max_plot)\n",
    "\n",
    "\n",
    "        # 3. eps+ vs y+ plot (log-log scale often used)\n",
    "        ax3 = plt.subplot(1, 3, 3)\n",
    "        ax3.loglog(y_plus_pinn, eps_plus_pinn, 'r.', ms=4, label='PINN')\n",
    "        if y_plus_ref is not None and eps_plus_ref is not None:\n",
    "            ax3.loglog(y_plus_ref, eps_plus_ref, 'bo', mfc='none', ms=5, label='Reference')\n",
    "        # Theoretical trend near wall: eps+ ~ 1 / (kappa * y+) -> C / y+\n",
    "        y_plus_theory_eps = np.logspace(np.log10(max(1,0.1)), np.log10(y_plus_max_plot*1.1), 100)\n",
    "        # Adjust constant C for better visual fit if needed, 1/kappa is theoretical\n",
    "        eps_plus_target_theory = 1.0 / (kappa * y_plus_theory_eps)\n",
    "        ax3.loglog(y_plus_theory_eps, eps_plus_target_theory, 'k:', lw=1.5, label='$\\\\epsilon^+ \\\\propto 1/y^+$')\n",
    "        ax3.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'Target $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
    "        ax3.set_xlabel('$y^+$')\n",
    "        ax3.set_ylabel('$\\\\epsilon^+$')\n",
    "        ax3.set_title('$\\\\epsilon^+$ vs $y^+$ (log-log)')\n",
    "        ax3.legend(fontsize=9)\n",
    "        ax3.grid(True, which='both', ls=':')\n",
    "        # Set reasonable y-limits for eps+\n",
    "        ax3.set_ylim(bottom=max(min_eps_plus_data * 0.5, 1e-5), top=max_eps_plus_data * 2)\n",
    "        ax3.set_xlim(left=0.1, right=y_plus_max_plot)\n",
    "\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
    "        plt.suptitle(f'Wall Unit Profiles Comparison (Top Wall, x  {actual_x_pinn:.2f}m)', fontsize=16)\n",
    "        plt.subplots_adjust(top=0.88) # Adjust top margin for suptitle\n",
    "\n",
    "        save_path = os.path.join(self.plots_dir, \"profile_comparison_wall_units.png\")\n",
    "        try:\n",
    "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Wall unit comparison plots saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save wall unit comparison plot: {e}\")\n",
    "\n",
    "    def plot_pressure_gradient_comparison(self):\n",
    "        \"\"\"Plots the streamwise pressure gradient dp/dx along the centerline.\"\"\"\n",
    "        if self.p_pred is None:\n",
    "            logging.warning(\"PINN pressure data unavailable. Skipping pressure gradient plot.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Generating centerline pressure gradient comparison plot...\")\n",
    "        x_coords_pinn = self.X_grid[0, :] # Streamwise coordinates\n",
    "        y_coords_pinn = self.Y_grid[:, 0] # Transverse coordinates\n",
    "        # Find index closest to centerline y=0\n",
    "        center_idx_pinn = np.argmin(np.abs(y_coords_pinn - 0.0))\n",
    "        actual_y_center = y_coords_pinn[center_idx_pinn]\n",
    "\n",
    "        # Extract PINN kinematic pressure along centerline\n",
    "        p_centerline_pinn = self.p_pred[center_idx_pinn, :]\n",
    "        # Calculate gradient using numpy.gradient\n",
    "        dp_dx_pinn = np.gradient(p_centerline_pinn, x_coords_pinn)\n",
    "\n",
    "        # --- Calculate Reference Pressure Gradient (if data available) ---\n",
    "        dp_dx_ref = None\n",
    "        x_coords_ref = None\n",
    "        if self.has_ref_data and 'p_ref' in self.ref_data.columns:\n",
    "            try:\n",
    "                # Filter reference data near centerline (allow some tolerance)\n",
    "                ref_centerline = self.ref_data[\n",
    "                    np.isclose(self.ref_data['y'], 0.0, atol=0.02 * self.config.CHANNEL_HALF_HEIGHT)\n",
    "                ].copy()\n",
    "                ref_centerline.sort_values(by='x', inplace=True)\n",
    "\n",
    "                if len(ref_centerline) > 1:\n",
    "                    # If multiple y-values close to center, average pressure at each x\n",
    "                    centerline_grouped = ref_centerline.groupby('x')['p_ref'].mean()\n",
    "                    if len(centerline_grouped) > 5: # Need sufficient points for gradient\n",
    "                        x_coords_ref = centerline_grouped.index.values\n",
    "                        p_centerline_ref = centerline_grouped.values\n",
    "                        if len(x_coords_ref) > 1:\n",
    "                            dp_dx_ref = np.gradient(p_centerline_ref, x_coords_ref)\n",
    "                            logging.info(f\"Calculated reference pressure gradient from {len(x_coords_ref)} centerline points.\")\n",
    "                        else: logging.warning(\"Not enough unique x-points in reference centerline data for gradient.\")\n",
    "                    else: logging.warning(\"Not enough grouped x-points in reference centerline data for gradient.\")\n",
    "                else: logging.warning(\"Not enough points near centerline in reference data for gradient.\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not calculate reference pressure gradient: {e}\")\n",
    "        elif self.has_ref_data:\n",
    "            logging.warning(\"Reference data loaded, but 'p_ref' column missing. Cannot plot reference pressure gradient.\")\n",
    "\n",
    "        # --- Create Plot ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_coords_pinn, dp_dx_pinn, 'r-', lw=2, label='PINN $dp/dx$')\n",
    "        if dp_dx_ref is not None and x_coords_ref is not None:\n",
    "            plt.plot(x_coords_ref, dp_dx_ref, 'b--', lw=1.5, label='Reference $dp/dx$ (CSV)')\n",
    "\n",
    "        plt.xlabel('x (m)')\n",
    "        plt.ylabel('$dp/dx$ $(m/s^2)$') # Assuming kinematic pressure p\n",
    "        plt.title(f'Streamwise Kinematic Pressure Gradient along Centerline (y  {actual_y_center:.3f}m)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, ls=':')\n",
    "\n",
    "        # Optional: Set y-limits based on expected range (often negative and near constant)\n",
    "        try:\n",
    "           # Focus on the developed region (e.g., middle half)\n",
    "           focus_start_idx = len(dp_dx_pinn) // 4\n",
    "           focus_end_idx = -len(dp_dx_pinn) // 4\n",
    "           if focus_start_idx < focus_end_idx : # Ensure valid slice\n",
    "                focus_region_pinn = dp_dx_pinn[focus_start_idx:focus_end_idx]\n",
    "                if len(focus_region_pinn) > 0:\n",
    "                     mean_dpdx = np.mean(focus_region_pinn)\n",
    "                     std_dpdx = np.std(focus_region_pinn)\n",
    "                     # Set ylim to mean +/- a few std deviations, or fixed range\n",
    "                     plt.ylim(mean_dpdx - 4*max(std_dpdx, abs(mean_dpdx)*0.1), mean_dpdx + 4*max(std_dpdx, abs(mean_dpdx)*0.1))\n",
    "        except Exception:\n",
    "             pass # Ignore errors in ylim setting\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.plots_dir, \"pressure_gradient_comparison.png\")\n",
    "        try:\n",
    "            plt.savefig(save_path, dpi=200)\n",
    "            plt.close()\n",
    "            logging.info(f\"Pressure gradient comparison plot saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save pressure gradient plot: {e}\")\n",
    "\n",
    "    def run_post_processing(self):\n",
    "        \"\"\"Runs the full post-processing sequence.\"\"\"\n",
    "        logging.info(\"--- Starting Full Post-Processing ---\")\n",
    "        self.plot_loss_history()\n",
    "\n",
    "        # Prediction is required for all field plots\n",
    "        prediction_successful = self.predict_pinn_fields()\n",
    "\n",
    "        if prediction_successful:\n",
    "            self.plot_contour_fields()\n",
    "            # Load reference data only if prediction was successful\n",
    "            self.load_reference_data()\n",
    "            if self.has_ref_data:\n",
    "                logging.info(\"Proceeding with PINN vs Reference CSV comparisons...\")\n",
    "                self.plot_profile_comparison()\n",
    "                self.plot_wall_unit_comparison()\n",
    "                self.plot_pressure_gradient_comparison()\n",
    "            else:\n",
    "                logging.warning(\"Skipping comparison plots as reference data is unavailable or failed to load.\")\n",
    "        else:\n",
    "            logging.error(\"PINN field prediction failed. Aborting further post-processing.\")\n",
    "\n",
    "        logging.info(\"--- Post-Processing Finished ---\")\n",
    "# --- End Plotter Class ---\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ===== Main Execution Block =====\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    main_start_time = time.time()\n",
    "\n",
    "    # --- 1. Initial Setup ---\n",
    "    main_cfg = Config() # Instantiate default config\n",
    "    main_plot_cfg = PlotterConfig()\n",
    "\n",
    "    # --- Google Drive Mount (if applicable) ---\n",
    "    # This will update main_cfg paths if running in Colab and mount is successful\n",
    "    mount_drive(main_cfg.DRIVE_MOUNT_POINT)\n",
    "\n",
    "    # --- Setup Output Dirs and Logging ---\n",
    "    # These must run AFTER mount_drive potentially updates main_cfg.OUTPUT_DIR etc.\n",
    "    setup_output_directories(main_cfg)\n",
    "    setup_logging(main_cfg.LOG_FILE) # Configure logging\n",
    "\n",
    "    logging.info(\"=\"*60); logging.info(\" PINN RANS k-epsilon Channel Flow Simulation Start \"); logging.info(\"=\"*60)\n",
    "    log_configuration(main_cfg, main_plot_cfg) # Log the final configuration used\n",
    "\n",
    "    # --- 2. Define Boundaries ---\n",
    "    try:\n",
    "        # Pass the config object to the function\n",
    "        bcs, anchor_points = get_boundary_conditions(main_cfg)\n",
    "        logging.info(f\"Defined {len(bcs)} boundary conditions.\")\n",
    "        if anchor_points is not None and len(anchor_points) > 0:\n",
    "            logging.info(f\"Using {anchor_points.shape[0]} anchor points for wall functions.\")\n",
    "        else:\n",
    "             # Should have anchor points if using wall functions as defined\n",
    "             logging.warning(\"No anchor points generated for wall functions, check get_boundary_conditions.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to define boundary conditions: {e}\", exc_info=True)\n",
    "        sys.exit(1) # Exit if BCs cannot be defined\n",
    "\n",
    "    # --- 3. Training Phase ---\n",
    "    model_trained, history_trained, state_trained = None, None, None\n",
    "    training_successful = False\n",
    "    try:\n",
    "        # Pass the config object to the Trainer\n",
    "        trainer = Trainer(main_cfg)\n",
    "        trainer.build_model(bcs, anchor_points)\n",
    "        # Execute the updated training method\n",
    "        model_trained, history_trained, state_trained = trainer.train()\n",
    "\n",
    "        # Check if training outputs seem valid\n",
    "        # Basic check: ensure they are not None\n",
    "        if model_trained is not None and history_trained is not None and state_trained is not None:\n",
    "             # Could add more checks, e.g., on final loss\n",
    "             training_successful = True\n",
    "             logging.info(\"Training phase returned valid model, history, and state objects.\")\n",
    "        else:\n",
    "             logging.error(\"Training phase finished but returned an invalid state (model, losshistory, or train_state is None).\")\n",
    "\n",
    "    except Exception as e:\n",
    "         logging.error(f\"A critical error occurred during model building or the training phase: {e}\", exc_info=True)\n",
    "         # model_trained, etc., will remain None, training_successful will be False\n",
    "\n",
    "    # --- 4. Post-processing and Plotting Phase ---\n",
    "    if training_successful:\n",
    "        logging.info(\"Proceeding to post-processing.\")\n",
    "        try:\n",
    "            # Pass the final trained state to the plotter\n",
    "            plotter = Plotter(main_cfg, main_plot_cfg, model_trained, history_trained, state_trained)\n",
    "            plotter.run_post_processing()\n",
    "        except Exception as e:\n",
    "             logging.error(f\"An error occurred during post-processing: {e}\", exc_info=True)\n",
    "    else:\n",
    "        # This message will be logged if training failed, returned None, or hit a critical error\n",
    "        logging.error(\"Training did not complete successfully or produced an invalid state. Skipping post-processing.\")\n",
    "\n",
    "    main_end_time = time.time()\n",
    "    logging.info(\"=\"*60); logging.info(f\" Script Execution Finished in {main_end_time - main_start_time:.2f} seconds\"); logging.info(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
