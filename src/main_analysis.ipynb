{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: write the code to mount the gdrive and list the files and folders in the drive , list also using the os\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List files and folders using Google Colab's drive API\n",
        "!ls '/content/drive/My Drive'\n",
        "\n",
        "# List files and folders using the os module\n",
        "print(\"\\nListing files and folders using os module:\")\n",
        "for root, dirs, files in os.walk('/content/drive/My Drive'):\n",
        "    level = root.replace('/content/drive/My Drive', '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print('{}{}'.format(subindent, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQJp_3HAtpSN",
        "outputId": "37eb09c5-dcee-4fad-c98a-cde6055b382e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'Colab Notebooks'   PINN_RANS_ChannelFlow\n",
            "\n",
            "Listing files and folders using os module:\n",
            "My Drive/\n",
            "    Colab Notebooks/\n",
            "        Untitled0.ipynb\n",
            "        Untitled1.ipynb\n",
            "    PINN_RANS_ChannelFlow/\n",
            "        model_checkpoints/\n",
            "            rans_channel_wf-30000.pt-17000.pt\n",
            "            rans_channel_wf-49000.pt-8000.pt\n",
            "            rans_channel_wf.ckpt-7000.pt\n",
            "            rans_channel_wf-27000.pt-14000.pt\n",
            "            rans_channel_wf-40000.pt-27000.pt\n",
            "            rans_channel_wf-33000.pt-20000.pt\n",
            "            rans_channel_wf.ckpt-6000.pt\n",
            "            rans_channel_wf.ckpt-10000.pt\n",
            "            rans_channel_wf-20000.pt-7000.pt\n",
            "            rans_channel_wf-42000.pt-1000.pt\n",
            "            rans_channel_wf-43000.pt-2000.pt\n",
            "            rans_channel_wf-15000.pt-2000.pt\n",
            "            rans_channel_wf-50000.pt-9000.pt\n",
            "            rans_channel_wf.ckpt-9000.pt\n",
            "            rans_channel_wf.ckpt-2000.pt\n",
            "            rans_channel_wf-24000.pt-11000.pt\n",
            "            rans_channel_wf-47000.pt-6000.pt\n",
            "            rans_channel_wf-14000.pt-1000.pt\n",
            "            rans_channel_wf-46000.pt-5000.pt\n",
            "            rans_channel_wf-44000.pt-3000.pt\n",
            "            rans_channel_wf.ckpt-1000.pt\n",
            "            rans_channel_wf-48000.pt-7000.pt\n",
            "            rans_channel_wf-37000.pt-24000.pt\n",
            "            rans_channel_wf.ckpt-8000.pt\n",
            "            rans_channel_wf.ckpt-3000.pt\n",
            "            rans_channel_wf-39000.pt-26000.pt\n",
            "            rans_channel_wf-38000.pt-25000.pt\n",
            "            rans_channel_wf.ckpt-5000.pt\n",
            "            rans_channel_wf-13000.pt\n",
            "            rans_channel_wf-45000.pt-4000.pt\n",
            "            rans_channel_wf.ckpt-11000.pt\n",
            "            rans_channel_wf.ckpt-12000.pt\n",
            "            rans_channel_wf-28000.pt-15000.pt\n",
            "            rans_channel_wf-17000.pt-4000.pt\n",
            "            rans_channel_wf-29000.pt-16000.pt\n",
            "            rans_channel_wf-34000.pt-21000.pt\n",
            "            rans_channel_wf-25000.pt-12000.pt\n",
            "            rans_channel_wf-35000.pt-22000.pt\n",
            "            rans_channel_wf-36000.pt-23000.pt\n",
            "            rans_channel_wf-22000.pt-9000.pt\n",
            "            rans_channel_wf-{step}.pt-1000.pt\n",
            "            rans_channel_wf-21000.pt-8000.pt\n",
            "            rans_channel_wf-26000.pt-13000.pt\n",
            "            rans_channel_wf-18000.pt-5000.pt\n",
            "            rans_channel_wf.ckpt-4000.pt\n",
            "            rans_channel_wf-23000.pt-10000.pt\n",
            "            rans_channel_wf-16000.pt-3000.pt\n",
            "            rans_channel_wf-41000.pt\n",
            "            rans_channel_wf-19000.pt-6000.pt\n",
            "            rans_channel_wf-51000.pt-10000.pt\n",
            "            rans_channel_wf-31000.pt-18000.pt\n",
            "            rans_channel_wf-32000.pt-19000.pt\n",
            "            rans_channel_wf-53000.pt-12000.pt\n",
            "            rans_channel_wf-54000.pt\n",
            "            rans_channel_wf-52000.pt-11000.pt\n",
            "            rans_channel_wf-59000.pt-5000.pt\n",
            "            rans_channel_wf-60000.pt-6000.pt\n",
            "            rans_channel_wf-61000.pt-7000.pt\n",
            "            rans_channel_wf-62000.pt-8000.pt\n",
            "            rans_channel_wf-63000.pt-9000.pt\n",
            "            rans_channel_wf-64000.pt-10000.pt\n",
            "            rans_channel_wf-65000.pt-11000.pt\n",
            "            rans_channel_wf-66000.pt-12000.pt\n",
            "            rans_channel_wf-67000.pt-13000.pt\n",
            "            rans_channel_wf-68000.pt-14000.pt\n",
            "            rans_channel_wf-69000.pt\n",
            "            .ipynb_checkpoints/\n",
            "        logs/\n",
            "            training_log.log\n",
            "            plotting_log.log\n",
            "        plots/\n",
            "            loss.dat\n",
            "            train.dat\n",
            "            test.dat\n",
            "            pinn_field_contours.png\n",
            "            profile_comparison_pinn_vs_csv.png\n",
            "            profile_comparison_wall_units.png\n",
            "            pressure_gradient_comparison.png\n",
            "        data/\n",
            "            reference_output_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqfBkYZVtQtB",
        "outputId": "5e308f72-ce09-4d2b-e0da-79052e568093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relied on environment variable DDE_BACKEND=pytorch.\n",
            "DeepXDE Backend requested: pytorch\n",
            "DeepXDE Backend actual: pytorch\n",
            "CUDA available.\n",
            "PyTorch CUDA device detected by DDE: 0 (Tesla T4)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Number of GPUs: 1\n",
            "PyTorch default dtype: torch.float32\n",
            "2025-04-14 20:39:08 [INFO] Google Drive already mounted.\n",
            "2025-04-14 20:39:08 [INFO] Output paths point to Google Drive: /content/drive/MyDrive/content/drive/MyDrive/PINN_RANS_ChannelFlow\n",
            "2025-04-14 20:39:08 [INFO] Setting up output directories...\n",
            "2025-04-14 20:39:08 [INFO] Output directories verified/created.\n",
            "2025-04-14 20:39:08 [INFO] Logging configured.\n",
            "2025-04-14 20:39:08 [INFO] ============================================================\n",
            "2025-04-14 20:39:08 [INFO]  PINN RANS k-epsilon Channel Flow Simulation Start \n",
            "2025-04-14 20:39:08 [INFO] ============================================================\n",
            "2025-04-14 20:39:08 [INFO] ==================================================\n",
            "2025-04-14 20:39:08 [INFO] Simulation Configuration:\n",
            "2025-04-14 20:39:08 [INFO]   Output Directory: /content/drive/MyDrive/PINN_RANS_ChannelFlow\n",
            "2025-04-14 20:39:08 [INFO]   Re_H: 10000\n",
            "2025-04-14 20:39:08 [INFO]   Wall Function y_p: 0.04 (Target y+: 14.00)\n",
            "2025-04-14 20:39:08 [INFO]   Network: 8 layers, 64 neurons\n",
            "2025-04-14 20:39:08 [INFO]   Inlet k (log): -5.5860, Inlet eps (log): -7.5257\n",
            "2025-04-14 20:39:08 [INFO]   Target WF U: 0.8402, k (log): -4.1145, eps (log): -3.8673\n",
            "2025-04-14 20:39:08 [INFO]   Adam Iterations: 50000, LR: 0.001\n",
            "2025-04-14 20:39:08 [INFO]   L-BFGS Iterations: 20000\n",
            "2025-04-14 20:39:08 [INFO]   Checkpoint Interval: 1000\n",
            "2025-04-14 20:39:08 [INFO]   Reference Data File (CSV): /content/drive/MyDrive/PINN_RANS_ChannelFlow/data/reference_output_data.csv\n",
            "2025-04-14 20:39:08 [INFO] Plotter Configuration:\n",
            "2025-04-14 20:39:08 [INFO]   Prediction Grid Nx: 200, Ny: 100\n",
            "2025-04-14 20:39:08 [INFO] ==================================================\n",
            "2025-04-14 20:39:08 [INFO] Generated 400 anchor points for wall functions.\n",
            "2025-04-14 20:39:08 [INFO] Defined 10 boundary conditions.\n",
            "2025-04-14 20:39:08 [INFO] Building the PINN model...\n",
            "Warning: 5000 points required, but 5088 points sampled.\n",
            "2025-04-14 20:39:09 [INFO] Using 400 anchor points for wall functions (passed to anchors).\n",
            "2025-04-14 20:39:09 [INFO] Model built successfully.\n",
            "2025-04-14 20:39:09 [INFO] Checkpoint filename base: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-\n",
            "2025-04-14 20:39:09 [INFO] Searching for checkpoints in: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints\n",
            "2025-04-14 20:39:09 [INFO] Using pattern: ^rans_channel_wf\\-(\\d+)\\.pt$\n",
            "2025-04-14 20:39:09 [INFO] Found latest valid checkpoint: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-70000.pt at step 70000\n",
            "2025-04-14 20:39:09 [INFO] Restored step (70000) >= Adam iterations (50000). Will compile with L-BFGS initially.\n",
            "2025-04-14 20:39:09 [INFO] Compiling model with L-BFGS optimizer initially.\n",
            "Compiling model...\n",
            "'compile' took 0.000639 s\n",
            "\n",
            "2025-04-14 20:39:09 [INFO] Model compiled with L-BFGS.\n",
            "2025-04-14 20:39:09 [INFO] Explicitly restoring model state from: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-70000.pt\n",
            "Restoring model from /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-70000.pt ...\n",
            "\n",
            "2025-04-14 20:39:09 [INFO] Model state restored. Internal DDE step count *after* restore: 0\n",
            "2025-04-14 20:39:09 [WARNING] Mismatch between expected restored step (70000) from filename and DDE internal step (0) after restore! Forcing DDE step to match filename.\n",
            "2025-04-14 20:39:09 [INFO] Internal step count after manual setting: 70000\n",
            "2025-04-14 20:39:09 [INFO] Skipping Adam phase as initial compilation was L-BFGS.\n",
            "2025-04-14 20:39:09 [INFO] Current step (70000) meets or exceeds total configured steps (70000). Skipping further training.\n",
            "2025-04-14 20:39:09 [INFO] Performing PyTorch-based post-training checks...\n",
            "2025-04-14 20:39:09 [INFO] Checking turbulence production term Pk...\n",
            "2025-04-14 20:39:09 [INFO] Turbulence production check passed (min P_k = 1.085e-12, max P_k = 2.012e+00)\n",
            "2025-04-14 20:39:09 [INFO] Training sequence complete.\n",
            "2025-04-14 20:39:09 [ERROR] Training phase finished but returned an invalid state (model, losshistory, or train_state is None).\n",
            "2025-04-14 20:39:09 [ERROR] Training did not complete successfully or produced an invalid state. Skipping post-processing.\n",
            "2025-04-14 20:39:09 [INFO] ============================================================\n",
            "2025-04-14 20:39:09 [INFO]  Script Execution Finished in 1.19 seconds\n",
            "2025-04-14 20:39:09 [INFO] ============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set environment variable *before* importing deepxde or torch\n",
        "os.environ[\"DDE_BACKEND\"] = \"pytorch\"\n",
        "try:\n",
        "  import torch\n",
        "except ImportError:\n",
        "  print(\"Installing torch...\")\n",
        "  !pip install torch -q\n",
        "try:\n",
        "  import deepxde\n",
        "except ImportError:\n",
        "  print(\"Installing deepxde...\")\n",
        "  !pip install deepxde -q\n",
        "try:\n",
        "  import pandas\n",
        "except ImportError:\n",
        "  print(\"Installing pandas...\")\n",
        "  !pip install pandas -q\n",
        "try:\n",
        "  import matplotlib\n",
        "except ImportError:\n",
        "  print(\"Installing matplotlib...\")\n",
        "  !pip install matplotlib -q\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch # Now import torch after potentially installing\n",
        "import deepxde as dde # Import deepxde after setting backend\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.interpolate import griddata\n",
        "import re # <<<--- IMPORT REGEX MODULE\n",
        "\n",
        "\n",
        "# --- Attempt to explicitly set backend (optional but good practice) ---\n",
        "try:\n",
        "    # This might still fail on older DeepXDE versions, but the env var is primary\n",
        "    dde.config.set_default_backend(\"pytorch\")\n",
        "    print(\"Attempted to explicitly set DeepXDE backend to PyTorch.\")\n",
        "except AttributeError:\n",
        "    print(f\"Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relied on environment variable DDE_BACKEND={os.environ.get('DDE_BACKEND')}.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not explicitly set backend via dde.config: {e}\")\n",
        "\n",
        "print(f\"DeepXDE Backend requested: {os.environ.get('DDE_BACKEND', 'Not Set')}\")\n",
        "\n",
        "# --- Check actual backend and setup device/dtype ---\n",
        "if \"deepxde\" in sys.modules and hasattr(dde, 'backend'):\n",
        "    print(f\"DeepXDE Backend actual: {dde.backend.backend_name}\")\n",
        "    if dde.backend.backend_name == \"pytorch\":\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"CUDA available.\")\n",
        "            try:\n",
        "                # Use float32 as it's common for PINNs and avoids potential double precision issues\n",
        "                torch.set_default_dtype(torch.float32)\n",
        "                current_device = torch.cuda.current_device()\n",
        "                print(f\"PyTorch CUDA device detected by DDE: {current_device} ({torch.cuda.get_device_name(current_device)})\")\n",
        "                print(f\"PyTorch version: {torch.__version__}\")\n",
        "                print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error during PyTorch device setup: {e}\")\n",
        "        else:\n",
        "            print(\"CUDA not available. Using CPU.\")\n",
        "            try:\n",
        "                torch.set_default_dtype(torch.float32)\n",
        "                print(f\"PyTorch default device set to: cpu\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to set default PyTorch device to CPU: {e}\")\n",
        "        print(f\"PyTorch default dtype: {torch.get_default_dtype()}\")\n",
        "    else:\n",
        "        print(f\"Warning: Backend is '{dde.backend.backend_name}', not PyTorch. PyTorch-specific device setup skipped.\")\n",
        "else:\n",
        "    print(\"Warning: deepxde module or dde.backend not fully available for backend check.\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "# ===== Configuration Classes =====\n",
        "# =============================\n",
        "\n",
        "class PlotterConfig:\n",
        "    \"\"\"Stores configuration parameters specifically for plotting.\"\"\"\n",
        "    NX_PRED = 200\n",
        "    NY_PRED = 100\n",
        "    CMAP_VELOCITY = 'viridis'\n",
        "    CMAP_PRESSURE = 'coolwarm'\n",
        "    CMAP_TURBULENCE = 'plasma'\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Stores configuration parameters for the simulation.\"\"\"\n",
        "    DRIVE_MOUNT_POINT = '/content/drive'\n",
        "      # Adjust path if necessary\n",
        "    GDRIVE_BASE_FOLDER = '/content/drive/MyDrive/PINN_RANS_ChannelFlow'\n",
        "    OUTPUT_DIR = GDRIVE_BASE_FOLDER # Default output dir (can be overwritten if not on Colab)\n",
        "    MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_checkpoints\")\n",
        "    LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
        "    PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
        "    DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
        "    LOG_FILE = os.path.join(LOG_DIR, \"training_log.log\")\n",
        "    REFERENCE_DATA_FILE = os.path.join(DATA_DIR, \"reference_output_data.csv\")\n",
        "\n",
        "    # Checkpoint filename base (without step number or extension)\n",
        "    CHECKPOINT_FILENAME_BASE  = \"rans_channel_wf\"\n",
        "\n",
        "    # --- Fluid and Geometry Parameters ---\n",
        "    NU = 0.0002 # Kinematic viscosity\n",
        "    RHO = 1.0 # Density (often set to 1 for incompressible flow)\n",
        "    MU = RHO * NU # Dynamic viscosity\n",
        "    U_INLET = 1.0 # Inlet velocity\n",
        "    H = 2.0 # Full channel height\n",
        "    CHANNEL_HALF_HEIGHT = H / 2.0\n",
        "    L = 10.0 # Channel length\n",
        "    RE_H = U_INLET * H / NU # Reynolds number based on full height\n",
        "    EPS_SMALL = 1e-10 # Small epsilon for numerical stability (avoid log(0), division by zero)\n",
        "\n",
        "    # --- k-epsilon Model Constants ---\n",
        "    CMU = 0.09\n",
        "    CEPS1 = 1.44\n",
        "    CEPS2 = 1.92\n",
        "    SIGMA_K = 1.0\n",
        "    SIGMA_EPS = 1.3\n",
        "    KAPPA = 0.41 # Von Karman constant\n",
        "\n",
        "    # --- Wall Function Parameters ---\n",
        "    E_WALL = 9.8 # Log-law constant for smooth walls\n",
        "    Y_P = 0.04 # Distance from wall for applying wall functions (y_p)\n",
        "    # Target values for deriving wall function BCs (can be based on desired Re_tau)\n",
        "    RE_TAU_TARGET = 350 # Target friction Reynolds number\n",
        "    U_TAU_TARGET = RE_TAU_TARGET * NU / CHANNEL_HALF_HEIGHT # Target friction velocity\n",
        "    YP_PLUS_TARGET = Y_P * U_TAU_TARGET / NU # Target y+ at y_p\n",
        "    # Target values at y_p based on log-law and turbulence equilibrium\n",
        "    U_TARGET_WF = (U_TAU_TARGET / KAPPA) * np.log(max(E_WALL * YP_PLUS_TARGET, EPS_SMALL))\n",
        "    K_TARGET_WF = U_TAU_TARGET**2 / np.sqrt(CMU)\n",
        "    EPS_TARGET_WF = U_TAU_TARGET**3 / max(KAPPA * Y_P, EPS_SMALL)\n",
        "\n",
        "    # --- Inlet Turbulence Parameters ---\n",
        "    TURBULENCE_INTENSITY = 0.05 # Typical value for channel flow inlet\n",
        "    MIXING_LENGTH_SCALE = 0.07 * CHANNEL_HALF_HEIGHT # Estimate based on boundary layer thickness\n",
        "    # Inlet k and epsilon based on intensity and length scale\n",
        "    K_INLET = 1.5 * (U_INLET * TURBULENCE_INTENSITY)**2\n",
        "    EPS_INLET = (CMU**0.75) * (K_INLET**1.5) / MIXING_LENGTH_SCALE\n",
        "    # Transformed values (log) for network prediction/BCs\n",
        "    K_INLET_TRANSFORMED = np.log(max(K_INLET, EPS_SMALL))\n",
        "    EPS_INLET_TRANSFORMED = np.log(max(EPS_INLET, EPS_SMALL))\n",
        "    K_TARGET_WF_TRANSFORMED = np.log(max(K_TARGET_WF, EPS_SMALL))\n",
        "    EPS_TARGET_WF_TRANSFORMED = np.log(max(EPS_TARGET_WF, EPS_SMALL))\n",
        "\n",
        "    # --- Domain Geometry ---\n",
        "    GEOM = dde.geometry.Rectangle(xmin=[0, -CHANNEL_HALF_HEIGHT], xmax=[L, CHANNEL_HALF_HEIGHT])\n",
        "\n",
        "    # --- Network Architecture ---\n",
        "    NUM_LAYERS = 8\n",
        "    NUM_NEURONS = 64\n",
        "    ACTIVATION = \"tanh\"\n",
        "    INITIALIZER = \"Glorot normal\"\n",
        "    NETWORK_INPUTS = 2 # x, y\n",
        "    NETWORK_OUTPUTS = 5 # u, v, p', log(k), log(eps)\n",
        "\n",
        "    # --- Training Parameters ---\n",
        "    NUM_DOMAIN_POINTS = 20000\n",
        "    NUM_BOUNDARY_POINTS = 4000 # For physical boundaries (inlet, outlet, walls)\n",
        "    NUM_TEST_POINTS = 5000 # For evaluating PDE residuals during training/testing\n",
        "    NUM_WF_POINTS_PER_WALL = 200 # Anchor points for wall function BCs (per wall)\n",
        "    LEARNING_RATE_ADAM = 1e-3\n",
        "    ADAM_ITERATIONS = 50000 # Number of iterations for Adam optimizer\n",
        "    LBFGS_ITERATIONS = 20000 # Max iterations for L-BFGS optimizer\n",
        "    # Loss weights: [PDE_cont, PDE_mom_x, PDE_mom_y, PDE_k, PDE_eps, BC_u_in, BC_v_in, BC_k_in, BC_eps_in, BC_p_out, BC_u_wall, BC_v_wall, BC_u_wf, BC_k_wf, BC_eps_wf]\n",
        "    PDE_WEIGHTS = [1, 1, 1, 1, 1] # Weights for the 5 PDE residuals\n",
        "    BC_WEIGHTS = [10, 10, 10, 10, 10, 10, 10, 20, 20, 20] # Weights for the 10 BCs (adjust as needed)\n",
        "    LOSS_WEIGHTS = PDE_WEIGHTS + BC_WEIGHTS\n",
        "    SAVE_INTERVAL = 1000 # Checkpoint saving interval (steps)\n",
        "    DISPLAY_EVERY = 1000 # Loss display interval (steps)\n",
        "\n",
        "\n",
        "# Instantiate config objects\n",
        "cfg = Config()\n",
        "plotter_cfg = PlotterConfig()\n",
        "\n",
        "\n",
        "# ==============================================\n",
        "# ===== Custom Checkpoint Callback Class ========\n",
        "# ==============================================\n",
        "class CustomModelCheckpoint(dde.callbacks.Callback):\n",
        "    \"\"\"Custom checkpoint callback that saves based on global step.\"\"\"\n",
        "    def __init__(self, filepath_base, period, verbose=1):\n",
        "        super().__init__()\n",
        "        self.filepath_base = filepath_base # e.g., /path/to/model_checkpoints/rans_channel_wf-\n",
        "        self.period = period\n",
        "        self.verbose = verbose\n",
        "        self._saved_steps = set() # Tracks steps saved in the current trainer.train() call\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Check step at the end of each epoch.\"\"\"\n",
        "        self._save_checkpoint()\n",
        "\n",
        "    # on_batch_end can also be used for finer control if needed, but on_epoch_end is common\n",
        "    # def on_batch_end(self):\n",
        "    #     self._save_checkpoint()\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        \"\"\"Internal method to check step and save.\"\"\"\n",
        "        if not hasattr(self, 'model') or not self.model or not hasattr(self.model, 'train_state') or not self.model.train_state:\n",
        "            logging.debug(\"Model or train_state not ready for checkpoint.\")\n",
        "            return # Model not ready\n",
        "\n",
        "        if dde.backend.backend_name != \"pytorch\":\n",
        "             logging.warning(\"CustomModelCheckpoint requires PyTorch backend for model.save behavior.\")\n",
        "             return\n",
        "\n",
        "        # Access the global step count directly from train_state\n",
        "        step = self.model.train_state.step\n",
        "\n",
        "        # Check if the current step is a multiple of the saving period,\n",
        "        # is positive (avoid saving at step 0 unnecessarily),\n",
        "        # and hasn't already been saved during this train() call.\n",
        "        if step > 0 and step % self.period == 0 and step not in self._saved_steps:\n",
        "            filepath = f\"{self.filepath_base}{step}.pt\" # Construct filename using global step\n",
        "            if self.verbose > 0:\n",
        "                logging.info(f\"Step {step}: saving model to {filepath} ...\")\n",
        "            try:\n",
        "                # Use DeepXDE's save method which handles backend specifics\n",
        "                # Important: Pass save_optimizer_state=True if you need to restore the optimizer too (crucial for resuming)\n",
        "                # Pass save_best_only=False as we are saving periodically based on steps\n",
        "                self.model.save(filepath, verbose=0) # verbose=0 prevents double logging\n",
        "                self._saved_steps.add(step) # Mark this step as saved for this run\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error saving checkpoint at step {step} to {filepath}: {e}\", exc_info=True)\n",
        "\n",
        "# ==============================================\n",
        "# ===== END: Custom Checkpoint Callback ========\n",
        "# ==============================================\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# ===== Utility Functions =====\n",
        "# ==========================\n",
        "def setup_logging(log_file):\n",
        "    \"\"\"Configures logging to file and console.\"\"\"\n",
        "    log_dir = os.path.dirname(log_file)\n",
        "    ensure_dir(log_dir)\n",
        "    root_logger = logging.getLogger()\n",
        "    # Clear existing handlers to avoid duplicate messages if run multiple times in notebook\n",
        "    if root_logger.hasHandlers():\n",
        "        root_logger.handlers.clear()\n",
        "    log_formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    root_logger.setLevel(logging.INFO) # Set root logger level\n",
        "\n",
        "    # File handler\n",
        "    try:\n",
        "        file_handler = logging.FileHandler(log_file, mode='a') # Append mode\n",
        "        file_handler.setFormatter(log_formatter)\n",
        "        root_logger.addHandler(file_handler)\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up file logger at {log_file}: {e}\")\n",
        "\n",
        "\n",
        "    # Console handler\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "    root_logger.addHandler(console_handler)\n",
        "    logging.info(\"Logging configured.\")\n",
        "\n",
        "def ensure_dir(directory):\n",
        "    \"\"\"Creates a directory if it doesn't exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "            logging.info(f\"Created directory: {directory}\")\n",
        "        except OSError as e:\n",
        "            logging.error(f\"Failed to create directory {directory}: {e}\")\n",
        "\n",
        "\n",
        "def mount_drive(mount_point):\n",
        "    \"\"\"Mounts Google Drive if running in Colab.\"\"\"\n",
        "    if 'google.colab' in sys.modules:\n",
        "        if not os.path.exists(os.path.join(mount_point, 'MyDrive')):\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                logging.info(f\"Mounting Google Drive at {mount_point}...\")\n",
        "                drive.mount(mount_point, force_remount=True)\n",
        "                logging.info(\"Google Drive mounted successfully.\")\n",
        "                # Verify base folder access after mount\n",
        "                # IMPORTANT: Use os.path.join correctly. Assumes GDRIVE_BASE_FOLDER starts relative to MyDrive\n",
        "                gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER.lstrip('/')) # Remove leading / if present\n",
        "                cfg.OUTPUT_DIR = gdrive_output_path # IMPORTANT: Update config path\n",
        "                cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "                cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "                cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "                cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "                cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
        "                cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "                logging.info(f\"Output paths updated to Google Drive: {cfg.OUTPUT_DIR}\")\n",
        "                ensure_dir(cfg.OUTPUT_DIR) # Create base dir on GDrive if it doesn't exist\n",
        "                if os.path.exists(cfg.OUTPUT_DIR):\n",
        "                    logging.info(f\"Base folder exists: {cfg.OUTPUT_DIR}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Configured base folder NOT found after mount attempt: {cfg.OUTPUT_DIR}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error mounting Google Drive or accessing path: {e}\")\n",
        "                # Fallback to local directory if mount fails\n",
        "                logging.warning(\"Falling back to local directory structure.\")\n",
        "                cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER # Use base folder name locally\n",
        "                cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "                cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "                cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "                cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "                cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
        "                cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "        else:\n",
        "            logging.info(\"Google Drive already mounted.\")\n",
        "            # Still update paths if already mounted\n",
        "            gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER.lstrip('/'))\n",
        "            cfg.OUTPUT_DIR = gdrive_output_path # IMPORTANT: Update config path\n",
        "            cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "            cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "            cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "            cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "            cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
        "            cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "            logging.info(f\"Output paths point to Google Drive: {cfg.OUTPUT_DIR}\")\n",
        "    else:\n",
        "        logging.info(\"Not running in Google Colab. Using local directory structure.\")\n",
        "        # Ensure local paths are based on the script location or CWD\n",
        "        cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER # Use base folder name locally\n",
        "        cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "        cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "        cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "        cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "        cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"training_log.log\")\n",
        "        cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "\n",
        "\n",
        "def setup_output_directories(config):\n",
        "    \"\"\"Creates all necessary output directories.\"\"\"\n",
        "    logging.info(\"Setting up output directories...\")\n",
        "    ensure_dir(config.OUTPUT_DIR)\n",
        "    ensure_dir(config.MODEL_DIR)\n",
        "    ensure_dir(config.LOG_DIR)\n",
        "    ensure_dir(config.PLOT_DIR)\n",
        "    ensure_dir(config.DATA_DIR)\n",
        "    logging.info(\"Output directories verified/created.\")\n",
        "\n",
        "def log_configuration(config, plotter_config):\n",
        "    \"\"\"Logs the simulation and plotter configuration.\"\"\"\n",
        "    logging.info(\"=\" * 50)\n",
        "    logging.info(\"Simulation Configuration:\")\n",
        "    logging.info(f\"  Output Directory: {config.OUTPUT_DIR}\")\n",
        "    logging.info(f\"  Re_H: {config.RE_H:.0f}\")\n",
        "    logging.info(f\"  Wall Function y_p: {config.Y_P} (Target y+: {config.YP_PLUS_TARGET:.2f})\")\n",
        "    logging.info(f\"  Network: {config.NUM_LAYERS} layers, {config.NUM_NEURONS} neurons\")\n",
        "    logging.info(f\"  Inlet k (log): {config.K_INLET_TRANSFORMED:.4f}, Inlet eps (log): {config.EPS_INLET_TRANSFORMED:.4f}\")\n",
        "    logging.info(f\"  Target WF U: {config.U_TARGET_WF:.4f}, k (log): {config.K_TARGET_WF_TRANSFORMED:.4f}, eps (log): {config.EPS_TARGET_WF_TRANSFORMED:.4f}\")\n",
        "    logging.info(f\"  Adam Iterations: {config.ADAM_ITERATIONS}, LR: {config.LEARNING_RATE_ADAM}\")\n",
        "    logging.info(f\"  L-BFGS Iterations: {config.LBFGS_ITERATIONS}\")\n",
        "    logging.info(f\"  Checkpoint Interval: {config.SAVE_INTERVAL}\")\n",
        "    logging.info(f\"  Reference Data File (CSV): {config.REFERENCE_DATA_FILE}\")\n",
        "    logging.info(\"Plotter Configuration:\")\n",
        "    logging.info(f\"  Prediction Grid Nx: {plotter_config.NX_PRED}, Ny: {plotter_config.NY_PRED}\")\n",
        "    logging.info(\"=\" * 50)\n",
        "# --- End Utility Functions ---\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ===== PDE System Definition =====\n",
        "# ===============================\n",
        "def pde(x, y, config): # Pass config explicitly\n",
        "    \"\"\"Defines the RANS k-epsilon PDE system.\"\"\"\n",
        "    # Ensure backend is PyTorch as autograd syntax is used\n",
        "    if dde.backend.backend_name != \"pytorch\":\n",
        "        raise RuntimeError(\"PDE function relies on PyTorch autograd. Backend mismatch.\")\n",
        "\n",
        "    nu = config.NU; Cmu = config.CMU; Ceps1 = config.CEPS1; Ceps2 = config.CEPS2\n",
        "    sigma_k = config.SIGMA_K; sigma_eps = config.SIGMA_EPS; eps_small = config.EPS_SMALL\n",
        "\n",
        "    # Network outputs: u, v, p', log(k), log(eps)\n",
        "    u, v, p_prime, k_raw, eps_raw = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4], y[:, 4:5]\n",
        "\n",
        "    # --- Apply inverse transformation and enforce positivity ---\n",
        "    # Add eps_small *after* exp to ensure positivity even if raw output is very small\n",
        "    k = torch.exp(k_raw) + eps_small\n",
        "    eps = torch.exp(eps_raw) + eps_small\n",
        "\n",
        "    # --- Calculate Gradients using PyTorch Autograd via DeepXDE wrappers ---\n",
        "    try:\n",
        "        # Gradients of primitive variables (u, v, p') directly from network output 'y'\n",
        "        u_x = dde.grad.jacobian(y, x, i=0, j=0); u_y = dde.grad.jacobian(y, x, i=0, j=1)\n",
        "        v_x = dde.grad.jacobian(y, x, i=1, j=0); v_y = dde.grad.jacobian(y, x, i=1, j=1)\n",
        "        p_prime_x = dde.grad.jacobian(y, x, i=2, j=0); p_prime_y = dde.grad.jacobian(y, x, i=2, j=1)\n",
        "\n",
        "        u_xx = dde.grad.hessian(y, x, component=0, i=0, j=0); u_yy = dde.grad.hessian(y, x, component=0, i=1, j=1)\n",
        "        v_xx = dde.grad.hessian(y, x, component=1, i=0, j=0); v_yy = dde.grad.hessian(y, x, component=1, i=1, j=1)\n",
        "        # Mixed derivatives (needed for momentum diffusion terms)\n",
        "        u_xy = dde.grad.hessian(y, x, component=0, i=0, j=1)\n",
        "        v_xy = dde.grad.hessian(y, x, component=1, i=0, j=1) # Order matters: d/dx(d/dy(...))\n",
        "\n",
        "        # --- Gradients of transformed k, eps using PyTorch autograd ---\n",
        "        # We need gradients of k and eps themselves, not log(k), log(eps)\n",
        "        # Ensure x requires grad if DeepXDE doesn't handle it automatically\n",
        "        if isinstance(x, torch.Tensor) and not x.requires_grad:\n",
        "            # This might be needed if x comes from data loading without gradients enabled\n",
        "            x.requires_grad_(True)\n",
        "\n",
        "        # Calculate grads for k\n",
        "        grad_k = torch.autograd.grad(k, x, grad_outputs=torch.ones_like(k), create_graph=True)[0]\n",
        "        k_x, k_y = grad_k[:, 0:1], grad_k[:, 1:2]\n",
        "        # Calculate grads for eps\n",
        "        grad_eps = torch.autograd.grad(eps, x, grad_outputs=torch.ones_like(eps), create_graph=True)[0]\n",
        "        eps_x, eps_y = grad_eps[:, 0:1], grad_eps[:, 1:2]\n",
        "\n",
        "        # --- Hessians of transformed k, eps using PyTorch autograd ---\n",
        "        # Calculate hessians for k (laplacian components)\n",
        "        # Need create_graph=True on first grad to compute second grad\n",
        "        grad_kx = torch.autograd.grad(k_x, x, grad_outputs=torch.ones_like(k_x), create_graph=True)[0]\n",
        "        k_xx = grad_kx[:, 0:1]\n",
        "        grad_ky = torch.autograd.grad(k_y, x, grad_outputs=torch.ones_like(k_y), create_graph=True)[0]\n",
        "        k_yy = grad_ky[:, 1:2]\n",
        "\n",
        "        # Calculate hessians for eps (laplacian components)\n",
        "        grad_epsx = torch.autograd.grad(eps_x, x, grad_outputs=torch.ones_like(eps_x), create_graph=True)[0]\n",
        "        eps_xx = grad_epsx[:, 0:1]\n",
        "        grad_epsy = torch.autograd.grad(eps_y, x, grad_outputs=torch.ones_like(eps_y), create_graph=True)[0]\n",
        "        eps_yy = grad_epsy[:, 1:2]\n",
        "\n",
        "    except RuntimeError as grad_e:\n",
        "        # Catch common autograd errors like trying to backward twice without retain_graph\n",
        "        logging.error(f\"PyTorch Autograd RuntimeError calculating gradients in PDE: {grad_e}. Ensure create_graph=True is used correctly for higher-order derivatives.\", exc_info=True)\n",
        "        # Return tensors of zeros with the correct shape and device matching input 'y'\n",
        "        zero_tensor = torch.zeros_like(y[:, 0:1])\n",
        "        return [zero_tensor] * 5 # Match the number of expected PDE residual outputs\n",
        "    except Exception as grad_e:\n",
        "        logging.error(f\"General error calculating gradients in PDE function: {grad_e}\", exc_info=True)\n",
        "        zero_tensor = torch.zeros_like(y[:, 0:1])\n",
        "        return [zero_tensor] * 5\n",
        "\n",
        "    # --- Turbulent Viscosity ---\n",
        "    # Use the transformed k, eps which are guaranteed positive\n",
        "    k_safe = k # k already has eps_small added after exp\n",
        "    eps_safe = eps # eps already has eps_small added after exp\n",
        "    # Add eps_small to denominator for extra safety, although eps_safe should be positive\n",
        "    nu_t = Cmu * torch.square(k_safe) / (eps_safe + eps_small) # Eq: nu_t = Cmu * k^2 / eps\n",
        "    nu_eff = nu + nu_t # Effective viscosity\n",
        "\n",
        "    # --- Gradients of nu_eff (Needed for diffusion terms in momentum eqns) ---\n",
        "    # Using chain rule: d(nu_eff)/dx = d(nu_t)/dk * dk/dx + d(nu_t)/deps * deps/dx\n",
        "    # Ensure denominators are safe\n",
        "    dnut_dk = 2.0 * Cmu * k_safe / (eps_safe + eps_small)\n",
        "    dnut_deps = -Cmu * torch.square(k_safe) / torch.square(eps_safe + eps_small)\n",
        "\n",
        "    nu_eff_x = dnut_dk * k_x + dnut_deps * eps_x\n",
        "    nu_eff_y = dnut_dk * k_y + dnut_deps * eps_y\n",
        "\n",
        "    # --- RANS Equation Residuals ---\n",
        "\n",
        "    # 1. Continuity Equation: d(u)/dx + d(v)/dy = 0\n",
        "    eq_continuity = u_x + v_y\n",
        "\n",
        "    # 2. X-Momentum Equation:\n",
        "    # d(u)/dt + u*du/dx + v*du/dy = -dp'/dx + d/dx[nu_eff * (2*du/dx)] + d/dy[nu_eff * (du/dy + dv/dx)]\n",
        "    # Steady state: u*du/dx + v*du/dy + dp'/dx - [d/dx(...) + d/dy(...)] = 0\n",
        "    adv_u = u * u_x + v * u_y # Advection\n",
        "    # Diffusion terms (expanded using product rule)\n",
        "    # d/dx[nu_eff * (2*du/dx)] = d(nu_eff)/dx * (2*du/dx) + nu_eff * (2*d^2u/dx^2)\n",
        "    diff_u_term1 = nu_eff_x * (2 * u_x) + nu_eff * (2 * u_xx)\n",
        "    # d/dy[nu_eff * (du/dy + dv/dx)] = d(nu_eff)/dy * (du/dy + dv/dx) + nu_eff * (d^2u/dy^2 + d^2v/dxdy)\n",
        "    # Assuming v_xy = d/dx(d/dy(v)) = d^2v/(dx dy)\n",
        "    diff_u_term2 = nu_eff_y * (u_y + v_x) + nu_eff * (u_yy + v_xy)\n",
        "    eq_mom_x = adv_u + p_prime_x - (diff_u_term1 + diff_u_term2)\n",
        "\n",
        "    # 3. Y-Momentum Equation:\n",
        "    # d(v)/dt + u*dv/dx + v*dv/dy = -dp'/dy + d/dx[nu_eff * (dv/dx + du/dy)] + d/dy[nu_eff * (2*dv/dy)]\n",
        "    # Steady state: u*dv/dx + v*dv/dy + dp'/dy - [d/dx(...) + d/dy(...)] = 0\n",
        "    adv_v = u * v_x + v * v_y # Advection\n",
        "    # Diffusion terms (expanded)\n",
        "    # d/dx[nu_eff * (dv/dx + du/dy)] = d(nu_eff)/dx * (dv/dx + du/dy) + nu_eff * (d^2v/dx^2 + d^2u/dxdy)\n",
        "    # Assuming u_xy = d/dx(d/dy(u)) = d^2u/(dx dy)\n",
        "    diff_v_term1 = nu_eff_x * (v_x + u_y) + nu_eff * (v_xx + u_xy)\n",
        "    # d/dy[nu_eff * (2*dv/dy)] = d(nu_eff)/dy * (2*dv/dy) + nu_eff * (2*d^2v/dy^2)\n",
        "    diff_v_term2 = nu_eff_y * (2 * v_y) + nu_eff * (2 * v_yy)\n",
        "    eq_mom_y = adv_v + p_prime_y - (diff_v_term1 + diff_v_term2)\n",
        "\n",
        "    # --- Turbulence Model Equations ---\n",
        "\n",
        "    # Production term P_k = nu_t * S^2, where S is the modulus of the mean strain rate tensor\n",
        "    # S^2 = 2*((du/dx)^2 + (dv/dy)^2) + (du/dy + dv/dx)^2  (for 2D)\n",
        "    S_squared = 2 * (torch.square(u_x) + torch.square(v_y)) + torch.square(u_y + v_x)\n",
        "    # Ensure P_k is non-negative (though theoretically it should be if nu_t >= 0 and S^2 >= 0)\n",
        "    P_k = torch.relu(nu_t * S_squared) # Using ReLU for safety, or just nu_t * S_squared if confident\n",
        "\n",
        "    # 4. k-Equation:\n",
        "    # d(k)/dt + u*dk/dx + v*dk/dy = d/dx[(nu + nu_t/sigma_k)*dk/dx] + d/dy[(nu + nu_t/sigma_k)*dk/dy] + P_k - eps\n",
        "    # Steady state: u*dk/dx + v*dk/dy - [Diffusion] - P_k + eps = 0\n",
        "    adv_k = u * k_x + v * k_y # Advection\n",
        "    # Diffusion term: div[ (nu + nu_t/sigma_k) * grad(k) ]\n",
        "    diffusivity_k = nu + nu_t / sigma_k\n",
        "    # Gradient of diffusivity: d(diff_k)/dx = (1/sigma_k) * d(nu_t)/dx\n",
        "    # Note: nu_eff_x = d(nu_t)/dx, nu_eff_y = d(nu_t)/dy\n",
        "    d_diffk_dx = (1 / sigma_k) * nu_eff_x\n",
        "    d_diffk_dy = (1 / sigma_k) * nu_eff_y\n",
        "    laplacian_k = k_xx + k_yy\n",
        "    # Expand divergence using product rule: div(D*grad(k)) = grad(D).grad(k) + D*laplacian(k)\n",
        "    diffusion_k = d_diffk_dx * k_x + d_diffk_dy * k_y + diffusivity_k * laplacian_k\n",
        "    # Use eps_safe which is guaranteed positive\n",
        "    eq_k = adv_k - diffusion_k - P_k + eps_safe\n",
        "\n",
        "    # 5. ε-Equation:\n",
        "    # d(eps)/dt + u*deps/dx + v*deps/dy = d/dx[(nu + nu_t/sigma_eps)*deps/dx] + d/dy[(nu + nu_t/sigma_eps)*deps/dy] + Ceps1*(eps/k)*P_k - Ceps2*(eps^2/k)\n",
        "    # Steady state: u*deps/dx + v*deps/dy - [Diffusion] - Source + Sink = 0\n",
        "    adv_eps = u * eps_x + v * eps_y # Advection\n",
        "    # Diffusion term: div[ (nu + nu_t/sigma_eps) * grad(eps) ]\n",
        "    diffusivity_eps = nu + nu_t / sigma_eps\n",
        "    d_diffeps_dx = (1 / sigma_eps) * nu_eff_x # Gradients of nu_t\n",
        "    d_diffeps_dy = (1 / sigma_eps) * nu_eff_y\n",
        "    laplacian_eps = eps_xx + eps_yy\n",
        "    diffusion_eps = d_diffeps_dx * eps_x + d_diffeps_dy * eps_y + diffusivity_eps * laplacian_eps\n",
        "    # Source/Sink terms (use safe, positive k and eps)\n",
        "    # Add eps_small to denominators for robustness, even if k_safe/eps_safe have it\n",
        "    source_eps = Ceps1 * (eps_safe / (k_safe + eps_small)) * P_k\n",
        "    sink_eps = Ceps2 * (torch.square(eps_safe) / (k_safe + eps_small))\n",
        "    eq_eps = adv_eps - diffusion_eps - source_eps + sink_eps\n",
        "\n",
        "    # Return the residuals of the 5 equations\n",
        "    return [eq_continuity, eq_mom_x, eq_mom_y, eq_k, eq_eps]\n",
        "# --- End PDE function ---\n",
        "\n",
        "\n",
        "# =============================\n",
        "# ===== Boundary Conditions =====\n",
        "# =============================\n",
        "def get_boundary_conditions(config):\n",
        "    \"\"\"Defines all boundary conditions for the channel flow problem.\"\"\"\n",
        "    geom = config.GEOM\n",
        "    h = config.CHANNEL_HALF_HEIGHT\n",
        "    L = config.L\n",
        "    y_p = config.Y_P # Wall function distance\n",
        "    n_wf_points = config.NUM_WF_POINTS_PER_WALL\n",
        "\n",
        "    # --- Boundary Definition Functions ---\n",
        "    def boundary_inlet(x, on_boundary):\n",
        "        return on_boundary and np.isclose(x[0], 0)\n",
        "\n",
        "    def boundary_outlet(x, on_boundary):\n",
        "        return on_boundary and np.isclose(x[0], L)\n",
        "\n",
        "    def boundary_bottom_wall_physical(x, on_boundary):\n",
        "        # Physical wall at y = -h\n",
        "        return on_boundary and np.isclose(x[1], -h)\n",
        "\n",
        "    def boundary_top_wall_physical(x, on_boundary):\n",
        "        # Physical wall at y = +h\n",
        "        return on_boundary and np.isclose(x[1], h)\n",
        "\n",
        "    def boundary_walls_physical(x, on_boundary):\n",
        "        # Combined physical walls\n",
        "        return boundary_bottom_wall_physical(x, on_boundary) or boundary_top_wall_physical(x, on_boundary)\n",
        "\n",
        "    # --- Inlet BCs (Dirichlet) ---\n",
        "    # u = U_INLET, v = 0, k = k_inlet_transformed, eps = eps_inlet_transformed\n",
        "    bc_u_inlet = dde.DirichletBC(geom, lambda x: config.U_INLET, boundary_inlet, component=0) # component=0 -> u\n",
        "    bc_v_inlet = dde.DirichletBC(geom, lambda x: 0, boundary_inlet, component=1) # component=1 -> v\n",
        "    bc_k_inlet = dde.DirichletBC(geom, lambda x: config.K_INLET_TRANSFORMED, boundary_inlet, component=3) # component=3 -> log(k)\n",
        "    bc_eps_inlet = dde.DirichletBC(geom, lambda x: config.EPS_INLET_TRANSFORMED, boundary_inlet, component=4) # component=4 -> log(eps)\n",
        "\n",
        "    # --- Outlet BC (Dirichlet) ---\n",
        "    # p' = 0 (gauge pressure relative to outlet)\n",
        "    bc_p_outlet = dde.DirichletBC(geom, lambda x: 0, boundary_outlet, component=2) # component=2 -> p'\n",
        "\n",
        "    # --- Physical Wall BCs (No-slip) ---\n",
        "    # u = 0, v = 0\n",
        "    bc_u_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=0) # u=0 on walls\n",
        "    bc_v_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=1) # v=0 on walls\n",
        "\n",
        "    # --- Wall Function BCs (PointSetBC near walls) ---\n",
        "    # Define anchor points slightly away from the walls at y = +/- (h - y_p)\n",
        "    # Avoid placing points exactly at inlet/outlet for stability\n",
        "    x_wf_coords = np.linspace(0 + L * 0.01, L - L * 0.01, n_wf_points)[:, None] # Exclude exact corners\n",
        "    points_bottom_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, -h + y_p)))\n",
        "    points_top_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, h - y_p)))\n",
        "    anchor_points_wf = np.vstack((points_bottom_wf, points_top_wf))\n",
        "    logging.info(f\"Generated {anchor_points_wf.shape[0]} anchor points for wall functions.\")\n",
        "\n",
        "    # Target values at these anchor points (using pre-calculated config values)\n",
        "    U_target_vals = np.full((anchor_points_wf.shape[0], 1), config.U_TARGET_WF)\n",
        "    k_target_vals = np.full((anchor_points_wf.shape[0], 1), config.K_TARGET_WF_TRANSFORMED)\n",
        "    eps_target_vals = np.full((anchor_points_wf.shape[0], 1), config.EPS_TARGET_WF_TRANSFORMED)\n",
        "\n",
        "    # Define PointSetBCs for u, k (log), eps (log) at the anchor points\n",
        "    bc_u_wf = dde.PointSetBC(anchor_points_wf, U_target_vals, component=0) # Target u at WF points\n",
        "    bc_k_wf = dde.PointSetBC(anchor_points_wf, k_target_vals, component=3) # Target log(k) at WF points\n",
        "    bc_eps_wf = dde.PointSetBC(anchor_points_wf, eps_target_vals, component=4) # Target log(eps) at WF points\n",
        "\n",
        "    # --- Collect all BCs ---\n",
        "    all_bcs = [\n",
        "        bc_u_inlet, bc_v_inlet, bc_k_inlet, bc_eps_inlet, # Inlet (4 BCs)\n",
        "        bc_p_outlet, # Outlet (1 BC)\n",
        "        bc_u_walls, bc_v_walls, # Physical Walls (2 BCs)\n",
        "        bc_u_wf, bc_k_wf, bc_eps_wf # Wall Functions (3 BCs)\n",
        "    ]\n",
        "    # Note: The anchor_points_wf array is also needed by the Data object later.\n",
        "    return all_bcs, anchor_points_wf\n",
        "# --- End Boundary Conditions ---\n",
        "\n",
        "\n",
        "# =======================\n",
        "# ===== Trainer Class =====\n",
        "# =======================\n",
        "class Trainer:\n",
        "    \"\"\"Handles the setup, training, and checkpointing of the PINN model.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.losshistory = None\n",
        "        self.train_state = None\n",
        "        self.pde = pde # Assign the PDE function\n",
        "\n",
        "    def build_model(self, bcs, anchor_points):\n",
        "        \"\"\"Builds the DeepXDE model including network and data.\"\"\"\n",
        "        logging.info(\"Building the PINN model...\")\n",
        "        if dde.backend.backend_name != \"pytorch\":\n",
        "             raise RuntimeError(\"This code relies on the PyTorch backend.\")\n",
        "\n",
        "        # Define the neural network\n",
        "        net = dde.maps.FNN(\n",
        "            layer_sizes=[self.config.NETWORK_INPUTS] + [self.config.NUM_NEURONS] * self.config.NUM_LAYERS + [self.config.NETWORK_OUTPUTS],\n",
        "            activation=self.config.ACTIVATION,\n",
        "            kernel_initializer=self.config.INITIALIZER\n",
        "        )\n",
        "\n",
        "        # Wrap PDE to include config\n",
        "        pde_with_config = lambda x, y: self.pde(x, y, config=self.config)\n",
        "\n",
        "        # Define the PDE data object\n",
        "        # Use 'anchor_points' argument if available in your DeepXDE version, otherwise rely on BC sampling\n",
        "        try:\n",
        "            data = dde.data.PDE(\n",
        "                geometry=self.config.GEOM,\n",
        "                pde=pde_with_config,\n",
        "                bcs=bcs, # List of boundary conditions including PointSetBCs\n",
        "                num_domain=self.config.NUM_DOMAIN_POINTS,\n",
        "                num_boundary=self.config.NUM_BOUNDARY_POINTS, # Sample points on physical boundaries\n",
        "                num_test=self.config.NUM_TEST_POINTS, # Points for testing PDE residual during training\n",
        "                anchors=anchor_points # Explicitly provide wall function anchor points here\n",
        "            )\n",
        "            logging.info(f\"Using {anchor_points.shape[0]} anchor points for wall functions (passed to anchors).\")\n",
        "        except TypeError: # Handle older DeepXDE versions that might not have 'anchors'\n",
        "            logging.warning(\"DeepXDE version might not support 'anchors' argument in PDE. Relying on PointSetBC sampling.\")\n",
        "            data = dde.data.PDE(\n",
        "                geometry=self.config.GEOM,\n",
        "                pde=pde_with_config,\n",
        "                bcs=bcs, # PointSetBCs are still included here\n",
        "                num_domain=self.config.NUM_DOMAIN_POINTS,\n",
        "                num_boundary=self.config.NUM_BOUNDARY_POINTS,\n",
        "                num_test=self.config.NUM_TEST_POINTS\n",
        "                # num_anchors is deprecated/removed in newer versions, do not use\n",
        "            )\n",
        "\n",
        "\n",
        "        self.model = dde.Model(data, net)\n",
        "        logging.info(\"Model built successfully.\")\n",
        "\n",
        "    # ==============================================================\n",
        "    # ========    UPDATED Trainer.train Method            ==========\n",
        "    # ==============================================================\n",
        "    def train(self):\n",
        "        \"\"\"Compiles and trains the model, handles checkpointing and optimizer switching.\"\"\"\n",
        "        if self.model is None:\n",
        "            logging.error(\"Model not built. Call build_model first.\")\n",
        "            return None, None, None\n",
        "\n",
        "        # --- Define Checkpoint Paths and Callback ---\n",
        "        filepath_base = os.path.join(self.config.MODEL_DIR, f\"{self.config.CHECKPOINT_FILENAME_BASE}-\")\n",
        "        logging.info(f\"Checkpoint filename base: {filepath_base}\")\n",
        "        custom_checkpointer = CustomModelCheckpoint(\n",
        "            filepath_base=filepath_base,\n",
        "            period=self.config.SAVE_INTERVAL,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # --- Check for Latest Checkpoint ---\n",
        "        latest_checkpoint = None\n",
        "        restored_step = 0\n",
        "        if os.path.exists(self.config.MODEL_DIR):\n",
        "            try:\n",
        "                # Use a regex that matches the base name, a hyphen, digits, and the .pt extension\n",
        "                filename_pattern = re.compile(rf\"^{re.escape(os.path.basename(filepath_base))}(\\d+)\\.pt$\")\n",
        "                checkpoint_files = []\n",
        "                logging.info(f\"Searching for checkpoints in: {self.config.MODEL_DIR}\")\n",
        "                logging.info(f\"Using pattern: {filename_pattern.pattern}\")\n",
        "\n",
        "                for f in os.listdir(self.config.MODEL_DIR):\n",
        "                    full_path = os.path.join(self.config.MODEL_DIR, f)\n",
        "                    if os.path.isfile(full_path): # Ensure it's a file\n",
        "                        match = filename_pattern.match(f)\n",
        "                        if match:\n",
        "                            step_num = int(match.group(1))\n",
        "                            checkpoint_files.append((step_num, full_path))\n",
        "                            logging.debug(f\"Found potential checkpoint: {f} (Step: {step_num})\") # Debug log\n",
        "\n",
        "                if checkpoint_files:\n",
        "                    checkpoint_files.sort(key=lambda item: item[0], reverse=True) # Sort descending by step\n",
        "                    restored_step, latest_checkpoint = checkpoint_files[0] # Get the one with the highest step\n",
        "                    logging.info(f\"Found latest valid checkpoint: {latest_checkpoint} at step {restored_step}\")\n",
        "                else:\n",
        "                    logging.info(\"No valid checkpoints found matching the pattern.\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error finding/parsing checkpoint filenames: {e}\", exc_info=True)\n",
        "                latest_checkpoint = None; restored_step = 0\n",
        "\n",
        "        restore_path = latest_checkpoint if (latest_checkpoint and os.path.isfile(latest_checkpoint)) else None\n",
        "\n",
        "        # --- Determine Initial Optimizer Based on Restored Step ---\n",
        "        initial_optimizer = \"adam\"\n",
        "        initial_lr = self.config.LEARNING_RATE_ADAM\n",
        "        if restore_path and restored_step >= self.config.ADAM_ITERATIONS:\n",
        "            # If restoring from a step within or after the L-BFGS phase, compile L-BFGS first.\n",
        "            initial_optimizer = \"L-BFGS\"\n",
        "            initial_lr = None # L-BFGS doesn't use LR in compile signature\n",
        "            logging.info(f\"Restored step ({restored_step}) >= Adam iterations ({self.config.ADAM_ITERATIONS}). Will compile with L-BFGS initially.\")\n",
        "        else:\n",
        "            # Start with Adam (from scratch or resuming within Adam phase).\n",
        "            logging.info(f\"Restored step ({restored_step}) < Adam iterations ({self.config.ADAM_ITERATIONS}) or no checkpoint. Will compile with Adam initially.\")\n",
        "\n",
        "        # --- Compile with the Determined Initial Optimizer ---\n",
        "        logging.info(f\"Compiling model with {initial_optimizer} optimizer initially.\")\n",
        "        if dde.backend.backend_name != \"pytorch\":\n",
        "             raise RuntimeError(\"Cannot compile model, backend is not PyTorch.\")\n",
        "\n",
        "        compile_args = {\"optimizer\": initial_optimizer, \"loss_weights\": self.config.LOSS_WEIGHTS}\n",
        "        if initial_lr is not None:\n",
        "            compile_args[\"lr\"] = initial_lr\n",
        "        try:\n",
        "            # Compile *before* restoring state\n",
        "            self.model.compile(**compile_args)\n",
        "            logging.info(f\"Model compiled with {initial_optimizer}.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to compile model with {initial_optimizer}: {e}\", exc_info=True)\n",
        "            return None, None, None # Stop if compilation fails\n",
        "\n",
        "\n",
        "        # --- Explicit Restore (if applicable) ---\n",
        "        if restore_path:\n",
        "            try:\n",
        "                logging.info(f\"Explicitly restoring model state from: {restore_path}\")\n",
        "                # Restore should work because the compiled optimizer matches the saved state (or will be handled by restore)\n",
        "                # Pass ignore_hyperparameters=True if you only want weights/biases and not optimizer state/step etc.\n",
        "                # However, for resuming, we *do* want the optimizer state and step.\n",
        "                self.model.restore(restore_path, verbose=1)\n",
        "\n",
        "                # DeepXDE's restore *should* update train_state.step internally. Let's verify.\n",
        "                current_step_after_restore = self.model.train_state.step if self.model.train_state else -1\n",
        "                logging.info(f\"Model state restored. Internal DDE step count *after* restore: {current_step_after_restore}\")\n",
        "\n",
        "                # Force set the step counter based on the filename step for consistency, IF different.\n",
        "                if self.model.train_state:\n",
        "                    if current_step_after_restore != restored_step:\n",
        "                         logging.warning(f\"Mismatch between expected restored step ({restored_step}) from filename and DDE internal step ({current_step_after_restore}) after restore! Forcing DDE step to match filename.\")\n",
        "                         self.model.train_state.step = restored_step\n",
        "                         # Log the step count again after forcing it\n",
        "                         current_step_after_manual_set = self.model.train_state.step\n",
        "                         logging.info(f\"Internal step count after manual setting: {current_step_after_manual_set}\")\n",
        "                    else:\n",
        "                         logging.info(\"DDE internal step count matches restored step from filename.\")\n",
        "                else:\n",
        "                    # This should not happen if restore was successful with a valid state\n",
        "                    logging.error(\"Cannot verify/set step count: model.train_state is None after restore. Restore might have failed silently or checkpoint was incomplete.\")\n",
        "                    self.model = None # Indicate failure\n",
        "                    return None, None, None # Stop if restore fails\n",
        "\n",
        "            except Exception as e:\n",
        "                # The specific KeyError: 'step' might occur if checkpoint is old or missing optimizer state.\n",
        "                # Other errors (file corruption, backend mismatch during save/load) might still occur.\n",
        "                logging.error(f\"Failed during explicit model restore from {restore_path}: {e}\", exc_info=True)\n",
        "                logging.error(\"Common causes: Incomplete checkpoint (missing optimizer state?), version mismatch, file corruption.\")\n",
        "                self.model = None # Indicate failure\n",
        "                return None, None, None # Stop if restore fails\n",
        "        else:\n",
        "            logging.info(\"No suitable checkpoint found. Starting training from scratch.\")\n",
        "            restored_step = 0 # Ensure this is 0 if starting fresh\n",
        "            if self.model.train_state:\n",
        "                self.model.train_state.step = 0 # Ensure internal step is 0 for new training\n",
        "            else:\n",
        "                 # This can happen if the initial compile failed\n",
        "                 logging.error(\"model.train_state is None when starting from scratch. Compile likely failed earlier.\")\n",
        "                 return None, None, None\n",
        "\n",
        "\n",
        "        # --- Adam Training Phase ---\n",
        "        run_adam_phase = False\n",
        "        adam_iters_to_run = 0\n",
        "        # Use the step count that's been synchronized after potential restore\n",
        "        current_step_synced = self.model.train_state.step if self.model.train_state else restored_step\n",
        "\n",
        "        if initial_optimizer == \"adam\":\n",
        "            # Only run Adam if we started with Adam and haven't finished its iterations\n",
        "            if current_step_synced < self.config.ADAM_ITERATIONS:\n",
        "                run_adam_phase = True\n",
        "                adam_iters_to_run = self.config.ADAM_ITERATIONS - current_step_synced\n",
        "                logging.info(f\"Starting Adam training phase from step {current_step_synced} for {adam_iters_to_run} iterations...\")\n",
        "            else:\n",
        "                logging.info(f\"Adam phase already completed (current step {current_step_synced} >= {self.config.ADAM_ITERATIONS}).\")\n",
        "        else:\n",
        "            # Adam is skipped if we restored directly into the L-BFGS phase\n",
        "            logging.info(\"Skipping Adam phase as initial compilation was L-BFGS.\")\n",
        "\n",
        "        adam_start_time = time.time()\n",
        "        if run_adam_phase and adam_iters_to_run > 0:\n",
        "            try:\n",
        "                # Ensure model is compiled with Adam before training Adam\n",
        "                if self.model.opt_name != \"adam\": # Safety check\n",
        "                     logging.warning(\"Model optimizer is not Adam before Adam training. Recompiling with Adam.\")\n",
        "                     self.model.compile(\"adam\", lr=self.config.LEARNING_RATE_ADAM, loss_weights=self.config.LOSS_WEIGHTS)\n",
        "                     # Restore step count if recompile reset it (might happen)\n",
        "                     if self.model.train_state and self.model.train_state.step != current_step_synced:\n",
        "                          logging.info(f\"Restoring step count to {current_step_synced} after Adam recompile.\")\n",
        "                          self.model.train_state.step = current_step_synced\n",
        "\n",
        "                self.losshistory, self.train_state = self.model.train(\n",
        "                    iterations=adam_iters_to_run,\n",
        "                    display_every=self.config.DISPLAY_EVERY,\n",
        "                    callbacks=[custom_checkpointer] # Use the custom callback\n",
        "                )\n",
        "                adam_time = time.time() - adam_start_time\n",
        "                # Update current step after Adam phase completes\n",
        "                current_step_synced = self.model.train_state.step if self.model.train_state else -1\n",
        "                # Check if losshistory is valid before accessing attributes\n",
        "                if self.losshistory and hasattr(self.losshistory, 'loss_train') and self.losshistory.loss_train:\n",
        "                    final_loss_adam = self.losshistory.loss_train[-1] if self.losshistory.loss_train else \"N/A\"\n",
        "                    logging.info(f\"Adam training ({adam_iters_to_run} iterations) finished in {adam_time:.2f}s. Final loss: {final_loss_adam}. Current step: {current_step_synced}\")\n",
        "                else:\n",
        "                    logging.error(f\"Adam training finished in {adam_time:.2f}s but loss history is empty/invalid. Current step: {current_step_synced}\")\n",
        "                    # Consider whether to proceed to L-BFGS or return failure\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error occurred during Adam training: {e}\", exc_info=True)\n",
        "                return self.model, self.losshistory, self.train_state # Exit on error\n",
        "\n",
        "        # --- L-BFGS Training Phase ---\n",
        "        run_lbfgs_phase = False\n",
        "        lbfgs_iters_to_run = 0\n",
        "        needs_lbfgs_compile = False\n",
        "\n",
        "        # Get the step count *after* any Adam training that might have occurred\n",
        "        current_step_after_adam = self.model.train_state.step if self.model.train_state else current_step_synced\n",
        "\n",
        "        # Calculate the nominal total steps required by the configuration\n",
        "        total_target_steps = self.config.ADAM_ITERATIONS + self.config.LBFGS_ITERATIONS\n",
        "\n",
        "        # ===> MODIFICATION START: Check if total steps already completed <===\n",
        "        if current_step_after_adam >= total_target_steps:\n",
        "            logging.info(f\"Current step ({current_step_after_adam}) meets or exceeds total configured steps ({total_target_steps}). Skipping further training.\")\n",
        "            run_lbfgs_phase = False # Explicitly prevent L-BFGS phase\n",
        "        # ===> MODIFICATION END <===\n",
        "        elif self.config.LBFGS_ITERATIONS > 0: # Original check: L-BFGS is configured to run\n",
        "             # Proceed with the logic to potentially run L-BFGS\n",
        "            if current_step_after_adam < self.config.ADAM_ITERATIONS:\n",
        "                 # This case should ideally not happen if Adam ran correctly, but acts as a safeguard\n",
        "                 logging.warning(f\"Attempting L-BFGS, but current step {current_step_after_adam} is less than Adam target {self.config.ADAM_ITERATIONS}. Check logic.\")\n",
        "                 # Decide if L-BFGS should run anyway or be skipped. Here we skip it.\n",
        "                 run_lbfgs_phase = False\n",
        "            else:\n",
        "                 # We are at or past the Adam iterations, L-BFGS is configured, and haven't hit the total target yet.\n",
        "                 run_lbfgs_phase = True\n",
        "                 # Simplification: Run the full configured L-BFGS iterations when entering this phase.\n",
        "                 lbfgs_iters_to_run = self.config.LBFGS_ITERATIONS\n",
        "                 logging.info(f\"Proceeding to L-BFGS phase (current step {current_step_after_adam}). Target iterations for this phase: {lbfgs_iters_to_run}\")\n",
        "\n",
        "                 # Check if we need to compile L-BFGS (i.e., if the *current* optimizer is Adam)\n",
        "                 if self.model.opt_name == \"adam\": # Check the actual current optimizer\n",
        "                     needs_lbfgs_compile = True\n",
        "                     logging.info(\"Switching optimizer: Will compile for L-BFGS.\")\n",
        "                 elif self.model.opt_name == \"L-BFGS\":\n",
        "                      logging.info(\"Optimizer is already L-BFGS (likely resumed). No recompilation needed.\")\n",
        "                 else:\n",
        "                      # Should ideally be either 'adam' or 'L-BFGS' at this point\n",
        "                      logging.warning(f\"Unexpected optimizer '{self.model.opt_name}' before L-BFGS phase. Forcing L-BFGS compile.\")\n",
        "                      needs_lbfgs_compile = True\n",
        "        else:\n",
        "            # L-BFGS iterations set to 0 in config\n",
        "            logging.info(\"L-BFGS iterations set to 0 in config, skipping L-BFGS training phase.\")\n",
        "            run_lbfgs_phase = False\n",
        "\n",
        "\n",
        "        # --- Execute L-BFGS if needed ---\n",
        "        if run_lbfgs_phase and lbfgs_iters_to_run > 0:\n",
        "             if self.model is not None and self.model.net is not None:\n",
        "                 lbfgs_start_time = time.time()\n",
        "                 try:\n",
        "                     # Re-compile for L-BFGS only if necessary\n",
        "                     if needs_lbfgs_compile:\n",
        "                         self.model.compile(\"L-BFGS\", loss_weights=self.config.LOSS_WEIGHTS)\n",
        "                         logging.info(f\"Model re-compiled with L-BFGS.\")\n",
        "                         # Restore step count if recompile reset it\n",
        "                         if self.model.train_state and self.model.train_state.step != current_step_after_adam:\n",
        "                              logging.info(f\"Restoring step count to {current_step_after_adam} after L-BFGS recompile.\")\n",
        "                              self.model.train_state.step = current_step_after_adam\n",
        "\n",
        "                     # Use the same CUSTOM checkpointer instance for L-BFGS saves\n",
        "                     # The 'iterations' for L-BFGS in DeepXDE usually means max_iter for the optimizer's internal loop.\n",
        "                     # The global step advancement depends on the callbacks and how DeepXDE handles L-BFGS steps.\n",
        "                     self.losshistory, self.train_state = self.model.train(\n",
        "                         iterations=lbfgs_iters_to_run, # Let L-BFGS run its course\n",
        "                         display_every=self.config.DISPLAY_EVERY, # Use display_every for LBFGS too\n",
        "                         callbacks=[custom_checkpointer] # Pass the custom callback instance\n",
        "                     )\n",
        "                     lbfgs_time = time.time() - lbfgs_start_time\n",
        "                     # Update current step after L-BFGS phase completes\n",
        "                     current_step_synced = self.model.train_state.step if self.model.train_state else -1\n",
        "                     if self.losshistory and hasattr(self.losshistory, 'loss_train') and self.losshistory.loss_train:\n",
        "                         final_loss_lbfgs = self.losshistory.loss_train[-1] if self.losshistory.loss_train else \"N/A\"\n",
        "                         logging.info(f\"L-BFGS training (max {lbfgs_iters_to_run} internal iterations) finished in {lbfgs_time:.2f}s. Final loss: {final_loss_lbfgs}. Final global step recorded: {current_step_synced}\")\n",
        "                     else:\n",
        "                         logging.error(f\"L-BFGS training finished in {lbfgs_time:.2f}s but loss history is invalid. Final global step: {current_step_synced}\")\n",
        "                         # Decide whether to continue based on Adam success\n",
        "\n",
        "                 except Exception as e:\n",
        "                     logging.error(f\"Error during L-BFGS compilation or training: {e}\", exc_info=True)\n",
        "                     # Decide whether to continue based on Adam success\n",
        "             else:\n",
        "                 logging.warning(\"Skipping L-BFGS training because the model state is not valid (e.g., restore failed or Adam failed).\")\n",
        "\n",
        "        # --- Post-training Validation (Using Corrected Gradient Calculation) ---\n",
        "        if self.model and self.model.net:\n",
        "            try:\n",
        "                self._post_training_checks()\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"Error during post-training checks: {e}\", exc_info=True)\n",
        "\n",
        "        logging.info(\"Training sequence complete.\")\n",
        "        # Make sure to return the potentially updated model, losshistory, train_state\n",
        "        return self.model, self.losshistory, self.train_state\n",
        "    # ==============================================================\n",
        "    #========== END OF UPDATED Trainer.train Method      ===========\n",
        "    # ==============================================================\n",
        "\n",
        "\n",
        "    # --- Optional Post Training Checks ---\n",
        "    def _post_training_checks(self):\n",
        "        \"\"\"Perform physics validation checks compatible with PyTorch backend.\"\"\"\n",
        "        logging.info(\"Performing PyTorch-based post-training checks...\")\n",
        "        if dde.backend.backend_name != \"pytorch\":\n",
        "             logging.warning(\"Post-training checks skipped, requires PyTorch backend.\")\n",
        "             return\n",
        "        if self.model is None or self.model.net is None:\n",
        "             logging.error(\"Model or network not available for post-training checks.\")\n",
        "             return\n",
        "        try:\n",
        "            # Run the turbulence production check\n",
        "            self._check_turbulence_production()\n",
        "            # Add other checks here if needed\n",
        "        except Exception as e:\n",
        "            # Catch errors specifically from the check functions\n",
        "            logging.error(f\"Error during post-training check execution: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "    def _check_turbulence_production(self):\n",
        "        \"\"\"Ensure turbulence production term P_k >= 0 using dde.grad.jacobian.\"\"\"\n",
        "        logging.info(\"Checking turbulence production term Pk...\")\n",
        "        if not hasattr(self.model.data, 'test_x') or self.model.data.test_x is None or len(self.model.data.test_x) == 0:\n",
        "            logging.warning(\"Test points not available, using training points for P_k check.\")\n",
        "            if not hasattr(self.model.data, 'train_x') or self.model.data.train_x is None or len(self.model.data.train_x) == 0:\n",
        "                 logging.error(\"No points available (train or test) for P_k check.\")\n",
        "                 return\n",
        "            X = self.model.data.train_x # Fallback to training points\n",
        "        else:\n",
        "             X = self.model.data.test_x # Prefer test points\n",
        "\n",
        "        if self.model.net is None or not list(self.model.net.parameters()):\n",
        "             logging.error(\"Model network not available or has no parameters for P_k check.\")\n",
        "             return\n",
        "\n",
        "        # Get device from model parameters\n",
        "        try:\n",
        "            # Ensure model parameters exist and get device\n",
        "            device = next(iter(self.model.net.parameters())).device\n",
        "        except StopIteration:\n",
        "             logging.error(\"Model network has no parameters.\")\n",
        "             return\n",
        "        except AttributeError:\n",
        "             logging.error(\"Model or network object structure unexpected.\")\n",
        "             return\n",
        "\n",
        "        # Ensure X is a tensor on the correct device with requires_grad=True\n",
        "        try:\n",
        "            x_tensor = torch.tensor(X, dtype=torch.float32, device=device, requires_grad=True)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to convert points to tensor for P_k check: {e}\")\n",
        "            return\n",
        "\n",
        "        # Forward pass with gradient tracking enabled by requires_grad=True on x_tensor\n",
        "        try:\n",
        "            y_tensor = self.model.net(x_tensor)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed during network forward pass for P_k check: {e}\")\n",
        "            return\n",
        "\n",
        "        if y_tensor is None or y_tensor.shape[1] != self.config.NETWORK_OUTPUTS:\n",
        "             logging.error(f\"Network output has unexpected shape {y_tensor.shape if y_tensor is not None else 'None'} for P_k check.\")\n",
        "             return\n",
        "\n",
        "        # Extract variables (these are tensors now)\n",
        "        u = y_tensor[:, 0:1]; v = y_tensor[:, 1:2]\n",
        "        k_raw = y_tensor[:, 3:4]; eps_raw = y_tensor[:, 4:5]\n",
        "\n",
        "        # Use the same transformation as in PDE\n",
        "        k_check = torch.exp(k_raw) + self.config.EPS_SMALL\n",
        "        eps_check = torch.exp(eps_raw) + self.config.EPS_SMALL\n",
        "\n",
        "        # Compute gradients using DeepXDE's jacobian function\n",
        "        try:\n",
        "            # Calculate individual gradient components using dde.grad.jacobian\n",
        "            u_x = dde.grad.jacobian(y_tensor, x_tensor, i=0, j=0) # d(output 0)/d(input 0) = du/dx\n",
        "            u_y = dde.grad.jacobian(y_tensor, x_tensor, i=0, j=1) # d(output 0)/d(input 1) = du/dy\n",
        "            v_x = dde.grad.jacobian(y_tensor, x_tensor, i=1, j=0) # d(output 1)/d(input 0) = dv/dx\n",
        "            v_y = dde.grad.jacobian(y_tensor, x_tensor, i=1, j=1) # d(output 1)/d(input 1) = dv/dy\n",
        "\n",
        "        except Exception as grad_e:\n",
        "             # Catch potential errors during gradient computation\n",
        "             logging.error(f\"Error computing velocity gradients via dde.grad.jacobian for P_k check: {grad_e}\", exc_info=True)\n",
        "             return\n",
        "\n",
        "        # Compute nu_t using safe k, eps from transformation\n",
        "        # Use torch.maximum for safe division, ensure tensors are on same device\n",
        "        eps_safe_check = torch.maximum(eps_check, torch.tensor(self.config.EPS_SMALL**2, device=device))\n",
        "        nu_t_check = self.config.CMU * torch.square(k_check) / eps_safe_check\n",
        "\n",
        "        # Strain rate tensor squared (S^2)\n",
        "        S_squared = 2*(torch.square(u_x) + torch.square(v_y)) + torch.square(u_y + v_x)\n",
        "        # Production term P_k = nu_t * S^2\n",
        "        P_k = nu_t_check * S_squared\n",
        "\n",
        "        # Check for negative production (detach before converting to numpy/item)\n",
        "        try:\n",
        "            P_k_detached = P_k.detach() # Detach from graph before summary stats\n",
        "            min_Pk = torch.min(P_k_detached).item()\n",
        "            max_Pk = torch.max(P_k_detached).item()\n",
        "            num_negative = torch.sum(P_k_detached < 0).item()\n",
        "            # Allow small negative values due to numerical precision\n",
        "            negative_threshold = -self.config.EPS_SMALL * 100\n",
        "            if min_Pk < negative_threshold:\n",
        "                logging.warning(f\"Negative turbulence production detected! Min P_k = {min_Pk:.3e}. ({num_negative}/{len(P_k)} points < {negative_threshold:.1e})\")\n",
        "            else:\n",
        "                logging.info(f\"Turbulence production check passed (min P_k = {min_Pk:.3e}, max P_k = {max_Pk:.3e})\")\n",
        "        except Exception as check_e:\n",
        "             logging.error(f\"Error checking P_k value statistics: {check_e}\", exc_info=True)\n",
        "\n",
        "# --- End Trainer Class ---\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ===== Plotter Class =====\n",
        "# ========================\n",
        "class Plotter:\n",
        "    \"\"\"Handles post-processing and plotting of simulation results.\"\"\"\n",
        "    def __init__(self, config, plotter_config, model, losshistory, train_state):\n",
        "        self.config = config\n",
        "        self.plotter_config = plotter_config\n",
        "        self.model = model\n",
        "        self.losshistory = losshistory\n",
        "        self.train_state = train_state\n",
        "        self.ref_data_path = config.REFERENCE_DATA_FILE\n",
        "        self.plots_dir = config.PLOT_DIR\n",
        "        self.ref_data = None\n",
        "        self.has_ref_data = False\n",
        "        self.ref_data_utau = None # Estimated friction velocity from reference data\n",
        "        self.pinn_data_utau = None # Estimated friction velocity from PINN data\n",
        "\n",
        "        # Predicted fields (initialized to None)\n",
        "        self.X_grid, self.Y_grid = None, None\n",
        "        self.u_pred, self.v_pred, self.p_prime_pred = None, None, None\n",
        "        self.k_pred, self.eps_pred, self.nu_t_pred = None, None, None\n",
        "        self.p_pred = None # Kinematic pressure\n",
        "\n",
        "        # Ensure plot directory exists before plotting\n",
        "        os.makedirs(self.plots_dir, exist_ok=True)\n",
        "        logging.info(f\"Plotter initialized. Plots will be saved in: {self.plots_dir}\")\n",
        "        if self.ref_data_path:\n",
        "             if os.path.exists(self.ref_data_path):\n",
        "                 logging.info(f\"Reference CSV data path found: {self.ref_data_path}\")\n",
        "             else:\n",
        "                 logging.warning(f\"Reference CSV file not found: {self.ref_data_path}. Comparisons will be skipped.\")\n",
        "        else:\n",
        "             logging.info(\"No reference data path provided in config. Comparisons will be skipped.\")\n",
        "\n",
        "    def plot_loss_history(self):\n",
        "        \"\"\"Plots and saves the training loss history.\"\"\"\n",
        "        if self.losshistory and self.train_state:\n",
        "            logging.info(\"Saving loss history plot...\")\n",
        "            try:\n",
        "                os.makedirs(self.plots_dir, exist_ok=True) # Ensure dir exists\n",
        "                # Use isplot=False to prevent showing plot in non-interactive envs like Colab background runs\n",
        "                dde.saveplot(self.losshistory, self.train_state, issave=True, isplot=False, output_dir=self.plots_dir)\n",
        "                # Check if the standard 'loss.png' was created and rename it\n",
        "                default_loss_file = os.path.join(self.plots_dir, \"loss.png\")\n",
        "                target_loss_file = os.path.join(self.plots_dir, \"training_loss_history.png\")\n",
        "                if os.path.exists(default_loss_file):\n",
        "                    try:\n",
        "                        # Use os.replace for atomic rename if possible, fallback to rename\n",
        "                        os.replace(default_loss_file, target_loss_file)\n",
        "                        logging.info(f\"Loss history plot saved as '{os.path.basename(target_loss_file)}'.\")\n",
        "                    except OSError as rename_err: # Fallback for cross-device links etc.\n",
        "                         logging.warning(f\"Could not replace/rename loss plot, using os.rename: {rename_err}\")\n",
        "                         os.rename(default_loss_file, target_loss_file)\n",
        "                         logging.info(f\"Loss history plot saved as '{os.path.basename(target_loss_file)}'.\")\n",
        "\n",
        "                else:\n",
        "                     # Check if the loss file was created with a different name pattern potentially\n",
        "                     potential_files = [f for f in os.listdir(self.plots_dir) if f.lower().endswith('.png') and 'loss' in f.lower()]\n",
        "                     if potential_files:\n",
        "                          logging.warning(f\"dde.saveplot might not have produced 'loss.png'. Found: {potential_files}. Check DeepXDE version/behavior.\")\n",
        "                     else:\n",
        "                          logging.warning(\"dde.saveplot did not produce 'loss.png'. Check file permissions, DeepXDE version, or if training actually ran.\")\n",
        "            except ImportError:\n",
        "                logging.error(\"Matplotlib might be needed by dde.saveplot but is not installed or importable.\")\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"Could not save loss history plot: {e}\", exc_info=True)\n",
        "        else:\n",
        "            logging.warning(\"Loss history or train state not available, skipping loss plot.\")\n",
        "\n",
        "    def load_reference_data(self):\n",
        "        \"\"\"Loads and preprocesses reference data from a CSV file.\"\"\"\n",
        "        self.has_ref_data = False # Assume false until successfully loaded\n",
        "        if not self.ref_data_path:\n",
        "            logging.info(\"No reference data path provided. Skipping load.\")\n",
        "            return\n",
        "        if not os.path.exists(self.ref_data_path):\n",
        "            logging.warning(f\"Reference CSV file not found: '{self.ref_data_path}'. Skipping load.\")\n",
        "            return\n",
        "\n",
        "        logging.info(f\"Loading reference data from: {self.ref_data_path}\")\n",
        "        try:\n",
        "            df_ref = pd.read_csv(self.ref_data_path)\n",
        "            logging.info(f\"Loaded reference data: {df_ref.shape[0]} rows, {df_ref.shape[1]} cols. Initial columns: {df_ref.columns.tolist()}\")\n",
        "\n",
        "            # --- Data Filtering (Optional, customize as needed) ---\n",
        "            # Example: Filter for latest time step if applicable\n",
        "            time_col = None\n",
        "            if 'Time' in df_ref.columns: time_col = 'Time'\n",
        "            elif 'TimeStep' in df_ref.columns: time_col = 'TimeStep' # Adapt to actual column name\n",
        "            if time_col:\n",
        "                latest_time = df_ref[time_col].max()\n",
        "                df_ref = df_ref[df_ref[time_col] == latest_time].copy()\n",
        "                logging.info(f\"Filtered for latest time/step ({time_col}={latest_time}): {df_ref.shape[0]} rows remaining.\")\n",
        "\n",
        "            # Identify coordinate columns (handle variations in naming robustly)\n",
        "            x_col, y_col, z_col = None, None, None\n",
        "            # Prioritize common exact names, then case-insensitive, then containing keywords\n",
        "            potential_x = ['x', 'Points:0', 'X', 'x-coordinate']\n",
        "            potential_y = ['y', 'Points:1', 'Y', 'y-coordinate']\n",
        "            potential_z = ['z', 'Points:2', 'Z', 'z-coordinate']\n",
        "\n",
        "            for p_x in potential_x:\n",
        "                if p_x in df_ref.columns: x_col = p_x; break\n",
        "            if not x_col: # Fallback: case-insensitive and keyword check\n",
        "                for col in df_ref.columns:\n",
        "                    if col.lower() in ['x', 'points:0', 'x-coordinate']: x_col = col; break\n",
        "\n",
        "            for p_y in potential_y:\n",
        "                if p_y in df_ref.columns: y_col = p_y; break\n",
        "            if not y_col:\n",
        "                 for col in df_ref.columns:\n",
        "                    if col.lower() in ['y', 'points:1', 'y-coordinate']: y_col = col; break\n",
        "\n",
        "            for p_z in potential_z:\n",
        "                 if p_z in df_ref.columns: z_col = p_z; break\n",
        "            if not z_col:\n",
        "                 for col in df_ref.columns:\n",
        "                    if col.lower() in ['z', 'points:2', 'z-coordinate']: z_col = col; break\n",
        "\n",
        "            if not x_col or not y_col:\n",
        "                 raise ValueError(f\"Could not identify x/y coordinates in reference columns: {df_ref.columns.tolist()}\")\n",
        "            logging.info(f\"Identified coordinate columns: x='{x_col}', y='{y_col}'\" + (f\", z='{z_col}'\" if z_col else \"\"))\n",
        "\n",
        "            # Filter for specific Z-plane if data is 3D and multiple Z values exist\n",
        "            if z_col and len(df_ref[z_col].unique()) > 1:\n",
        "                target_z = 0.0 # Target the center plane\n",
        "                unique_z = df_ref[z_col].unique()\n",
        "                nearest_z_idx = np.argmin(np.abs(unique_z - target_z))\n",
        "                nearest_z = unique_z[nearest_z_idx]\n",
        "                # Use a tolerance for floating point comparison\n",
        "                df_ref = df_ref[np.isclose(df_ref[z_col], nearest_z)].copy()\n",
        "                logging.info(f\"Filtered for z-plane near {target_z} (actual: {nearest_z:.4f}): {df_ref.shape[0]} rows remaining.\")\n",
        "\n",
        "            # --- Variable Renaming (Handle variations) ---\n",
        "            # Map potential CSV column names (lowercase) to consistent internal names\n",
        "            var_map = {\n",
        "                'u:0':'u_ref', 'u_x':'u_ref', 'velocity:0':'u_ref', 'velocity_x':'u_ref', 'u':'u_ref', 'velocityu':'u_ref',\n",
        "                'u:1':'v_ref', 'u_y':'v_ref', 'velocity:1':'v_ref', 'velocity_y':'v_ref', 'v':'v_ref', 'velocityv':'v_ref',\n",
        "                'p':'p_ref', 'pressure':'p_ref', 'kinematicpressure':'p_ref', 'kinematic_pressure':'p_ref', # Assume kinematic pressure if 'p'\n",
        "                'k':'k_ref', 'turbulentkinetienergy':'k_ref', 'turbulentkineticenergy':'k_ref', 'tke':'k_ref',\n",
        "                'epsilon':'eps_ref', 'turbulencedissipationrate':'eps_ref', 'dissipationrate':'eps_ref', 'dissipation':'eps_ref',\n",
        "                'nut':'nut_ref', 'turbulentviscosity':'nut_ref', 'eddyviscosity':'nut_ref', 'nutilda':'nut_ref' # Check nuTilda if using Spalart-Allmaras\n",
        "            }\n",
        "            rename_dict = {}\n",
        "            processed_cols = set() # Track columns already mapped\n",
        "            # Apply mapping based on lowercase, stripped column names\n",
        "            for col in df_ref.columns:\n",
        "                col_lower = col.lower().strip().replace('_','').replace('-','').replace(' ','') # Normalize heavily\n",
        "                if col_lower in var_map and col not in processed_cols:\n",
        "                    rename_dict[col] = var_map[col_lower]\n",
        "                    processed_cols.add(col) # Mark as processed\n",
        "\n",
        "            # Add coordinate renaming (original column name -> standard name)\n",
        "            rename_dict[x_col] = 'x'\n",
        "            rename_dict[y_col] = 'y'\n",
        "            if z_col: rename_dict[z_col] = 'z'\n",
        "            processed_cols.update([x_col, y_col, z_col] if z_col else [x_col, y_col])\n",
        "\n",
        "            # Check for unmapped columns that might be important\n",
        "            unmapped_cols = [col for col in df_ref.columns if col not in processed_cols]\n",
        "            if unmapped_cols:\n",
        "                 logging.debug(f\"Unmapped columns in reference data: {unmapped_cols}\")\n",
        "\n",
        "            df_ref.rename(columns=rename_dict, inplace=True)\n",
        "            logging.info(f\"Renamed reference columns based on mapping: {rename_dict}\")\n",
        "            logging.info(f\"Columns after rename: {df_ref.columns.tolist()}\")\n",
        "\n",
        "\n",
        "            # --- Check for Required Columns (after renaming) ---\n",
        "            # Define which columns are absolutely essential for comparison plots\n",
        "            required_cols_for_plots = ['x', 'y', 'u_ref'] # Minimal for velocity profile\n",
        "            # Add others based on which plots you intend to generate\n",
        "            if 'plot_profile_comparison' in dir(self): required_cols_for_plots.extend(['p_ref', 'k_ref', 'eps_ref'])\n",
        "            if 'plot_wall_unit_comparison' in dir(self): required_cols_for_plots.extend(['k_ref', 'eps_ref'])\n",
        "            # Add nut_ref if needed\n",
        "            required_cols_for_plots = list(set(required_cols_for_plots)) # Remove duplicates\n",
        "\n",
        "            missing_cols = [col for col in required_cols_for_plots if col not in df_ref.columns]\n",
        "            if missing_cols:\n",
        "                # Raise error only if essential columns like x, y, u are missing\n",
        "                if any(c in missing_cols for c in ['x', 'y', 'u_ref']):\n",
        "                    raise ValueError(f\"Missing essential columns after renaming in reference data: {missing_cols}. Available: {df_ref.columns.tolist()}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Missing some optional columns for plots: {missing_cols}. Comparison plots might be incomplete.\")\n",
        "\n",
        "            # Keep only necessary columns + optional ones if present\n",
        "            cols_to_keep = ['x', 'y'] + [col for col in ['u_ref', 'v_ref', 'p_ref', 'k_ref', 'eps_ref', 'nut_ref'] if col in df_ref.columns]\n",
        "            if z_col and 'z' in df_ref.columns: cols_to_keep.append('z') # Keep z if it was present\n",
        "            df_ref = df_ref[list(set(cols_to_keep))] # Ensure unique columns\n",
        "\n",
        "            # Sort and reset index\n",
        "            df_ref.sort_values(by=['x', 'y'], inplace=True)\n",
        "            df_ref.reset_index(drop=True, inplace=True)\n",
        "            self.ref_data = df_ref\n",
        "            self.has_ref_data = True # Set flag only on successful load and processing\n",
        "            logging.info(f\"Successfully loaded and preprocessed reference CSV data. Final columns: {df_ref.columns.tolist()}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            # Already logged warning, just pass\n",
        "            pass\n",
        "        except ValueError as ve: # Catch specific errors like missing columns\n",
        "            logging.error(f\"ValueError processing reference CSV: {ve}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Unexpected error loading or processing reference CSV: {e}\", exc_info=True)\n",
        "            self.ref_data = None\n",
        "            self.has_ref_data = False\n",
        "\n",
        "    def predict_pinn_fields(self):\n",
        "        \"\"\"Predicts flow fields using the trained PINN model on a grid.\"\"\"\n",
        "        if self.model is None or self.model.net is None:\n",
        "             logging.error(\"PINN Model or network not available for prediction.\")\n",
        "             return False\n",
        "\n",
        "        logging.info(\"Predicting PINN flow fields on evaluation grid...\")\n",
        "        nx = self.plotter_config.NX_PRED\n",
        "        ny = self.plotter_config.NY_PRED\n",
        "        x_coords = np.linspace(0, self.config.L, nx)\n",
        "        y_coords = np.linspace(-self.config.CHANNEL_HALF_HEIGHT, self.config.CHANNEL_HALF_HEIGHT, ny)\n",
        "        self.X_grid, self.Y_grid = np.meshgrid(x_coords, y_coords)\n",
        "        # Create prediction points (N, 2) array\n",
        "        pred_points = np.vstack((np.ravel(self.X_grid), np.ravel(self.Y_grid))).T\n",
        "\n",
        "        try:\n",
        "            # Use model.predict for inference\n",
        "            # Ensure input is float32, as model was likely trained with it\n",
        "            # Convert to numpy explicitly if it's a tensor\n",
        "            if isinstance(pred_points, torch.Tensor):\n",
        "                pred_points_np = pred_points.cpu().numpy()\n",
        "            else:\n",
        "                pred_points_np = np.array(pred_points, dtype=np.float32)\n",
        "\n",
        "            # Check model state before predicting\n",
        "            if not hasattr(self.model, 'sess') and dde.backend.backend_name == \"tensorflow.compat.v1\":\n",
        "                 logging.warning(\"TensorFlow v1 backend detected, but model session (sess) seems unavailable. Prediction might fail.\")\n",
        "            elif not hasattr(self.model, 'net') or self.model.net is None:\n",
        "                 logging.error(\"Model network attribute is missing or None. Cannot predict.\")\n",
        "                 return False\n",
        "\n",
        "            predictions_raw = self.model.predict(pred_points_np) # Predict expects numpy\n",
        "\n",
        "            if predictions_raw is None or not isinstance(predictions_raw, np.ndarray) or predictions_raw.shape[1] != self.config.NETWORK_OUTPUTS:\n",
        "                logging.error(f\"Prediction shape mismatch or invalid type. Expected {self.config.NETWORK_OUTPUTS} outputs, got shape {predictions_raw.shape if predictions_raw is not None else 'None'} and type {type(predictions_raw)}.\")\n",
        "                return False\n",
        "\n",
        "        except AttributeError as ae:\n",
        "             logging.error(f\"AttributeError during PINN prediction (check model state/backend compatibility): {ae}\", exc_info=True)\n",
        "             return False\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during PINN prediction: {e}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "        # --- Process Raw Predictions ---\n",
        "        # Reshape predictions back to grid format (ny, nx)\n",
        "        try:\n",
        "            self.u_pred = predictions_raw[:, 0].reshape(ny, nx)\n",
        "            self.v_pred = predictions_raw[:, 1].reshape(ny, nx)\n",
        "            self.p_prime_pred = predictions_raw[:, 2].reshape(ny, nx)\n",
        "            k_raw_pred = predictions_raw[:, 3].reshape(ny, nx)\n",
        "            eps_raw_pred = predictions_raw[:, 4].reshape(ny, nx)\n",
        "\n",
        "            # Apply inverse transform (exp) and add epsilon for positivity\n",
        "            # Use np.exp for numpy arrays\n",
        "            self.k_pred = np.exp(k_raw_pred) + self.config.EPS_SMALL\n",
        "            self.eps_pred = np.exp(eps_raw_pred) + self.config.EPS_SMALL\n",
        "\n",
        "            # Calculate kinematic pressure p = p' - (2/3)*k (assuming isotropic normal stress contribution)\n",
        "            # This definition assumes p' in the RANS equations represents p_kinematic + (2/3)k\n",
        "            # Verify this assumption based on the specific RANS formulation used.\n",
        "            self.p_pred = self.p_prime_pred - (2.0 / 3.0) * self.k_pred\n",
        "\n",
        "            # Calculate turbulent viscosity nu_t = Cmu * k^2 / eps\n",
        "            # Use np.maximum for safe division with numpy arrays\n",
        "            eps_safe_pred = np.maximum(self.eps_pred, self.config.EPS_SMALL**2) # Use squared epsilon to match units if needed\n",
        "            self.nu_t_pred = self.config.CMU * np.square(self.k_pred) / eps_safe_pred\n",
        "\n",
        "            logging.info(\"PINN field prediction and processing complete.\")\n",
        "            return True\n",
        "        except Exception as proc_e:\n",
        "            logging.error(f\"Error processing raw predictions: {proc_e}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "\n",
        "    def plot_contour_fields(self):\n",
        "        \"\"\"Plots contour fields of the predicted PINN variables.\"\"\"\n",
        "        if self.u_pred is None: # Check if prediction data exists\n",
        "            logging.warning(\"PINN data unavailable for plotting. Run predict_pinn_fields first. Skipping contours.\")\n",
        "            return\n",
        "\n",
        "        logging.info(\"Generating PINN contour plots...\")\n",
        "        try:\n",
        "            # Create a figure with subplots\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 10)) # Adjust figure size as needed\n",
        "            axes = axes.ravel() # Flatten the axes array for easy indexing\n",
        "\n",
        "            cmap_vel = self.plotter_config.CMAP_VELOCITY\n",
        "            cmap_p = self.plotter_config.CMAP_PRESSURE\n",
        "            cmap_turb = self.plotter_config.CMAP_TURBULENCE\n",
        "\n",
        "            # Data to plot: (data_array, title, label, cmap, optional: use_log)\n",
        "            plot_data_list = [\n",
        "                (self.u_pred, 'PINN Streamwise Velocity (u)', 'u (m/s)', cmap_vel),\n",
        "                (self.v_pred, 'PINN Transverse Velocity (v)', 'v (m/s)', cmap_vel),\n",
        "                (self.p_pred, \"PINN Kinematic Pressure (p)\", r'$p/\\rho$ ($m^2/s^2$)', cmap_p),\n",
        "                (self.k_pred, 'PINN TKE (k)', r'$k$ ($m^2/s^2$)', cmap_turb),\n",
        "                (self.eps_pred, 'PINN Dissipation ($\\epsilon$)', r'$\\epsilon$ ($m^2/s^3$)', cmap_turb),\n",
        "                (self.nu_t_pred / self.config.NU, 'PINN Eddy Viscosity Ratio', r'$\\nu_t / \\nu$', cmap_turb, True) # Plot ratio, optionally log scale\n",
        "            ]\n",
        "\n",
        "            for i, (data, title, label, cmap, *log_flag) in enumerate(plot_data_list):\n",
        "                ax = axes[i]\n",
        "                plot_values = data\n",
        "                cbar_label = label\n",
        "                levels = 50 # Number of contour levels\n",
        "                use_log = log_flag[0] if log_flag else False # Check if log flag is provided\n",
        "\n",
        "                # Optional log scale for positive quantities\n",
        "                if use_log and np.nanmin(data) > self.config.EPS_SMALL: # Check if data is positive before log\n",
        "                    try:\n",
        "                        # Floor values slightly above zero before taking log10\n",
        "                        min_positive = np.nanmin(data[data > self.config.EPS_SMALL*10])\n",
        "                        plot_values = np.log10(np.maximum(data, min_positive * 0.01)) # Use nanmin\n",
        "                        cbar_label = f'log10({label})'\n",
        "                        levels = np.logspace(np.log10(min_positive*0.01), np.log10(np.nanmax(data)), levels) # Log spaced levels\n",
        "                        logging.debug(f\"Using log scale for {title}\")\n",
        "                    except Exception as log_err:\n",
        "                         logging.warning(f\"Could not apply log scale for {title}: {log_err}. Using linear scale.\")\n",
        "                         use_log = False # Revert to linear scale on error\n",
        "                         plot_values = data # Ensure plot_values is reset\n",
        "\n",
        "                # Use contourf for filled contours\n",
        "                try:\n",
        "                    if use_log: # Use log levels if calculated\n",
        "                        cf = ax.contourf(self.X_grid, self.Y_grid, plot_values, levels=levels, cmap=cmap, extend='both', locator=plt.LogLocator())\n",
        "                    else: # Linear levels\n",
        "                         cf = ax.contourf(self.X_grid, self.Y_grid, plot_values, levels=levels, cmap=cmap, extend='both')\n",
        "\n",
        "                    fig.colorbar(cf, ax=ax, label=cbar_label)\n",
        "                    ax.set_title(title)\n",
        "                    ax.set_xlabel('x (m)')\n",
        "                    ax.set_ylabel('y (m)')\n",
        "                    ax.set_aspect('equal', adjustable='box') # Make aspect ratio equal\n",
        "                except ValueError as ve:\n",
        "                     logging.error(f\"ValueError during contour plot for {title} (check data range/levels): {ve}\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error plotting contour for {title}: {e}\")\n",
        "\n",
        "            # Hide any unused subplots if necessary (e.g., if plot_data_list has < 6 items)\n",
        "            for j in range(i + 1, len(axes)):\n",
        "                 fig.delaxes(axes[j])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            save_path = os.path.join(self.plots_dir, \"pinn_field_contours.png\")\n",
        "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "            plt.close(fig) # Close the specific figure\n",
        "            logging.info(f\"PINN contour field plots saved to {os.path.basename(save_path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or save contour plots: {e}\", exc_info=True)\n",
        "            # Ensure figure is closed if error occurs mid-plotting\n",
        "            if 'fig' in locals() and plt.fignum_exists(fig.number):\n",
        "                plt.close(fig)\n",
        "\n",
        "    def _estimate_utau(self, data_source='pinn', x_slice_loc=None):\n",
        "        \"\"\"Estimates friction velocity u_tau from data near the wall using gradient and log-law.\"\"\"\n",
        "        if x_slice_loc is None:\n",
        "            # Choose a location away from inlet/outlet, e.g., 80% downstream\n",
        "            x_slice_loc = self.config.L * 0.8\n",
        "\n",
        "        h = self.config.CHANNEL_HALF_HEIGHT\n",
        "        y_p = self.config.Y_P # Use the wall function distance y_p for context, but sample closer\n",
        "        nu = self.config.NU\n",
        "        rho = self.config.RHO # Density needed for stress calculation\n",
        "\n",
        "        # Define two points very near the physical wall (e.g., y+ of ~1 and ~5 if possible)\n",
        "        # This requires an initial guess of u_tau or an iterative approach.\n",
        "        # Simpler: use fixed small distances from the wall.\n",
        "        y_dist_1 = 0.001 * h # Very close to wall\n",
        "        y_dist_2 = 0.01 * h  # Still close, but further than y_dist_1\n",
        "\n",
        "        # Calculate y-coordinates relative to centerline for top wall\n",
        "        y_eval_top_1 = h - y_dist_1\n",
        "        y_eval_top_2 = h - y_dist_2\n",
        "        eval_points_top = np.array([[x_slice_loc, y_eval_top_1], [x_slice_loc, y_eval_top_2]])\n",
        "\n",
        "        # Calculate y-coordinates relative to centerline for bottom wall\n",
        "        y_eval_bot_1 = -h + y_dist_1\n",
        "        y_eval_bot_2 = -h + y_dist_2\n",
        "        eval_points_bot = np.array([[x_slice_loc, y_eval_bot_1], [x_slice_loc, y_eval_bot_2]])\n",
        "\n",
        "        # Average distance from the *nearest* wall (used for y+ estimate later)\n",
        "        y_dist_wall_avg_top = (y_dist_1 + y_dist_2) / 2.0\n",
        "        y_dist_wall_avg_bot = (y_dist_1 + y_dist_2) / 2.0\n",
        "\n",
        "\n",
        "        u1_top, k1_top, eps1_top, u2_top, k2_top, eps2_top = [None]*6\n",
        "        u1_bot, k1_bot, eps1_bot, u2_bot, k2_bot, eps2_bot = [None]*6\n",
        "\n",
        "        try:\n",
        "            # --- Get u, k, eps at the evaluation points ---\n",
        "            interp_method = 'linear' # Start with linear interpolation for reference data\n",
        "\n",
        "            if data_source == 'pinn':\n",
        "                if self.model is None: return None\n",
        "                # Predict for both top and bottom points together\n",
        "                eval_points_all = np.vstack((eval_points_top, eval_points_bot))\n",
        "                pred_raw = self.model.predict(eval_points_all)\n",
        "                if pred_raw is None or pred_raw.shape[0] < 4:\n",
        "                     logging.error(f\"PINN prediction failed or returned insufficient points for u_tau estimate at x={x_slice_loc:.2f}\")\n",
        "                     return None\n",
        "                # Extract and transform\n",
        "                u_all = pred_raw[:, 0]\n",
        "                k_raw_all = pred_raw[:, 3]\n",
        "                eps_raw_all = pred_raw[:, 4]\n",
        "                k_all = np.exp(k_raw_all) + self.config.EPS_SMALL\n",
        "                eps_all = np.exp(eps_raw_all) + self.config.EPS_SMALL\n",
        "\n",
        "                # Split results\n",
        "                u1_top, u2_top, u1_bot, u2_bot = u_all\n",
        "                k1_top, k2_top, k1_bot, k2_bot = k_all\n",
        "                eps1_top, eps2_top, eps1_bot, eps2_bot = eps_all\n",
        "\n",
        "            elif data_source == 'reference' and self.has_ref_data:\n",
        "                if self.ref_data is None: return None\n",
        "                points_ref = self.ref_data[['x', 'y']].values\n",
        "                req_cols = ['u_ref', 'k_ref', 'eps_ref'] # Need these for nu_eff calculation\n",
        "                if not all(col in self.ref_data.columns for col in req_cols):\n",
        "                    logging.warning(f\"Reference data missing required columns {req_cols} for u_tau estimation from gradient.\")\n",
        "                    return None\n",
        "\n",
        "                # Interpolate required values for top and bottom points\n",
        "                values_to_interp = {}\n",
        "                for col in req_cols:\n",
        "                    values_to_interp[col] = self.ref_data[col].values\n",
        "\n",
        "                interp_results = {}\n",
        "                eval_points_all = np.vstack((eval_points_top, eval_points_bot))\n",
        "\n",
        "                for col, values in values_to_interp.items():\n",
        "                    interp_vals = griddata(points_ref, values, eval_points_all, method=interp_method)\n",
        "                    nan_mask = np.isnan(interp_vals)\n",
        "                    if np.any(nan_mask):\n",
        "                        logging.debug(f\"Linear interpolation failed for '{col}' ({data_source}) at x={x_slice_loc:.2f}. Trying 'nearest'.\")\n",
        "                        interp_nearest = griddata(points_ref, values, eval_points_all[nan_mask], method='nearest')\n",
        "                        interp_vals[nan_mask] = interp_nearest\n",
        "                        if np.any(np.isnan(interp_vals)):\n",
        "                             logging.error(f\"Interpolation (linear & nearest) failed for '{col}' ({data_source}) at x={x_slice_loc:.2f}. Cannot estimate u_tau.\")\n",
        "                             return None\n",
        "                    interp_results[col] = interp_vals\n",
        "\n",
        "                # Split results\n",
        "                u1_top, u2_top, u1_bot, u2_bot = interp_results['u_ref']\n",
        "                k1_top, k2_top, k1_bot, k2_bot = interp_results['k_ref']\n",
        "                eps1_top, eps2_top, eps1_bot, eps2_bot = interp_results['eps_ref']\n",
        "\n",
        "            else:\n",
        "                logging.warning(f\"Invalid data_source '{data_source}' or missing data for u_tau estimation.\")\n",
        "                return None\n",
        "\n",
        "            # --- Estimate Gradients and u_tau for Top Wall ---\n",
        "            # Gradient du/dy = (u_further - u_closer) / (y_further - y_closer)\n",
        "            # y_eval_top_2 is further from center, closer to wall than y_eval_top_1\n",
        "            du_dy_top = (u2_top - u1_top) / (y_eval_top_2 - y_eval_top_1) # Should be negative\n",
        "            # Effective viscosity near wall (average of the two points)\n",
        "            k_avg_top = (k1_top + k2_top) / 2.0\n",
        "            eps_avg_top = (eps1_top + eps2_top) / 2.0\n",
        "            nu_t_avg_top = self.config.CMU * k_avg_top**2 / max(eps_avg_top, self.config.EPS_SMALL**2)\n",
        "            nu_eff_avg_top = nu + nu_t_avg_top\n",
        "            # Wall shear stress tau_w = rho * nu_eff * |du/dy| (note the absolute value)\n",
        "            tau_w_top = rho * nu_eff_avg_top * abs(du_dy_top)\n",
        "            u_tau_top = np.sqrt(max(tau_w_top / rho, self.config.EPS_SMALL)) # Ensure non-negative sqrt arg\n",
        "\n",
        "             # --- Estimate Gradients and u_tau for Bottom Wall ---\n",
        "            # y_eval_bot_2 is further from center, closer to wall than y_eval_bot_1\n",
        "            du_dy_bot = (u2_bot - u1_bot) / (y_eval_bot_2 - y_eval_bot_1) # Should be positive\n",
        "            k_avg_bot = (k1_bot + k2_bot) / 2.0\n",
        "            eps_avg_bot = (eps1_bot + eps2_bot) / 2.0\n",
        "            nu_t_avg_bot = self.config.CMU * k_avg_bot**2 / max(eps_avg_bot, self.config.EPS_SMALL**2)\n",
        "            nu_eff_avg_bot = nu + nu_t_avg_bot\n",
        "            tau_w_bot = rho * nu_eff_avg_bot * abs(du_dy_bot)\n",
        "            u_tau_bot = np.sqrt(max(tau_w_bot / rho, self.config.EPS_SMALL))\n",
        "\n",
        "            # --- Average or Choose ---\n",
        "            # Average the top and bottom estimates for a single channel value\n",
        "            u_tau_estimated = (u_tau_top + u_tau_bot) / 2.0\n",
        "\n",
        "            # Optional: Log-law refinement (less reliable if points are deep in viscous sublayer)\n",
        "            # y_plus_est_top = y_dist_wall_avg_top * u_tau_top / nu\n",
        "            # y_plus_est_bot = y_dist_wall_avg_bot * u_tau_bot / nu\n",
        "            # logging.debug(f\"Intermediate u_tau ({data_source}): Top={u_tau_top:.4f} (y+ ~{y_plus_est_top:.1f}), Bottom={u_tau_bot:.4f} (y+ ~{y_plus_est_bot:.1f})\")\n",
        "\n",
        "            logging.info(f\"Estimated u_tau ({data_source}) at x={x_slice_loc:.2f} m: {u_tau_estimated:.4f} m/s (avg of top/bottom grad estimates)\")\n",
        "            return u_tau_estimated\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error estimating u_tau for {data_source} at x={x_slice_loc:.2f}: {e}\", exc_info=True)\n",
        "            return None\n",
        "\n",
        "    def plot_profile_comparison(self):\n",
        "        \"\"\"Plots profiles of PINN vs Reference data at a channel cross-section.\"\"\"\n",
        "        if self.u_pred is None: # Check if prediction data exists\n",
        "            logging.warning(\"PINN data unavailable for plotting. Skipping profile comparison.\")\n",
        "            return\n",
        "        if not self.has_ref_data:\n",
        "            logging.warning(\"Reference CSV data not loaded or failed processing. Skipping profile comparison.\")\n",
        "            return\n",
        "\n",
        "        logging.info(\"Generating profile comparison plots...\")\n",
        "        # Define slice location (e.g., channel midpoint or further downstream)\n",
        "        x_slice_loc = self.config.L * 0.8 # Use same location as u_tau estimate for consistency\n",
        "        ny_pinn = self.plotter_config.NY_PRED # Number of points in y from PINN grid\n",
        "\n",
        "        # Find the closest x-coordinate in the PINN grid\n",
        "        y_coords_pinn = self.Y_grid[:, 0] # y-coordinates from PINN grid\n",
        "        x_coords_pinn = self.X_grid[0, :] # x-coordinates from PINN grid\n",
        "        try:\n",
        "             x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc))\n",
        "             actual_x_pinn = x_coords_pinn[x_slice_idx_pinn] # Actual x used for slicing\n",
        "        except IndexError:\n",
        "             logging.error(\"PINN grid coordinates seem invalid. Cannot find x-slice.\")\n",
        "             return\n",
        "\n",
        "        # Extract PINN data slice at the chosen x-index\n",
        "        pinn_slice = {\n",
        "            'y': y_coords_pinn,\n",
        "            'u': self.u_pred[:, x_slice_idx_pinn],\n",
        "            'v': self.v_pred[:, x_slice_idx_pinn],\n",
        "            'p': self.p_pred[:, x_slice_idx_pinn], # Use calculated kinematic p\n",
        "            'k': self.k_pred[:, x_slice_idx_pinn],\n",
        "            'eps': self.eps_pred[:, x_slice_idx_pinn],\n",
        "            'nut': self.nu_t_pred[:, x_slice_idx_pinn]\n",
        "        }\n",
        "\n",
        "        # --- Interpolate Reference Data onto PINN y-coordinates at the same x ---\n",
        "        ref_slice = {'y': y_coords_pinn} # Initialize dict for interpolated ref data\n",
        "        interpolation_successful = False\n",
        "        try:\n",
        "            if self.ref_data is None: raise ValueError(\"Reference data frame is None.\")\n",
        "\n",
        "            # Points from reference data (x, y)\n",
        "            points_ref = self.ref_data[['x', 'y']].values\n",
        "            # Target points for interpolation (same x, PINN y-coords)\n",
        "            target_points = np.vstack((np.full(ny_pinn, actual_x_pinn), y_coords_pinn)).T\n",
        "\n",
        "            logging.info(f\"Interpolating reference data onto {ny_pinn} points at x={actual_x_pinn:.3f}...\")\n",
        "\n",
        "            # Interpolate each variable present in the reference data\n",
        "            variables_to_interpolate = [\n",
        "                ('u_ref', 'u'), ('v_ref', 'v'), ('p_ref', 'p'),\n",
        "                ('k_ref', 'k'), ('eps_ref', 'eps'), ('nut_ref', 'nut')\n",
        "            ]\n",
        "            missing_ref_vars = []\n",
        "            for var_ref, var_pinn in variables_to_interpolate:\n",
        "                if var_ref in self.ref_data.columns:\n",
        "                    values_ref = self.ref_data[var_ref].values\n",
        "                    # Linear interpolation\n",
        "                    interp_values = griddata(points_ref, values_ref, target_points, method='linear')\n",
        "                    # Handle NaNs with nearest neighbor fallback\n",
        "                    nan_mask = np.isnan(interp_values)\n",
        "                    num_nans = np.sum(nan_mask)\n",
        "                    if num_nans > 0:\n",
        "                        logging.debug(f\"{num_nans} NaNs found for '{var_ref}' after linear interp. Trying nearest neighbor.\")\n",
        "                        # Only interpolate NaN points with nearest\n",
        "                        interp_nearest = griddata(points_ref, values_ref, target_points[nan_mask], method='nearest')\n",
        "                        interp_values[nan_mask] = interp_nearest\n",
        "                        if np.any(np.isnan(interp_values)): # Check if nearest also failed\n",
        "                             logging.warning(f\"Interpolation (linear & nearest) failed for '{var_ref}' at some points. Profile will be incomplete.\")\n",
        "                    ref_slice[var_pinn] = interp_values\n",
        "                else:\n",
        "                    logging.debug(f\"Reference variable '{var_ref}' not found in CSV data. Skipping interpolation.\")\n",
        "                    ref_slice[var_pinn] = np.full(ny_pinn, np.nan) # Fill with NaN if missing\n",
        "                    missing_ref_vars.append(var_ref)\n",
        "\n",
        "            interpolation_successful = True # Mark as successful if loop completes\n",
        "            if missing_ref_vars:\n",
        "                 logging.warning(f\"Could not interpolate reference variables: {missing_ref_vars}\")\n",
        "\n",
        "        except ValueError as ve:\n",
        "             logging.error(f\"ValueError during reference data interpolation: {ve}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error interpolating reference data for profiles: {e}\", exc_info=True)\n",
        "            # If interpolation fails, mark ref data as unavailable for this plot\n",
        "            interpolation_successful = False\n",
        "\n",
        "        # --- Create Plots ---\n",
        "        try:\n",
        "            fig, axes = plt.subplots(3, 2, figsize=(12, 15)) # Adjust size\n",
        "            axes = axes.ravel()\n",
        "            plot_idx = 0\n",
        "            h = self.config.CHANNEL_HALF_HEIGHT\n",
        "            plot_vars = [ # Variables to plot, their names, and units\n",
        "                ('u', 'Velocity u', 'm/s'),\n",
        "                ('v', 'Velocity v', 'm/s'),\n",
        "                ('p', 'Kinematic Pressure p', r'$m^2/s^2$'),\n",
        "                ('k', 'TKE k', r'$m^2/s^2$'),\n",
        "                ('eps', 'Dissipation eps', r'$m^2/s^3$'),\n",
        "                ('nut', 'Eddy Viscosity nu_t', r'$m^2/s$')\n",
        "            ]\n",
        "\n",
        "            for key, name, unit in plot_vars:\n",
        "                if plot_idx >= len(axes): break # Avoid index error if fewer plots than axes\n",
        "                ax = axes[plot_idx]\n",
        "\n",
        "                # Plot PINN data\n",
        "                ax.plot(pinn_slice[key], pinn_slice['y'] / h, 'r-', linewidth=2, label='PINN')\n",
        "\n",
        "                # Plot Reference data if interpolation was successful and data exists\n",
        "                if interpolation_successful and key in ref_slice and not np.all(np.isnan(ref_slice[key])):\n",
        "                    ax.plot(ref_slice[key], ref_slice['y'] / h, 'b--', linewidth=1.5, label='Reference (CSV)')\n",
        "                elif not interpolation_successful and key != 'y': # Add empty placeholder if interp failed\n",
        "                     ax.plot([], [], 'b--', label='Reference (Failed)')\n",
        "\n",
        "                ax.set_xlabel(f'{name} ({unit})')\n",
        "                ax.set_ylabel('y/h') # Normalize y by half-height\n",
        "                ax.set_title(f'{name} Profile') # Title moved to suptitle\n",
        "                ax.legend(fontsize=8)\n",
        "                ax.grid(True, linestyle=':')\n",
        "\n",
        "                # Use log scale for x-axis for turbulence quantities (k, eps, nut) if values are positive\n",
        "                if key in ['k', 'eps', 'nut']:\n",
        "                     try:\n",
        "                         # Check if plotted values are sufficiently positive\n",
        "                         min_val_for_log = self.config.EPS_SMALL\n",
        "                         pinn_valid = np.nanmin(pinn_slice[key]) > min_val_for_log\n",
        "                         ref_valid = False\n",
        "                         if interpolation_successful and key in ref_slice and not np.all(np.isnan(ref_slice[key])):\n",
        "                             ref_valid = np.nanmin(ref_slice[key]) > min_val_for_log\n",
        "\n",
        "                         if pinn_valid and (ref_valid or not interpolation_successful): # Allow log if ref failed but PINN is ok\n",
        "                              ax.set_xscale('log')\n",
        "                              ax.grid(True, which='both', linestyle=':') # Add minor grid for log scale\n",
        "                              logging.debug(f\"Using log scale for {key} profile.\")\n",
        "                     except ValueError: # Handle cases where data might be exactly zero or negative\n",
        "                           logging.warning(f\"Could not apply log scale for {key} (likely non-positive values).\")\n",
        "                     except Exception as log_e:\n",
        "                          logging.warning(f\"Error applying log scale for {key}: {log_e}\")\n",
        "\n",
        "                plot_idx += 1\n",
        "\n",
        "            # Hide unused axes\n",
        "            for j in range(plot_idx, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "\n",
        "            plt.suptitle(f'Profile Comparison at x ≈ {actual_x_pinn:.3f} m', fontsize=16)\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "\n",
        "            save_path = os.path.join(self.plots_dir, \"profile_comparison_pinn_vs_csv.png\")\n",
        "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "            plt.close(fig)\n",
        "            logging.info(f\"Profile comparison plot saved to {os.path.basename(save_path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or save profile comparison plot: {e}\", exc_info=True)\n",
        "            if 'fig' in locals() and plt.fignum_exists(fig.number):\n",
        "                 plt.close(fig)\n",
        "\n",
        "\n",
        "    def plot_wall_unit_comparison(self):\n",
        "        \"\"\"Plots profiles in wall units (y+, U+, k+, eps+) vs reference/theory.\"\"\"\n",
        "        if self.u_pred is None: # Check if prediction data exists\n",
        "            logging.warning(\"PINN data unavailable for plotting. Skipping wall unit plots.\")\n",
        "            return\n",
        "\n",
        "        logging.info(\"Generating wall unit comparison plots...\")\n",
        "\n",
        "        # --- Estimate Friction Velocity (u_tau) ---\n",
        "        # Use a location away from inlet/outlet for potentially more developed flow\n",
        "        x_slice_loc_utau = self.config.L * 0.8\n",
        "        self.pinn_data_utau = self._estimate_utau(data_source='pinn', x_slice_loc=x_slice_loc_utau)\n",
        "        if self.has_ref_data:\n",
        "            self.ref_data_utau = self._estimate_utau(data_source='reference', x_slice_loc=x_slice_loc_utau)\n",
        "        else:\n",
        "            self.ref_data_utau = None\n",
        "\n",
        "        # Proceed only if PINN u_tau could be estimated\n",
        "        if not self.pinn_data_utau:\n",
        "            logging.error(\"Could not estimate PINN u_tau. Skipping wall unit plots.\")\n",
        "            return\n",
        "        if self.has_ref_data and not self.ref_data_utau:\n",
        "            logging.warning(\"Could not estimate reference u_tau. Plotting PINN wall units only vs theory.\")\n",
        "\n",
        "        # --- Prepare Data for Wall Units ---\n",
        "        nu = self.config.NU\n",
        "        h = self.config.CHANNEL_HALF_HEIGHT\n",
        "        kappa = self.config.KAPPA\n",
        "        # Use E+ (often denoted B or C+ in literature) instead of E_wall for log-law plot\n",
        "        # E+ = E_wall * exp(kappa * B_offset) if B_offset is used, but often just a constant ~5.0-5.5\n",
        "        # Let's use a typical value, adjust if needed based on expected Re_tau\n",
        "        B_const = 5.2 # Typical log-law intercept constant for smooth walls (E+ or C+)\n",
        "\n",
        "        # Use the same x-slice as the profile plots or the u_tau estimate location\n",
        "        x_slice_loc_plot = x_slice_loc_utau # Use u_tau location for consistency\n",
        "        y_coords_pinn = self.Y_grid[:, 0]\n",
        "        x_coords_pinn = self.X_grid[0, :]\n",
        "        try:\n",
        "            x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc_plot))\n",
        "            actual_x_pinn = x_coords_pinn[x_slice_idx_pinn]\n",
        "        except IndexError:\n",
        "             logging.error(\"PINN grid coordinates seem invalid. Cannot find x-slice for wall units.\")\n",
        "             return\n",
        "\n",
        "\n",
        "        # Extract PINN data near one wall (e.g., top wall: y >= 0) for clarity\n",
        "        # Could potentially average top/bottom walls if symmetry is expected\n",
        "        wall_indices_pinn = y_coords_pinn >= -self.config.EPS_SMALL # Include centerline point if present\n",
        "        y_wall_pinn = y_coords_pinn[wall_indices_pinn]\n",
        "        # Distance from the NEAREST wall (top wall if y>=0, bottom wall if y<0)\n",
        "        # For top wall (y>=0): distance = h - y\n",
        "        y_dist_wall_pinn = np.maximum(h - y_wall_pinn, self.config.EPS_SMALL * h) # Avoid zero distance\n",
        "\n",
        "        # Get corresponding u, k, eps from the PINN slice\n",
        "        u_wall_pinn = self.u_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
        "        k_wall_pinn = self.k_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
        "        eps_wall_pinn = self.eps_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
        "\n",
        "        # Calculate wall units for PINN data\n",
        "        utau_pinn_safe = max(self.pinn_data_utau, self.config.EPS_SMALL) # Avoid division by zero\n",
        "        y_plus_pinn = y_dist_wall_pinn * utau_pinn_safe / nu\n",
        "        u_plus_pinn = u_wall_pinn / utau_pinn_safe\n",
        "        # k+ = k / u_tau^2\n",
        "        k_plus_pinn = k_wall_pinn / max(utau_pinn_safe**2, self.config.EPS_SMALL**2)\n",
        "        # eps+ = eps * nu / u_tau^4\n",
        "        eps_plus_pinn = eps_wall_pinn * nu / max(utau_pinn_safe**4, self.config.EPS_SMALL**4)\n",
        "\n",
        "        # Sort PINN data by y+ for plotting lines correctly\n",
        "        sort_idx_pinn = np.argsort(y_plus_pinn)\n",
        "        y_plus_pinn = y_plus_pinn[sort_idx_pinn]\n",
        "        u_plus_pinn = u_plus_pinn[sort_idx_pinn]\n",
        "        k_plus_pinn = k_plus_pinn[sort_idx_pinn]\n",
        "        eps_plus_pinn = eps_plus_pinn[sort_idx_pinn]\n",
        "\n",
        "        # --- Prepare Reference Data (if available and u_tau estimated) ---\n",
        "        y_plus_ref, u_plus_ref, k_plus_ref, eps_plus_ref = None, None, None, None\n",
        "        ref_processed = False\n",
        "        if self.has_ref_data and self.ref_data_utau:\n",
        "            try:\n",
        "                # Filter reference data near the chosen x-slice and top wall (y>=0)\n",
        "                # Use a tolerance for x matching\n",
        "                ref_wall_data = self.ref_data[\n",
        "                    (np.isclose(self.ref_data['x'], actual_x_pinn, rtol=0.05, atol=0.1*self.config.L)) & # Wider tolerance for x\n",
        "                    (self.ref_data['y'] >= -self.config.EPS_SMALL) # Include y=0\n",
        "                ].copy()\n",
        "\n",
        "                if not ref_wall_data.empty:\n",
        "                    y_wall_ref = ref_wall_data['y'].values\n",
        "                    y_dist_wall_ref = np.maximum(h - y_wall_ref, self.config.EPS_SMALL * h)\n",
        "                    utau_ref_safe = max(self.ref_data_utau, self.config.EPS_SMALL)\n",
        "                    y_plus_ref = y_dist_wall_ref * utau_ref_safe / nu\n",
        "\n",
        "                    # Check if required columns exist before accessing\n",
        "                    if 'u_ref' in ref_wall_data.columns: u_plus_ref = ref_wall_data['u_ref'].values / utau_ref_safe\n",
        "                    if 'k_ref' in ref_wall_data.columns: k_plus_ref = ref_wall_data['k_ref'].values / max(utau_ref_safe**2, self.config.EPS_SMALL**2)\n",
        "                    if 'eps_ref' in ref_wall_data.columns: eps_plus_ref = ref_wall_data['eps_ref'].values * nu / max(utau_ref_safe**4, self.config.EPS_SMALL**4)\n",
        "\n",
        "                    # Sort reference data by y+\n",
        "                    sort_idx_ref = np.argsort(y_plus_ref)\n",
        "                    y_plus_ref = y_plus_ref[sort_idx_ref]\n",
        "                    if u_plus_ref is not None: u_plus_ref = u_plus_ref[sort_idx_ref]\n",
        "                    if k_plus_ref is not None: k_plus_ref = k_plus_ref[sort_idx_ref]\n",
        "                    if eps_plus_ref is not None: eps_plus_ref = eps_plus_ref[sort_idx_ref]\n",
        "                    logging.info(f\"Processed {len(y_plus_ref)} reference points for wall unit comparison.\")\n",
        "                    ref_processed = True\n",
        "                else:\n",
        "                    logging.warning(f\"No reference data found near x={actual_x_pinn:.3f}, y>=0 for wall unit plots.\")\n",
        "            except KeyError as ke:\n",
        "                 logging.error(f\"Missing column in reference data needed for wall units: {ke}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing reference data for wall units: {e}\", exc_info=True)\n",
        "                # Prevent plotting bad ref data\n",
        "                y_plus_ref, u_plus_ref, k_plus_ref, eps_plus_ref = None, None, None, None\n",
        "\n",
        "        # --- Create Wall Unit Plots ---\n",
        "        try:\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 5.5)) # Figure for U+, k+, eps+\n",
        "\n",
        "            # Determine plot limits dynamically\n",
        "            y_plus_max_pinn = np.max(y_plus_pinn) if len(y_plus_pinn) > 0 else 100\n",
        "            y_plus_max_ref = np.max(y_plus_ref) if ref_processed and y_plus_ref is not None and len(y_plus_ref) > 0 else y_plus_max_pinn\n",
        "            y_plus_max_plot = 1.1 * max(y_plus_max_pinn, y_plus_max_ref, self.config.YP_PLUS_TARGET * 1.5) # Ensure target y+ visible, extend slightly\n",
        "\n",
        "            u_plus_max_pinn = np.max(u_plus_pinn) if len(u_plus_pinn) > 0 else 25\n",
        "            u_plus_max_ref = np.max(u_plus_ref) if ref_processed and u_plus_ref is not None and len(u_plus_ref) > 0 else u_plus_max_pinn\n",
        "            u_plus_max_plot = 1.1 * max(u_plus_max_pinn, u_plus_max_ref)\n",
        "\n",
        "            k_plus_max_pinn = np.max(k_plus_pinn) if len(k_plus_pinn) > 0 else 5\n",
        "            k_plus_max_ref = np.max(k_plus_ref) if ref_processed and k_plus_ref is not None and len(k_plus_ref) > 0 else k_plus_max_pinn\n",
        "            k_plus_max_plot = 1.1 * max(k_plus_max_pinn, k_plus_max_ref)\n",
        "\n",
        "\n",
        "            # 1. U+ vs y+ plot\n",
        "            ax = axes[0]\n",
        "            ax.semilogx(y_plus_pinn, u_plus_pinn, 'r.', ms=4, label=f'PINN ($u_\\\\tau \\\\approx {self.pinn_data_utau:.3f}$)')\n",
        "            if ref_processed and y_plus_ref is not None and u_plus_ref is not None:\n",
        "                ax.semilogx(y_plus_ref, u_plus_ref, 'bo', mfc='none', ms=5, label=f'Ref ($u_\\\\tau \\\\approx {self.ref_data_utau:.3f}$)' if self.ref_data_utau else 'Ref (u_tau N/A)')\n",
        "            # Theoretical laws\n",
        "            y_plus_log_min = 11 # Start log law around y+=11\n",
        "            y_plus_theory_log = np.logspace(np.log10(max(y_plus_log_min, 1)), np.log10(y_plus_max_plot*1.1), 100) # Extend slightly beyond max y+\n",
        "            u_plus_loglaw = (1 / kappa) * np.log(y_plus_theory_log) + B_const\n",
        "            y_plus_theory_vis = np.linspace(0.1, 30, 50) # Viscous sublayer range (y+ < 5), buffer (y+ 5-30)\n",
        "            u_plus_viscous = y_plus_theory_vis # U+ = y+\n",
        "            ax.semilogx(y_plus_theory_log, u_plus_loglaw, 'k:', lw=1.5, label=f'Log Law ($\\\\kappa={kappa}, B={B_const}$)')\n",
        "            ax.semilogx(y_plus_theory_vis, u_plus_viscous, 'k--', lw=1.5, label='Viscous ($U^+=y^+$)')\n",
        "            ax.set_xlabel('$y^+$')\n",
        "            ax.set_ylabel('$U^+$')\n",
        "            ax.set_title(f'$U^+$ vs $y^+$ Profile')\n",
        "            ax.legend(fontsize=9)\n",
        "            ax.grid(True, which='both', ls=':')\n",
        "            ax.set_ylim(bottom=0, top=u_plus_max_plot)\n",
        "            ax.set_xlim(left=0.1, right=y_plus_max_plot) # Start x-axis slightly > 0 for log scale\n",
        "\n",
        "\n",
        "            # 2. k+ vs y+ plot\n",
        "            ax = axes[1]\n",
        "            ax.semilogx(y_plus_pinn, k_plus_pinn, 'r.', ms=4, label='PINN')\n",
        "            if ref_processed and y_plus_ref is not None and k_plus_ref is not None:\n",
        "                ax.semilogx(y_plus_ref, k_plus_ref, 'bo', mfc='none', ms=5, label='Reference')\n",
        "            # Add line for target y+ location used in wall function (if defined)\n",
        "            if hasattr(self.config, 'YP_PLUS_TARGET'):\n",
        "                 ax.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'WF $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
        "            ax.set_xlabel('$y^+$')\n",
        "            ax.set_ylabel('$k^+$')\n",
        "            ax.set_title('$k^+$ vs $y^+$ Profile')\n",
        "            ax.legend(fontsize=9)\n",
        "            ax.grid(True, which='both', ls=':')\n",
        "            ax.set_ylim(bottom=0, top=k_plus_max_plot) # k+ starts from 0\n",
        "            ax.set_xlim(left=0.1, right=y_plus_max_plot)\n",
        "\n",
        "\n",
        "            # 3. eps+ vs y+ plot (log-log scale often used)\n",
        "            ax = axes[2]\n",
        "            # Filter out non-positive values before plotting on log-log scale\n",
        "            valid_idx_pinn = eps_plus_pinn > self.config.EPS_SMALL**2 # Use small threshold\n",
        "            if np.any(valid_idx_pinn):\n",
        "                 ax.loglog(y_plus_pinn[valid_idx_pinn], eps_plus_pinn[valid_idx_pinn], 'r.', ms=4, label='PINN')\n",
        "            else: ax.plot([],[], 'r.', label='PINN (No positive data)') # Placeholder\n",
        "\n",
        "            if ref_processed and y_plus_ref is not None and eps_plus_ref is not None:\n",
        "                 valid_idx_ref = eps_plus_ref > self.config.EPS_SMALL**2\n",
        "                 if np.any(valid_idx_ref):\n",
        "                     ax.loglog(y_plus_ref[valid_idx_ref], eps_plus_ref[valid_idx_ref], 'bo', mfc='none', ms=5, label='Reference')\n",
        "                 else: ax.plot([],[], 'bo', mfc='none', label='Reference (No positive data)')\n",
        "\n",
        "            # Theoretical trend near wall (local equilibrium): eps+ ~ 1 / (kappa * y+) -> C / y+\n",
        "            # Valid mainly away from y+=0\n",
        "            y_plus_theory_eps = np.logspace(np.log10(max(1, 0.1)), np.log10(y_plus_max_plot*1.1), 100)\n",
        "            eps_plus_target_theory = 1.0 / (kappa * y_plus_theory_eps) # Simplified equilibrium scaling\n",
        "            ax.loglog(y_plus_theory_eps, eps_plus_target_theory, 'k:', lw=1.5, label='$\\\\epsilon^+ \\\\propto 1/y^+$')\n",
        "\n",
        "            if hasattr(self.config, 'YP_PLUS_TARGET'):\n",
        "                ax.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'WF $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
        "\n",
        "            ax.set_xlabel('$y^+$')\n",
        "            ax.set_ylabel('$\\\\epsilon^+$')\n",
        "            ax.set_title('$\\\\epsilon^+$ vs $y^+$ Profile (log-log)')\n",
        "            ax.legend(fontsize=9)\n",
        "            ax.grid(True, which='both', ls=':')\n",
        "\n",
        "            # Set reasonable y-limits for eps+ based on observed data\n",
        "            min_eps_plus_data = np.min(eps_plus_pinn[valid_idx_pinn]) if np.any(valid_idx_pinn) else 1e-5\n",
        "            max_eps_plus_data = np.max(eps_plus_pinn[valid_idx_pinn]) if np.any(valid_idx_pinn) else 1\n",
        "            if ref_processed and eps_plus_ref is not None and np.any(valid_idx_ref):\n",
        "                  min_eps_plus_data = min(min_eps_plus_data, np.min(eps_plus_ref[valid_idx_ref]))\n",
        "                  max_eps_plus_data = max(max_eps_plus_data, np.max(eps_plus_ref[valid_idx_ref]))\n",
        "\n",
        "            ax.set_ylim(bottom=max(min_eps_plus_data * 0.1, 1e-6), top=max_eps_plus_data * 10)\n",
        "            ax.set_xlim(left=0.1, right=y_plus_max_plot)\n",
        "\n",
        "\n",
        "            plt.suptitle(f'Wall Unit Profiles (Top Wall, x ≈ {actual_x_pinn:.3f}m)', fontsize=16)\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.93]) # Adjust layout for suptitle\n",
        "\n",
        "            save_path = os.path.join(self.plots_dir, \"profile_comparison_wall_units.png\")\n",
        "            plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "            plt.close(fig)\n",
        "            logging.info(f\"Wall unit comparison plots saved to {os.path.basename(save_path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or save wall unit comparison plot: {e}\", exc_info=True)\n",
        "            if 'fig' in locals() and plt.fignum_exists(fig.number):\n",
        "                 plt.close(fig)\n",
        "\n",
        "    def plot_pressure_gradient_comparison(self):\n",
        "        \"\"\"Plots the streamwise pressure gradient dp/dx along the centerline.\"\"\"\n",
        "        if self.p_pred is None: # Check if PINN kinematic pressure is available\n",
        "            logging.warning(\"PINN pressure data unavailable. Skipping pressure gradient plot.\")\n",
        "            return\n",
        "\n",
        "        logging.info(\"Generating centerline pressure gradient comparison plot...\")\n",
        "        try:\n",
        "            x_coords_pinn = self.X_grid[0, :] # Streamwise coordinates from PINN grid\n",
        "            y_coords_pinn = self.Y_grid[:, 0] # Transverse coordinates\n",
        "            # Find index closest to centerline y=0\n",
        "            center_idx_pinn = np.argmin(np.abs(y_coords_pinn - 0.0))\n",
        "            actual_y_center = y_coords_pinn[center_idx_pinn]\n",
        "\n",
        "            # Extract PINN kinematic pressure along centerline\n",
        "            p_centerline_pinn = self.p_pred[center_idx_pinn, :]\n",
        "            # Calculate gradient using numpy.gradient (central difference)\n",
        "            dp_dx_pinn = np.gradient(p_centerline_pinn, x_coords_pinn)\n",
        "\n",
        "            # --- Calculate Reference Pressure Gradient (if data available) ---\n",
        "            dp_dx_ref = None\n",
        "            x_coords_ref = None\n",
        "            ref_grad_calculated = False\n",
        "            if self.has_ref_data and 'p_ref' in self.ref_data.columns:\n",
        "                try:\n",
        "                    # Filter reference data near centerline (allow some tolerance)\n",
        "                    centerline_tol = 0.05 * self.config.CHANNEL_HALF_HEIGHT\n",
        "                    ref_centerline = self.ref_data[\n",
        "                        np.abs(self.ref_data['y']) <= centerline_tol\n",
        "                    ].copy()\n",
        "\n",
        "                    if not ref_centerline.empty:\n",
        "                        # If multiple y-values close to center, average pressure at each unique x\n",
        "                        # Group by x and calculate the mean pressure\n",
        "                        centerline_grouped = ref_centerline.groupby('x')['p_ref'].mean()\n",
        "                        # Sort by x just in case grouping changed order\n",
        "                        centerline_grouped = centerline_grouped.sort_index()\n",
        "\n",
        "                        if len(centerline_grouped) > 5: # Need sufficient points for reliable gradient\n",
        "                            x_coords_ref = centerline_grouped.index.values\n",
        "                            p_centerline_ref = centerline_grouped.values\n",
        "                            if len(x_coords_ref) > 1:\n",
        "                                dp_dx_ref = np.gradient(p_centerline_ref, x_coords_ref)\n",
        "                                logging.info(f\"Calculated reference pressure gradient from {len(x_coords_ref)} centerline points.\")\n",
        "                                ref_grad_calculated = True\n",
        "                            else: logging.warning(\"Not enough unique x-points in reference centerline data for gradient.\")\n",
        "                        else: logging.warning(f\"Not enough grouped x-points ({len(centerline_grouped)}) near centerline in reference data for gradient.\")\n",
        "                    else: logging.warning(\"No points found near centerline in reference data.\")\n",
        "                except KeyError as ke:\n",
        "                     logging.warning(f\"Could not calculate reference pressure gradient due to missing column: {ke}\")\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Could not calculate reference pressure gradient: {e}\")\n",
        "            elif self.has_ref_data:\n",
        "                logging.warning(\"Reference data loaded, but 'p_ref' column missing. Cannot plot reference pressure gradient.\")\n",
        "\n",
        "            # --- Create Plot ---\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            ax.plot(x_coords_pinn, dp_dx_pinn, 'r-', lw=2, label='PINN $dp/dx$')\n",
        "            if ref_grad_calculated and dp_dx_ref is not None and x_coords_ref is not None:\n",
        "                ax.plot(x_coords_ref, dp_dx_ref, 'b--', lw=1.5, label='Reference $dp/dx$ (CSV)')\n",
        "            elif self.has_ref_data: # Add placeholder if ref data exists but grad failed\n",
        "                ax.plot([], [], 'b--', label='Reference $dp/dx$ (Failed)')\n",
        "\n",
        "\n",
        "            ax.set_xlabel('x / L') # Normalize x-axis by channel length\n",
        "            ax.set_ylabel(r'$dp/dx$ $(m/s^2)$') # Assuming kinematic pressure p\n",
        "            ax.set_title(f'Streamwise Kinematic Pressure Gradient along Centerline (y ≈ {actual_y_center:.3f}m)')\n",
        "            ax.legend()\n",
        "            ax.grid(True, ls=':')\n",
        "            # Normalize x coordinates for axis limits\n",
        "            ax.set_xlim(0, 1) # Range from 0 to L\n",
        "\n",
        "            # Optional: Set y-limits based on expected range (often negative and near constant in developed region)\n",
        "            try:\n",
        "               # Focus on the developed region (e.g., latter half, excluding outlet proximity)\n",
        "               focus_start_idx = len(dp_dx_pinn) // 2\n",
        "               focus_end_idx = int(len(dp_dx_pinn) * 0.95) # Exclude last 5%\n",
        "               if focus_start_idx < focus_end_idx : # Ensure valid slice\n",
        "                    focus_region_pinn = dp_dx_pinn[focus_start_idx:focus_end_idx]\n",
        "                    if len(focus_region_pinn) > 0:\n",
        "                         mean_dpdx = np.mean(focus_region_pinn)\n",
        "                         std_dpdx = np.std(focus_region_pinn)\n",
        "                         # Set ylim to mean +/- a few std deviations, or use padding\n",
        "                         pad = 5 * max(std_dpdx, abs(mean_dpdx)*0.1, 1e-4) # Ensure pad is reasonable\n",
        "                         ax.set_ylim(mean_dpdx - pad, mean_dpdx + pad)\n",
        "                         logging.debug(f\"Adjusted pressure gradient plot ylim based on developed region: [{mean_dpdx-pad:.2e}, {mean_dpdx+pad:.2e}]\")\n",
        "            except Exception as ylim_e:\n",
        "                 logging.warning(f\"Could not automatically set y-limits for pressure gradient plot: {ylim_e}\")\n",
        "\n",
        "            # Normalize x-axis ticks\n",
        "            ax.set_xticks(np.linspace(0, self.config.L, 6))\n",
        "            ax.set_xticklabels([f\"{x/self.config.L:.1f}\" for x in np.linspace(0, self.config.L, 6)])\n",
        "            ax.set_xlabel('x / L')\n",
        "\n",
        "\n",
        "            plt.tight_layout()\n",
        "            save_path = os.path.join(self.plots_dir, \"pressure_gradient_comparison.png\")\n",
        "            plt.savefig(save_path, dpi=200)\n",
        "            plt.close(fig)\n",
        "            logging.info(f\"Pressure gradient comparison plot saved to {os.path.basename(save_path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or save pressure gradient plot: {e}\", exc_info=True)\n",
        "            if 'fig' in locals() and plt.fignum_exists(fig.number):\n",
        "                 plt.close(fig)\n",
        "\n",
        "    def run_post_processing(self):\n",
        "        \"\"\"Runs the full post-processing sequence.\"\"\"\n",
        "        if self.model is None:\n",
        "             logging.error(\"Model object is None. Cannot run post-processing.\")\n",
        "             return\n",
        "\n",
        "        logging.info(\"--- Starting Full Post-Processing ---\")\n",
        "        # Plot loss history regardless of prediction success\n",
        "        self.plot_loss_history()\n",
        "\n",
        "        # Prediction is required for all field/profile plots\n",
        "        prediction_successful = self.predict_pinn_fields()\n",
        "\n",
        "        if prediction_successful:\n",
        "            # Plot contours based on prediction\n",
        "            self.plot_contour_fields()\n",
        "\n",
        "            # Load reference data only if prediction was successful (needed for comparisons)\n",
        "            self.load_reference_data() # Sets self.has_ref_data flag\n",
        "\n",
        "            # Proceed with comparisons if both prediction and ref data are available\n",
        "            if self.has_ref_data:\n",
        "                logging.info(\"Proceeding with PINN vs Reference CSV comparisons...\")\n",
        "                self.plot_profile_comparison()\n",
        "                self.plot_wall_unit_comparison() # Depends on _estimate_utau which needs ref data\n",
        "                self.plot_pressure_gradient_comparison() # Depends on ref data pressure\n",
        "            else:\n",
        "                logging.warning(\"Skipping comparison plots as reference data is unavailable or failed to load.\")\n",
        "        else:\n",
        "            logging.error(\"PINN field prediction failed. Aborting further post-processing that depends on predictions.\")\n",
        "\n",
        "        logging.info(\"--- Post-Processing Finished ---\")\n",
        "# --- End Plotter Class ---\n",
        "\n",
        "\n",
        "# =============================\n",
        "# ===== Main Execution Block =====\n",
        "# =============================\n",
        "if __name__ == \"__main__\":\n",
        "    main_start_time = time.time()\n",
        "\n",
        "    # --- 1. Initial Setup ---\n",
        "    main_cfg = Config() # Instantiate default config\n",
        "    main_plot_cfg = PlotterConfig()\n",
        "\n",
        "    # --- Google Drive Mount (if applicable) ---\n",
        "    # This will update main_cfg paths if running in Colab and mount is successful\n",
        "    # Run this *before* setting up logging/dirs based on potentially updated paths\n",
        "    mount_drive(main_cfg.DRIVE_MOUNT_POINT)\n",
        "\n",
        "    # --- Setup Output Dirs and Logging ---\n",
        "    # These must run AFTER mount_drive potentially updates main_cfg.OUTPUT_DIR etc.\n",
        "    setup_output_directories(main_cfg)\n",
        "    setup_logging(main_cfg.LOG_FILE) # Configure logging (will log to updated path)\n",
        "\n",
        "    logging.info(\"=\"*60); logging.info(\" PINN RANS k-epsilon Channel Flow Simulation Start \"); logging.info(\"=\"*60)\n",
        "    log_configuration(main_cfg, main_plot_cfg) # Log the final configuration used\n",
        "\n",
        "    # --- 2. Define Boundaries ---\n",
        "    try:\n",
        "        # Pass the config object to the function\n",
        "        bcs, anchor_points = get_boundary_conditions(main_cfg)\n",
        "        logging.info(f\"Defined {len(bcs)} boundary conditions.\")\n",
        "        if anchor_points is not None and len(anchor_points) > 0:\n",
        "            # Logging moved to build_model where 'anchors' is used\n",
        "            pass\n",
        "            # logging.info(f\"Using {anchor_points.shape[0]} anchor points for wall functions.\")\n",
        "        else:\n",
        "             # Should have anchor points if using wall functions as defined\n",
        "             logging.warning(\"No anchor points generated for wall functions, check get_boundary_conditions.\")\n",
        "             # Ensure anchor_points is None or empty list if not generated, for build_model compatibility\n",
        "             anchor_points = None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to define boundary conditions: {e}\", exc_info=True)\n",
        "        sys.exit(1) # Exit if BCs cannot be defined\n",
        "\n",
        "    # --- 3. Training Phase ---\n",
        "    model_trained, history_trained, state_trained = None, None, None\n",
        "    training_successful = False\n",
        "    try:\n",
        "        # Pass the config object to the Trainer\n",
        "        trainer = Trainer(main_cfg)\n",
        "        # Pass anchor points to build_model\n",
        "        trainer.build_model(bcs, anchor_points)\n",
        "        # Execute the updated training method\n",
        "        model_trained, history_trained, state_trained = trainer.train()\n",
        "\n",
        "        # Check if training outputs seem valid\n",
        "        if model_trained is not None and history_trained is not None and state_trained is not None:\n",
        "             # Add more checks? e.g., check if final loss is reasonable (not NaN/inf)\n",
        "             final_loss = history_trained.loss_train[-1] if history_trained.loss_train else float('inf')\n",
        "             if np.isfinite(np.sum(final_loss)): # Check if final loss is finite\n",
        "                 training_successful = True\n",
        "                 logging.info(\"Training phase returned valid model, history, and state objects.\")\n",
        "             else:\n",
        "                  logging.error(f\"Training phase finished but final loss is invalid: {final_loss}. Considering training failed.\")\n",
        "                  training_successful = False # Mark as failed if loss is bad\n",
        "        else:\n",
        "             logging.error(\"Training phase finished but returned an invalid state (model, losshistory, or train_state is None).\")\n",
        "             training_successful = False\n",
        "\n",
        "    except Exception as e:\n",
        "         logging.error(f\"A critical error occurred during model building or the training phase: {e}\", exc_info=True)\n",
        "         # Ensure training_successful remains False\n",
        "         training_successful = False\n",
        "\n",
        "    # --- 4. Post-processing and Plotting Phase ---\n",
        "    if training_successful:\n",
        "        logging.info(\"Proceeding to post-processing.\")\n",
        "        try:\n",
        "            # Pass the final trained state to the plotter\n",
        "            plotter = Plotter(main_cfg, main_plot_cfg, model_trained, history_trained, state_trained)\n",
        "            plotter.run_post_processing()\n",
        "        except Exception as e:\n",
        "             logging.error(f\"An error occurred during post-processing: {e}\", exc_info=True)\n",
        "    else:\n",
        "        # This message will be logged if training failed, returned None, or hit a critical error\n",
        "        logging.error(\"Training did not complete successfully or produced an invalid state. Skipping post-processing.\")\n",
        "\n",
        "    main_end_time = time.time()\n",
        "    logging.info(\"=\"*60); logging.info(f\" Script Execution Finished in {main_end_time - main_start_time:.2f} seconds\"); logging.info(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# # Set environment variable *before* importing deepxde or torch\n",
        "# os.environ[\"DDE_BACKEND\"] = \"pytorch\"\n",
        "# try:\n",
        "#   import torch\n",
        "# except ImportError:\n",
        "#   print(\"Installing torch...\")\n",
        "#   !pip install torch -q\n",
        "# try:\n",
        "#   import deepxde\n",
        "# except ImportError:\n",
        "#   print(\"Installing deepxde...\")\n",
        "#   !pip install deepxde -q\n",
        "# try:\n",
        "#   import pandas\n",
        "# except ImportError:\n",
        "#   print(\"Installing pandas...\")\n",
        "#   !pip install pandas -q\n",
        "# try:\n",
        "#   import matplotlib\n",
        "# except ImportError:\n",
        "#   print(\"Installing matplotlib...\")\n",
        "#   !pip install matplotlib -q\n",
        "\n",
        "# import sys\n",
        "# import time\n",
        "# import logging\n",
        "# import numpy as np\n",
        "# import torch # Now import torch after potentially installing\n",
        "# import deepxde as dde # Import deepxde after setting backend\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# from scipy.interpolate import griddata\n",
        "# import re # <<<--- IMPORT REGEX MODULE\n",
        "\n",
        "\n",
        "# # --- Attempt to explicitly set backend (optional but good practice) ---\n",
        "# try:\n",
        "#     # This might still fail on older DeepXDE versions, but the env var is primary\n",
        "#     dde.config.set_default_backend(\"pytorch\")\n",
        "#     print(\"Attempted to explicitly set DeepXDE backend to PyTorch.\")\n",
        "# except AttributeError:\n",
        "#     print(f\"Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relied on environment variable DDE_BACKEND={os.environ.get('DDE_BACKEND')}.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Warning: Could not explicitly set backend via dde.config: {e}\")\n",
        "\n",
        "# print(f\"DeepXDE Backend requested: {os.environ.get('DDE_BACKEND', 'Not Set')}\")\n",
        "\n",
        "# # --- Check actual backend and setup device/dtype ---\n",
        "# if \"deepxde\" in sys.modules and hasattr(dde, 'backend'):\n",
        "#     print(f\"DeepXDE Backend actual: {dde.backend.backend_name}\")\n",
        "#     if dde.backend.backend_name == \"pytorch\":\n",
        "#         if torch.cuda.is_available():\n",
        "#             print(\"CUDA available.\")\n",
        "#             try:\n",
        "#                 # Use float32 as it's common for PINNs and avoids potential double precision issues\n",
        "#                 torch.set_default_dtype(torch.float32)\n",
        "#                 current_device = torch.cuda.current_device()\n",
        "#                 print(f\"PyTorch CUDA device detected by DDE: {current_device} ({torch.cuda.get_device_name(current_device)})\")\n",
        "#                 print(f\"PyTorch version: {torch.__version__}\")\n",
        "#                 print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Warning: Error during PyTorch device setup: {e}\")\n",
        "#         else:\n",
        "#             print(\"CUDA not available. Using CPU.\")\n",
        "#             try:\n",
        "#                 torch.set_default_dtype(torch.float32)\n",
        "#                 print(f\"PyTorch default device set to: cpu\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Warning: Failed to set default PyTorch device to CPU: {e}\")\n",
        "#         print(f\"PyTorch default dtype: {torch.get_default_dtype()}\")\n",
        "#     else:\n",
        "#         print(f\"Warning: Backend is '{dde.backend.backend_name}', not PyTorch. PyTorch-specific device setup skipped.\")\n",
        "# else:\n",
        "#     print(\"Warning: deepxde module or dde.backend not fully available for backend check.\")\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # ===== Configuration Classes =====\n",
        "# # =============================\n",
        "\n",
        "# class PlotterConfig:\n",
        "#     \"\"\"Stores configuration parameters specifically for plotting.\"\"\"\n",
        "#     NX_PRED = 200\n",
        "#     NY_PRED = 100\n",
        "#     CMAP_VELOCITY = 'viridis'\n",
        "#     CMAP_PRESSURE = 'coolwarm'\n",
        "#     CMAP_TURBULENCE = 'plasma'\n",
        "\n",
        "\n",
        "# class Config:\n",
        "#     \"\"\"Stores configuration parameters for the simulation.\"\"\"\n",
        "#     DRIVE_MOUNT_POINT = '/content/drive'\n",
        "#       # Adjust path if necessary\n",
        "#     # === PATH CORRECTION ===\n",
        "#     GDRIVE_BASE_FOLDER = 'PINN_RANS_ChannelFlow' # Path relative to MyDrive\n",
        "#     # =======================\n",
        "#     OUTPUT_DIR = os.path.join(DRIVE_MOUNT_POINT, 'MyDrive', GDRIVE_BASE_FOLDER) # Default if not overwritten\n",
        "#     MODEL_DIR = os.path.join(OUTPUT_DIR, \"model_checkpoints\")\n",
        "#     LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
        "#     PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
        "#     DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
        "#     LOG_FILE = os.path.join(LOG_DIR, \"plotting_log.log\") # Changed log file name\n",
        "#     REFERENCE_DATA_FILE = os.path.join(DATA_DIR, \"reference_output_data.csv\")\n",
        "\n",
        "#     # Checkpoint filename base (without step number or extension)\n",
        "#     CHECKPOINT_FILENAME_BASE  = \"rans_channel_wf\"\n",
        "\n",
        "#     # --- Fluid and Geometry Parameters ---\n",
        "#     NU = 0.0002 # Kinematic viscosity\n",
        "#     RHO = 1.0 # Density (often set to 1 for incompressible flow)\n",
        "#     MU = RHO * NU # Dynamic viscosity\n",
        "#     U_INLET = 1.0 # Inlet velocity - STILL NEEDED FOR BC DEFINITION\n",
        "#     H = 2.0 # Full channel height\n",
        "#     CHANNEL_HALF_HEIGHT = H / 2.0\n",
        "#     L = 10.0 # Channel length\n",
        "#     RE_H = U_INLET * H / NU # Reynolds number based on full height\n",
        "#     EPS_SMALL = 1e-10 # Small epsilon for numerical stability (avoid log(0), division by zero)\n",
        "\n",
        "#     # --- k-epsilon Model Constants ---\n",
        "#     CMU = 0.09\n",
        "#     CEPS1 = 1.44\n",
        "#     CEPS2 = 1.92\n",
        "#     SIGMA_K = 1.0\n",
        "#     SIGMA_EPS = 1.3\n",
        "#     KAPPA = 0.41 # Von Karman constant\n",
        "\n",
        "#     # --- Wall Function Parameters (Needed for BCs) ---\n",
        "#     E_WALL = 9.8 # Log-law constant for smooth walls\n",
        "#     Y_P = 0.04 # Distance from wall for applying wall functions (y_p)\n",
        "#     RE_TAU_TARGET = 350 # Target friction Reynolds number\n",
        "#     U_TAU_TARGET = RE_TAU_TARGET * NU / CHANNEL_HALF_HEIGHT # Target friction velocity\n",
        "#     YP_PLUS_TARGET = Y_P * U_TAU_TARGET / NU # Target y+ at y_p\n",
        "#     U_TARGET_WF = (U_TAU_TARGET / KAPPA) * np.log(max(E_WALL * YP_PLUS_TARGET, EPS_SMALL))\n",
        "#     K_TARGET_WF = U_TAU_TARGET**2 / np.sqrt(CMU)\n",
        "#     EPS_TARGET_WF = U_TAU_TARGET**3 / max(KAPPA * Y_P, EPS_SMALL)\n",
        "\n",
        "#     # --- Inlet Turbulence Parameters (Needed for BCs) ---\n",
        "#     TURBULENCE_INTENSITY = 0.05 # Typical value for channel flow inlet\n",
        "#     MIXING_LENGTH_SCALE = 0.07 * CHANNEL_HALF_HEIGHT # Estimate based on boundary layer thickness\n",
        "#     K_INLET = 1.5 * (U_INLET * TURBULENCE_INTENSITY)**2\n",
        "#     EPS_INLET = (CMU**0.75) * (K_INLET**1.5) / MIXING_LENGTH_SCALE\n",
        "#     K_INLET_TRANSFORMED = np.log(max(K_INLET, EPS_SMALL))\n",
        "#     EPS_INLET_TRANSFORMED = np.log(max(EPS_INLET, EPS_SMALL))\n",
        "#     K_TARGET_WF_TRANSFORMED = np.log(max(K_TARGET_WF, EPS_SMALL))\n",
        "#     EPS_TARGET_WF_TRANSFORMED = np.log(max(EPS_TARGET_WF, EPS_SMALL))\n",
        "\n",
        "#     # --- Domain Geometry ---\n",
        "#     GEOM = dde.geometry.Rectangle(xmin=[0, -CHANNEL_HALF_HEIGHT], xmax=[L, CHANNEL_HALF_HEIGHT])\n",
        "\n",
        "#     # --- Network Architecture (MUST MATCH SAVED MODEL) ---\n",
        "#     NUM_LAYERS = 8\n",
        "#     NUM_NEURONS = 64\n",
        "#     ACTIVATION = \"tanh\"\n",
        "#     INITIALIZER = \"Glorot normal\" # Initializer doesn't matter for loading weights\n",
        "#     NETWORK_INPUTS = 2 # x, y\n",
        "#     NETWORK_OUTPUTS = 5 # u, v, p', log(k), log(eps)\n",
        "\n",
        "#     # --- Training Parameters (Not used for training, but needed for Data object) ---\n",
        "#     NUM_DOMAIN_POINTS = 1 # Minimal points needed just to build Data object\n",
        "#     NUM_BOUNDARY_POINTS = 1 # Minimal points\n",
        "#     NUM_TEST_POINTS = 1 # Minimal points\n",
        "#     NUM_WF_POINTS_PER_WALL = 200 # Needed for get_boundary_conditions\n",
        "#     # Other training params (LR, iterations, weights) are irrelevant now\n",
        "#     LOSS_WEIGHTS = None # Not needed for prediction\n",
        "\n",
        "\n",
        "# # Instantiate config objects\n",
        "# cfg = Config()\n",
        "# plotter_cfg = PlotterConfig()\n",
        "\n",
        "\n",
        "# # ==============================================\n",
        "# # ===== Custom Checkpoint Callback Class ========\n",
        "# # ==============================================\n",
        "# # --- Callback is NOT needed for loading/plotting ---\n",
        "\n",
        "\n",
        "# # ==========================\n",
        "# # ===== Utility Functions =====\n",
        "# # ==========================\n",
        "# def setup_logging(log_file):\n",
        "#     \"\"\"Configures logging to file and console.\"\"\"\n",
        "#     log_dir = os.path.dirname(log_file)\n",
        "#     ensure_dir(log_dir)\n",
        "#     root_logger = logging.getLogger()\n",
        "#     # Clear existing handlers to avoid duplicate messages if run multiple times in notebook\n",
        "#     if root_logger.hasHandlers():\n",
        "#         root_logger.handlers.clear()\n",
        "#     log_formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
        "#     root_logger.setLevel(logging.INFO) # Set root logger level\n",
        "\n",
        "#     # File handler\n",
        "#     try:\n",
        "#         file_handler = logging.FileHandler(log_file, mode='a') # Append mode\n",
        "#         file_handler.setFormatter(log_formatter)\n",
        "#         root_logger.addHandler(file_handler)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error setting up file logger at {log_file}: {e}\")\n",
        "\n",
        "\n",
        "#     # Console handler\n",
        "#     console_handler = logging.StreamHandler(sys.stdout)\n",
        "#     console_handler.setFormatter(log_formatter)\n",
        "#     root_logger.addHandler(console_handler)\n",
        "#     logging.info(\"Logging configured.\")\n",
        "\n",
        "# def ensure_dir(directory):\n",
        "#     \"\"\"Creates a directory if it doesn't exist.\"\"\"\n",
        "#     if not os.path.exists(directory):\n",
        "#         try:\n",
        "#             os.makedirs(directory)\n",
        "#             logging.info(f\"Created directory: {directory}\")\n",
        "#         except OSError as e:\n",
        "#             logging.error(f\"Failed to create directory {directory}: {e}\")\n",
        "\n",
        "\n",
        "# def mount_drive(mount_point):\n",
        "#     \"\"\"Mounts Google Drive if running in Colab.\"\"\"\n",
        "#     # === Uses Corrected Config Path ===\n",
        "#     if 'google.colab' in sys.modules:\n",
        "#         if not os.path.exists(os.path.join(mount_point, 'MyDrive')):\n",
        "#             try:\n",
        "#                 from google.colab import drive\n",
        "#                 logging.info(f\"Mounting Google Drive at {mount_point}...\")\n",
        "#                 drive.mount(mount_point, force_remount=True)\n",
        "#                 logging.info(\"Google Drive mounted successfully.\")\n",
        "#                 # Construct path correctly using relative GDRIVE_BASE_FOLDER\n",
        "#                 gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER)\n",
        "#                 cfg.OUTPUT_DIR = gdrive_output_path\n",
        "#                 cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "#                 cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "#                 cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "#                 cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "#                 cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"plotting_log.log\") # Use new log file name\n",
        "#                 cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "#                 logging.info(f\"Output paths updated to Google Drive: {cfg.OUTPUT_DIR}\")\n",
        "#                 ensure_dir(cfg.OUTPUT_DIR)\n",
        "#                 if not os.path.exists(cfg.OUTPUT_DIR):\n",
        "#                     logging.warning(f\"Configured base folder NOT found after mount attempt: {cfg.OUTPUT_DIR}\")\n",
        "#             except Exception as e:\n",
        "#                 logging.error(f\"Error mounting Google Drive or accessing path: {e}\")\n",
        "#                 logging.warning(\"Falling back to local directory structure.\")\n",
        "#                 # Use relative path locally too if mount fails\n",
        "#                 cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER\n",
        "#                 cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "#                 cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "#                 cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "#                 cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "#                 cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"plotting_log.log\")\n",
        "#                 cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "#         else:\n",
        "#             logging.info(\"Google Drive already mounted.\")\n",
        "#             # Update paths correctly if already mounted\n",
        "#             gdrive_output_path = os.path.join(mount_point, 'MyDrive', cfg.GDRIVE_BASE_FOLDER)\n",
        "#             cfg.OUTPUT_DIR = gdrive_output_path\n",
        "#             cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "#             cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "#             cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "#             cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "#             cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"plotting_log.log\")\n",
        "#             cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "#             logging.info(f\"Output paths point to Google Drive: {cfg.OUTPUT_DIR}\")\n",
        "#     else:\n",
        "#         logging.info(\"Not running in Google Colab. Using local directory structure.\")\n",
        "#         # Use relative path locally\n",
        "#         cfg.OUTPUT_DIR = cfg.GDRIVE_BASE_FOLDER\n",
        "#         cfg.MODEL_DIR = os.path.join(cfg.OUTPUT_DIR, \"model_checkpoints\")\n",
        "#         cfg.LOG_DIR = os.path.join(cfg.OUTPUT_DIR, \"logs\")\n",
        "#         cfg.PLOT_DIR = os.path.join(cfg.OUTPUT_DIR, \"plots\")\n",
        "#         cfg.DATA_DIR = os.path.join(cfg.OUTPUT_DIR, \"data\")\n",
        "#         cfg.LOG_FILE = os.path.join(cfg.LOG_DIR, \"plotting_log.log\")\n",
        "#         cfg.REFERENCE_DATA_FILE = os.path.join(cfg.DATA_DIR, \"reference_output_data.csv\")\n",
        "\n",
        "# def setup_output_directories(config):\n",
        "#     \"\"\"Creates all necessary output directories.\"\"\"\n",
        "#     logging.info(\"Setting up output directories...\")\n",
        "#     ensure_dir(config.OUTPUT_DIR)\n",
        "#     ensure_dir(config.MODEL_DIR)\n",
        "#     ensure_dir(config.LOG_DIR)\n",
        "#     ensure_dir(config.PLOT_DIR)\n",
        "#     ensure_dir(config.DATA_DIR)\n",
        "#     logging.info(\"Output directories verified/created.\")\n",
        "\n",
        "# def log_configuration(config, plotter_config):\n",
        "#     \"\"\"Logs the simulation and plotter configuration (relevant parts).\"\"\"\n",
        "#     logging.info(\"=\" * 50)\n",
        "#     logging.info(\"Plotting Configuration:\")\n",
        "#     logging.info(f\"  Output Directory: {config.OUTPUT_DIR}\")\n",
        "#     logging.info(f\"  Model Directory: {config.MODEL_DIR}\")\n",
        "#     logging.info(f\"  Plot Directory: {config.PLOT_DIR}\")\n",
        "#     logging.info(f\"  Ref Data File: {config.REFERENCE_DATA_FILE}\")\n",
        "#     logging.info(f\"  Network Expected: {config.NUM_LAYERS} layers, {config.NUM_NEURONS} neurons\")\n",
        "#     logging.info(f\"  Prediction Grid Nx: {plotter_config.NX_PRED}, Ny: {plotter_config.NY_PRED}\")\n",
        "#     logging.info(\"=\" * 50)\n",
        "# # --- End Utility Functions ---\n",
        "\n",
        "\n",
        "# # ===============================\n",
        "# # ===== PDE System Definition =====\n",
        "# # ===============================\n",
        "# # --- PDE function is needed for Data object creation, keep as is ---\n",
        "# def pde(x, y, config):\n",
        "#     if dde.backend.backend_name != \"pytorch\":\n",
        "#         raise RuntimeError(\"PDE function relies on PyTorch autograd. Backend mismatch.\")\n",
        "#     nu = config.NU; Cmu = config.CMU; Ceps1 = config.CEPS1; Ceps2 = config.CEPS2\n",
        "#     sigma_k = config.SIGMA_K; sigma_eps = config.SIGMA_EPS; eps_small = config.EPS_SMALL\n",
        "#     u, v, p_prime, k_raw, eps_raw = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4], y[:, 4:5]\n",
        "#     k = torch.exp(k_raw) + eps_small\n",
        "#     eps = torch.exp(eps_raw) + eps_small\n",
        "#     try:\n",
        "#         u_x = dde.grad.jacobian(y, x, i=0, j=0); u_y = dde.grad.jacobian(y, x, i=0, j=1)\n",
        "#         v_x = dde.grad.jacobian(y, x, i=1, j=0); v_y = dde.grad.jacobian(y, x, i=1, j=1)\n",
        "#         p_prime_x = dde.grad.jacobian(y, x, i=2, j=0); p_prime_y = dde.grad.jacobian(y, x, i=2, j=1)\n",
        "#         u_xx = dde.grad.hessian(y, x, component=0, i=0, j=0); u_yy = dde.grad.hessian(y, x, component=0, i=1, j=1)\n",
        "#         v_xx = dde.grad.hessian(y, x, component=1, i=0, j=0); v_yy = dde.grad.hessian(y, x, component=1, i=1, j=1)\n",
        "#         u_xy = dde.grad.hessian(y, x, component=0, i=0, j=1)\n",
        "#         v_xy = dde.grad.hessian(y, x, component=1, i=0, j=1)\n",
        "#         if isinstance(x, torch.Tensor) and not x.requires_grad: x.requires_grad_(True)\n",
        "#         grad_k = torch.autograd.grad(k, x, grad_outputs=torch.ones_like(k), create_graph=True)[0]\n",
        "#         k_x, k_y = grad_k[:, 0:1], grad_k[:, 1:2]\n",
        "#         grad_eps = torch.autograd.grad(eps, x, grad_outputs=torch.ones_like(eps), create_graph=True)[0]\n",
        "#         eps_x, eps_y = grad_eps[:, 0:1], grad_eps[:, 1:2]\n",
        "#         grad_kx = torch.autograd.grad(k_x, x, grad_outputs=torch.ones_like(k_x), create_graph=True)[0]\n",
        "#         k_xx = grad_kx[:, 0:1]\n",
        "#         grad_ky = torch.autograd.grad(k_y, x, grad_outputs=torch.ones_like(k_y), create_graph=True)[0]\n",
        "#         k_yy = grad_ky[:, 1:2]\n",
        "#         grad_epsx = torch.autograd.grad(eps_x, x, grad_outputs=torch.ones_like(eps_x), create_graph=True)[0]\n",
        "#         eps_xx = grad_epsx[:, 0:1]\n",
        "#         grad_epsy = torch.autograd.grad(eps_y, x, grad_outputs=torch.ones_like(eps_y), create_graph=True)[0]\n",
        "#         eps_yy = grad_epsy[:, 1:2]\n",
        "#     except RuntimeError as grad_e:\n",
        "#         logging.error(f\"PyTorch Autograd RuntimeError calculating gradients in PDE: {grad_e}. Ensure create_graph=True is used correctly for higher-order derivatives.\", exc_info=True)\n",
        "#         zero_tensor = torch.zeros_like(y[:, 0:1])\n",
        "#         return [zero_tensor] * 5\n",
        "#     except Exception as grad_e:\n",
        "#         logging.error(f\"General error calculating gradients in PDE function: {grad_e}\", exc_info=True)\n",
        "#         zero_tensor = torch.zeros_like(y[:, 0:1])\n",
        "#         return [zero_tensor] * 5\n",
        "#     k_safe = k; eps_safe = eps\n",
        "#     nu_t = Cmu * torch.square(k_safe) / (eps_safe + eps_small)\n",
        "#     nu_eff = nu + nu_t\n",
        "#     dnut_dk = 2.0 * Cmu * k_safe / (eps_safe + eps_small)\n",
        "#     dnut_deps = -Cmu * torch.square(k_safe) / torch.square(eps_safe + eps_small)\n",
        "#     nu_eff_x = dnut_dk * k_x + dnut_deps * eps_x\n",
        "#     nu_eff_y = dnut_dk * k_y + dnut_deps * eps_y\n",
        "#     eq_continuity = u_x + v_y\n",
        "#     adv_u = u * u_x + v * u_y\n",
        "#     diff_u_term1 = nu_eff_x * (2 * u_x) + nu_eff * (2 * u_xx)\n",
        "#     diff_u_term2 = nu_eff_y * (u_y + v_x) + nu_eff * (u_yy + v_xy)\n",
        "#     eq_mom_x = adv_u + p_prime_x - (diff_u_term1 + diff_u_term2)\n",
        "#     adv_v = u * v_x + v * v_y\n",
        "#     diff_v_term1 = nu_eff_x * (v_x + u_y) + nu_eff * (v_xx + u_xy)\n",
        "#     diff_v_term2 = nu_eff_y * (2 * v_y) + nu_eff * (2 * v_yy)\n",
        "#     eq_mom_y = adv_v + p_prime_y - (diff_v_term1 + diff_v_term2)\n",
        "#     S_squared = 2 * (torch.square(u_x) + torch.square(v_y)) + torch.square(u_y + v_x)\n",
        "#     P_k = torch.relu(nu_t * S_squared)\n",
        "#     adv_k = u * k_x + v * k_y\n",
        "#     diffusivity_k = nu + nu_t / sigma_k\n",
        "#     d_diffk_dx = (1 / sigma_k) * nu_eff_x\n",
        "#     d_diffk_dy = (1 / sigma_k) * nu_eff_y\n",
        "#     laplacian_k = k_xx + k_yy\n",
        "#     diffusion_k = d_diffk_dx * k_x + d_diffk_dy * k_y + diffusivity_k * laplacian_k\n",
        "#     eq_k = adv_k - diffusion_k - P_k + eps_safe\n",
        "#     adv_eps = u * eps_x + v * eps_y\n",
        "#     diffusivity_eps = nu + nu_t / sigma_eps\n",
        "#     d_diffeps_dx = (1 / sigma_eps) * nu_eff_x\n",
        "#     d_diffeps_dy = (1 / sigma_eps) * nu_eff_y\n",
        "#     laplacian_eps = eps_xx + eps_yy\n",
        "#     diffusion_eps = d_diffeps_dx * eps_x + d_diffeps_dy * eps_y + diffusivity_eps * laplacian_eps\n",
        "#     source_eps = Ceps1 * (eps_safe / (k_safe + eps_small)) * P_k\n",
        "#     sink_eps = Ceps2 * (torch.square(eps_safe) / (k_safe + eps_small))\n",
        "#     eq_eps = adv_eps - diffusion_eps - source_eps + sink_eps\n",
        "#     return [eq_continuity, eq_mom_x, eq_mom_y, eq_k, eq_eps]\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # ===== Boundary Conditions =====\n",
        "# # =============================\n",
        "# # --- BC function is needed for Data object creation, keep as is ---\n",
        "# def get_boundary_conditions(config):\n",
        "#     geom = config.GEOM; h = config.CHANNEL_HALF_HEIGHT; L = config.L\n",
        "#     y_p = config.Y_P; n_wf_points = config.NUM_WF_POINTS_PER_WALL\n",
        "#     def boundary_inlet(x, on_boundary): return on_boundary and np.isclose(x[0], 0)\n",
        "#     def boundary_outlet(x, on_boundary): return on_boundary and np.isclose(x[0], L)\n",
        "#     def boundary_bottom_wall_physical(x, on_boundary): return on_boundary and np.isclose(x[1], -h)\n",
        "#     def boundary_top_wall_physical(x, on_boundary): return on_boundary and np.isclose(x[1], h)\n",
        "#     def boundary_walls_physical(x, on_boundary): return boundary_bottom_wall_physical(x, on_boundary) or boundary_top_wall_physical(x, on_boundary)\n",
        "#     bc_u_inlet = dde.DirichletBC(geom, lambda x: config.U_INLET, boundary_inlet, component=0)\n",
        "#     bc_v_inlet = dde.DirichletBC(geom, lambda x: 0, boundary_inlet, component=1)\n",
        "#     bc_k_inlet = dde.DirichletBC(geom, lambda x: config.K_INLET_TRANSFORMED, boundary_inlet, component=3)\n",
        "#     bc_eps_inlet = dde.DirichletBC(geom, lambda x: config.EPS_INLET_TRANSFORMED, boundary_inlet, component=4)\n",
        "#     bc_p_outlet = dde.DirichletBC(geom, lambda x: 0, boundary_outlet, component=2)\n",
        "#     bc_u_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=0)\n",
        "#     bc_v_walls = dde.DirichletBC(geom, lambda x: 0, boundary_walls_physical, component=1)\n",
        "#     x_wf_coords = np.linspace(0 + L * 0.01, L - L * 0.01, n_wf_points)[:, None]\n",
        "#     points_bottom_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, -h + y_p)))\n",
        "#     points_top_wf = np.hstack((x_wf_coords, np.full_like(x_wf_coords, h - y_p)))\n",
        "#     anchor_points_wf = np.vstack((points_bottom_wf, points_top_wf))\n",
        "#     logging.info(f\"Generated {anchor_points_wf.shape[0]} anchor points for wall functions (needed for BC setup).\")\n",
        "#     U_target_vals = np.full((anchor_points_wf.shape[0], 1), config.U_TARGET_WF)\n",
        "#     k_target_vals = np.full((anchor_points_wf.shape[0], 1), config.K_TARGET_WF_TRANSFORMED)\n",
        "#     eps_target_vals = np.full((anchor_points_wf.shape[0], 1), config.EPS_TARGET_WF_TRANSFORMED)\n",
        "#     bc_u_wf = dde.PointSetBC(anchor_points_wf, U_target_vals, component=0)\n",
        "#     bc_k_wf = dde.PointSetBC(anchor_points_wf, k_target_vals, component=3)\n",
        "#     bc_eps_wf = dde.PointSetBC(anchor_points_wf, eps_target_vals, component=4)\n",
        "#     all_bcs = [bc_u_inlet, bc_v_inlet, bc_k_inlet, bc_eps_inlet, bc_p_outlet, bc_u_walls, bc_v_walls, bc_u_wf, bc_k_wf, bc_eps_wf]\n",
        "#     return all_bcs, anchor_points_wf\n",
        "\n",
        "\n",
        "# # =======================\n",
        "# # ===== Trainer Class =====\n",
        "# # =======================\n",
        "# # --- Trainer class is only used for build_model ---\n",
        "# class Trainer:\n",
        "#     \"\"\"Handles the setup of the PINN model structure for loading/plotting.\"\"\"\n",
        "#     def __init__(self, config):\n",
        "#         self.config = config\n",
        "#         self.model = None\n",
        "#         self.pde = pde # Assign the PDE function\n",
        "\n",
        "#     def build_model(self, bcs, anchor_points):\n",
        "#         \"\"\"Builds the DeepXDE model structure (network and data).\"\"\"\n",
        "#         logging.info(\"Building the PINN model structure...\")\n",
        "#         if dde.backend.backend_name != \"pytorch\":\n",
        "#              raise RuntimeError(\"This code relies on the PyTorch backend.\")\n",
        "\n",
        "#         # Define the neural network (MUST MATCH SAVED ARCHITECTURE)\n",
        "#         net = dde.maps.FNN(\n",
        "#             layer_sizes=[self.config.NETWORK_INPUTS] + [self.config.NUM_NEURONS] * self.config.NUM_LAYERS + [self.config.NETWORK_OUTPUTS],\n",
        "#             activation=self.config.ACTIVATION,\n",
        "#             kernel_initializer=self.config.INITIALIZER # Initializer doesn't matter for loading\n",
        "#         )\n",
        "\n",
        "#         # Wrap PDE to include config\n",
        "#         pde_with_config = lambda x, y: self.pde(x, y, config=self.config)\n",
        "\n",
        "#         # Define the PDE data object (minimal points needed)\n",
        "#         try:\n",
        "#             data = dde.data.PDE(\n",
        "#                 geometry=self.config.GEOM,\n",
        "#                 pde=pde_with_config,\n",
        "#                 bcs=bcs, # BCs are needed for model structure\n",
        "#                 num_domain=self.config.NUM_DOMAIN_POINTS,\n",
        "#                 num_boundary=self.config.NUM_BOUNDARY_POINTS,\n",
        "#                 num_test=self.config.NUM_TEST_POINTS, # Use test points for potential evaluation later\n",
        "#                 anchors=anchor_points\n",
        "#             )\n",
        "#             logging.info(f\"Using {len(anchor_points) if anchor_points is not None else 0} anchor points in Data object.\")\n",
        "#         except TypeError:\n",
        "#             logging.warning(\"DeepXDE version might not support 'anchors'.\")\n",
        "#             data = dde.data.PDE(\n",
        "#                 geometry=self.config.GEOM,\n",
        "#                 pde=pde_with_config,\n",
        "#                 bcs=bcs,\n",
        "#                 num_domain=self.config.NUM_DOMAIN_POINTS,\n",
        "#                 num_boundary=self.config.NUM_BOUNDARY_POINTS,\n",
        "#                 num_test=self.config.NUM_TEST_POINTS\n",
        "#             )\n",
        "\n",
        "#         self.model = dde.Model(data, net)\n",
        "#         logging.info(\"Model structure built successfully.\")\n",
        "#         return self.model # Return the built model\n",
        "\n",
        "#     # --- train() and _post_training_checks() are removed ---\n",
        "\n",
        "# # --- End Trainer Class ---\n",
        "\n",
        "\n",
        "# # ========================\n",
        "# # ===== Plotter Class =====\n",
        "# # ========================\n",
        "# # --- Plotter class remains the same ---\n",
        "# class Plotter:\n",
        "#     \"\"\"Handles post-processing and plotting of simulation results.\"\"\"\n",
        "#     def __init__(self, config, plotter_config, model, losshistory, train_state):\n",
        "#         self.config = config\n",
        "#         self.plotter_config = plotter_config\n",
        "#         self.model = model\n",
        "#         self.losshistory = losshistory if losshistory else None\n",
        "#         self.train_state = train_state if train_state else None\n",
        "#         self.ref_data_path = config.REFERENCE_DATA_FILE\n",
        "#         self.plots_dir = config.PLOT_DIR\n",
        "#         self.ref_data = None\n",
        "#         self.has_ref_data = False\n",
        "#         self.ref_data_utau = None\n",
        "#         self.pinn_data_utau = None\n",
        "#         self.X_grid, self.Y_grid = None, None\n",
        "#         self.u_pred, self.v_pred, self.p_prime_pred = None, None, None\n",
        "#         self.k_pred, self.eps_pred, self.nu_t_pred = None, None, None\n",
        "#         self.p_pred = None\n",
        "#         os.makedirs(self.plots_dir, exist_ok=True)\n",
        "#         logging.info(f\"Plotter initialized. Plots will be saved in: {self.plots_dir}\")\n",
        "#         if self.ref_data_path:\n",
        "#              if os.path.exists(self.ref_data_path):\n",
        "#                  logging.info(f\"Reference CSV data path found: {self.ref_data_path}\")\n",
        "#              else:\n",
        "#                  logging.warning(f\"Reference CSV file not found: {self.ref_data_path}. Comparisons will be skipped.\")\n",
        "#         else:\n",
        "#              logging.info(\"No reference data path provided in config. Comparisons will be skipped.\")\n",
        "\n",
        "#     def plot_loss_history(self):\n",
        "#         if self.losshistory and self.train_state:\n",
        "#             logging.info(\"Saving loss history plot...\")\n",
        "#             try:\n",
        "#                 os.makedirs(self.plots_dir, exist_ok=True)\n",
        "#                 dde.saveplot(self.losshistory, self.train_state, issave=True, isplot=False, output_dir=self.plots_dir)\n",
        "#                 default_loss_file = os.path.join(self.plots_dir, \"loss.png\")\n",
        "#                 target_loss_file = os.path.join(self.plots_dir, \"training_loss_history.png\")\n",
        "#                 if os.path.exists(default_loss_file):\n",
        "#                     try:\n",
        "#                         os.replace(default_loss_file, target_loss_file)\n",
        "#                         logging.info(f\"Loss history plot saved as '{os.path.basename(target_loss_file)}'.\")\n",
        "#                     except OSError as rename_err:\n",
        "#                          logging.warning(f\"Could not replace/rename loss plot, using os.rename: {rename_err}\")\n",
        "#                          os.rename(default_loss_file, target_loss_file)\n",
        "#                          logging.info(f\"Loss history plot saved as '{os.path.basename(target_loss_file)}'.\")\n",
        "#                 else:\n",
        "#                      potential_files = [f for f in os.listdir(self.plots_dir) if f.lower().endswith('.png') and 'loss' in f.lower()]\n",
        "#                      if potential_files: logging.warning(f\"dde.saveplot might not have produced 'loss.png'. Found: {potential_files}.\")\n",
        "#                      else: logging.warning(\"dde.saveplot did not produce 'loss.png'.\")\n",
        "#             except ImportError: logging.error(\"Matplotlib might be needed by dde.saveplot but is not installed.\")\n",
        "#             except Exception as e: logging.error(f\"Could not save loss history plot: {e}\", exc_info=True)\n",
        "#         else:\n",
        "#             logging.info(\"Loss history or train state not available (likely loaded model). Skipping loss plot.\")\n",
        "\n",
        "#     def load_reference_data(self):\n",
        "#         self.has_ref_data = False\n",
        "#         if not self.ref_data_path: return\n",
        "#         if not os.path.exists(self.ref_data_path):\n",
        "#             logging.warning(f\"Reference CSV file not found: '{self.ref_data_path}'. Skipping load.\")\n",
        "#             return\n",
        "#         logging.info(f\"Loading reference data from: {self.ref_data_path}\")\n",
        "#         try:\n",
        "#             df_ref = pd.read_csv(self.ref_data_path)\n",
        "#             logging.info(f\"Loaded reference data: {df_ref.shape[0]} rows, {df_ref.shape[1]} cols.\")\n",
        "#             time_col = None\n",
        "#             if 'Time' in df_ref.columns: time_col = 'Time'\n",
        "#             elif 'TimeStep' in df_ref.columns: time_col = 'TimeStep'\n",
        "#             if time_col:\n",
        "#                 latest_time = df_ref[time_col].max()\n",
        "#                 df_ref = df_ref[df_ref[time_col] == latest_time].copy()\n",
        "#                 logging.info(f\"Filtered for latest time/step ({time_col}={latest_time}): {df_ref.shape[0]} rows remaining.\")\n",
        "#             x_col, y_col, z_col = None, None, None\n",
        "#             potential_x = ['x', 'Points:0', 'X', 'x-coordinate']\n",
        "#             potential_y = ['y', 'Points:1', 'Y', 'y-coordinate']\n",
        "#             potential_z = ['z', 'Points:2', 'Z', 'z-coordinate']\n",
        "#             for p_x in potential_x:\n",
        "#                 if p_x in df_ref.columns: x_col = p_x; break\n",
        "#             if not x_col:\n",
        "#                 for col in df_ref.columns:\n",
        "#                     if col.lower() in ['x', 'points:0', 'x-coordinate']: x_col = col; break\n",
        "#             for p_y in potential_y:\n",
        "#                 if p_y in df_ref.columns: y_col = p_y; break\n",
        "#             if not y_col:\n",
        "#                  for col in df_ref.columns:\n",
        "#                     if col.lower() in ['y', 'points:1', 'y-coordinate']: y_col = col; break\n",
        "#             for p_z in potential_z:\n",
        "#                  if p_z in df_ref.columns: z_col = p_z; break\n",
        "#             if not z_col:\n",
        "#                  for col in df_ref.columns:\n",
        "#                     if col.lower() in ['z', 'points:2', 'z-coordinate']: z_col = col; break\n",
        "#             if not x_col or not y_col: raise ValueError(f\"Could not identify x/y coordinates in reference columns: {df_ref.columns.tolist()}\")\n",
        "#             logging.info(f\"Identified coordinate columns: x='{x_col}', y='{y_col}'\" + (f\", z='{z_col}'\" if z_col else \"\"))\n",
        "#             if z_col and len(df_ref[z_col].unique()) > 1:\n",
        "#                 target_z = 0.0\n",
        "#                 unique_z = df_ref[z_col].unique()\n",
        "#                 nearest_z_idx = np.argmin(np.abs(unique_z - target_z))\n",
        "#                 nearest_z = unique_z[nearest_z_idx]\n",
        "#                 df_ref = df_ref[np.isclose(df_ref[z_col], nearest_z)].copy()\n",
        "#                 logging.info(f\"Filtered for z-plane near {target_z} (actual: {nearest_z:.4f}): {df_ref.shape[0]} rows remaining.\")\n",
        "#             var_map = {\n",
        "#                 'u:0':'u_ref', 'u_x':'u_ref', 'velocity:0':'u_ref', 'velocity_x':'u_ref', 'u':'u_ref', 'velocityu':'u_ref',\n",
        "#                 'u:1':'v_ref', 'u_y':'v_ref', 'velocity:1':'v_ref', 'velocity_y':'v_ref', 'v':'v_ref', 'velocityv':'v_ref',\n",
        "#                 'p':'p_ref', 'pressure':'p_ref', 'kinematicpressure':'p_ref', 'kinematic_pressure':'p_ref',\n",
        "#                 'k':'k_ref', 'turbulentkinetienergy':'k_ref', 'turbulentkineticenergy':'k_ref', 'tke':'k_ref',\n",
        "#                 'epsilon':'eps_ref', 'turbulencedissipationrate':'eps_ref', 'dissipationrate':'eps_ref', 'dissipation':'eps_ref',\n",
        "#                 'nut':'nut_ref', 'turbulentviscosity':'nut_ref', 'eddyviscosity':'nut_ref', 'nutilda':'nut_ref'\n",
        "#             }\n",
        "#             rename_dict = {}\n",
        "#             processed_cols = set()\n",
        "#             for col in df_ref.columns:\n",
        "#                 col_lower = col.lower().strip().replace('_','').replace('-','').replace(' ','')\n",
        "#                 if col_lower in var_map and col not in processed_cols:\n",
        "#                     rename_dict[col] = var_map[col_lower]\n",
        "#                     processed_cols.add(col)\n",
        "#             rename_dict[x_col] = 'x'; rename_dict[y_col] = 'y'\n",
        "#             if z_col: rename_dict[z_col] = 'z'\n",
        "#             processed_cols.update([x_col, y_col, z_col] if z_col else [x_col, y_col])\n",
        "#             unmapped_cols = [col for col in df_ref.columns if col not in processed_cols]\n",
        "#             if unmapped_cols: logging.debug(f\"Unmapped columns in reference data: {unmapped_cols}\")\n",
        "#             df_ref.rename(columns=rename_dict, inplace=True)\n",
        "#             logging.info(f\"Renamed reference columns based on mapping. New columns: {df_ref.columns.tolist()}\")\n",
        "#             required_cols_for_plots = list(set(['x', 'y', 'u_ref', 'p_ref', 'k_ref', 'eps_ref']))\n",
        "#             missing_cols = [col for col in required_cols_for_plots if col not in df_ref.columns]\n",
        "#             if missing_cols:\n",
        "#                 if any(c in missing_cols for c in ['x', 'y', 'u_ref']):\n",
        "#                     raise ValueError(f\"Missing essential columns after renaming in reference data: {missing_cols}.\")\n",
        "#                 else: logging.warning(f\"Missing some optional columns for plots: {missing_cols}.\")\n",
        "#             cols_to_keep = ['x', 'y'] + [col for col in ['u_ref', 'v_ref', 'p_ref', 'k_ref', 'eps_ref', 'nut_ref'] if col in df_ref.columns]\n",
        "#             if z_col and 'z' in df_ref.columns: cols_to_keep.append('z')\n",
        "#             df_ref = df_ref[list(set(cols_to_keep))]\n",
        "#             df_ref.sort_values(by=['x', 'y'], inplace=True)\n",
        "#             df_ref.reset_index(drop=True, inplace=True)\n",
        "#             self.ref_data = df_ref\n",
        "#             self.has_ref_data = True\n",
        "#             logging.info(f\"Successfully loaded and preprocessed reference CSV data. Final columns: {df_ref.columns.tolist()}\")\n",
        "#         except FileNotFoundError: pass\n",
        "#         except ValueError as ve: logging.error(f\"ValueError processing reference CSV: {ve}\")\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Unexpected error loading or processing reference CSV: {e}\", exc_info=True)\n",
        "#             self.ref_data = None; self.has_ref_data = False\n",
        "\n",
        "#     def predict_pinn_fields(self):\n",
        "#         if self.model is None or self.model.net is None:\n",
        "#              logging.error(\"PINN Model or network not available for prediction.\")\n",
        "#              return False\n",
        "#         logging.info(\"Predicting PINN flow fields on evaluation grid...\")\n",
        "#         nx = self.plotter_config.NX_PRED; ny = self.plotter_config.NY_PRED\n",
        "#         x_coords = np.linspace(0, self.config.L, nx); y_coords = np.linspace(-self.config.CHANNEL_HALF_HEIGHT, self.config.CHANNEL_HALF_HEIGHT, ny)\n",
        "#         self.X_grid, self.Y_grid = np.meshgrid(x_coords, y_coords)\n",
        "#         pred_points = np.vstack((np.ravel(self.X_grid), np.ravel(self.Y_grid))).T\n",
        "#         try:\n",
        "#             if isinstance(pred_points, torch.Tensor): pred_points_np = pred_points.cpu().numpy()\n",
        "#             else: pred_points_np = np.array(pred_points, dtype=np.float32)\n",
        "#             if not hasattr(self.model, 'net') or self.model.net is None:\n",
        "#                  logging.error(\"Model network attribute is missing or None. Cannot predict.\")\n",
        "#                  return False\n",
        "#             predictions_raw = self.model.predict(pred_points_np)\n",
        "#             if predictions_raw is None or not isinstance(predictions_raw, np.ndarray) or predictions_raw.shape[1] != self.config.NETWORK_OUTPUTS:\n",
        "#                 logging.error(f\"Prediction shape mismatch or invalid type. Expected {self.config.NETWORK_OUTPUTS} outputs, got shape {predictions_raw.shape if predictions_raw is not None else 'None'} and type {type(predictions_raw)}.\")\n",
        "#                 return False\n",
        "#         except AttributeError as ae:\n",
        "#              logging.error(f\"AttributeError during PINN prediction: {ae}\", exc_info=True)\n",
        "#              return False\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error during PINN prediction: {e}\", exc_info=True)\n",
        "#             return False\n",
        "#         try:\n",
        "#             self.u_pred = predictions_raw[:, 0].reshape(ny, nx)\n",
        "#             self.v_pred = predictions_raw[:, 1].reshape(ny, nx)\n",
        "#             self.p_prime_pred = predictions_raw[:, 2].reshape(ny, nx)\n",
        "#             k_raw_pred = predictions_raw[:, 3].reshape(ny, nx); eps_raw_pred = predictions_raw[:, 4].reshape(ny, nx)\n",
        "#             self.k_pred = np.exp(k_raw_pred) + self.config.EPS_SMALL\n",
        "#             self.eps_pred = np.exp(eps_raw_pred) + self.config.EPS_SMALL\n",
        "#             self.p_pred = self.p_prime_pred - (2.0 / 3.0) * self.k_pred\n",
        "#             eps_safe_pred = np.maximum(self.eps_pred, self.config.EPS_SMALL**2)\n",
        "#             self.nu_t_pred = self.config.CMU * np.square(self.k_pred) / eps_safe_pred\n",
        "#             logging.info(\"PINN field prediction and processing complete.\")\n",
        "#             return True\n",
        "#         except Exception as proc_e:\n",
        "#             logging.error(f\"Error processing raw predictions: {proc_e}\", exc_info=True)\n",
        "#             return False\n",
        "\n",
        "#     def plot_contour_fields(self):\n",
        "#         if self.u_pred is None:\n",
        "#             logging.warning(\"PINN data unavailable for plotting. Run predict_pinn_fields first. Skipping contours.\")\n",
        "#             return\n",
        "#         logging.info(\"Generating PINN contour plots...\")\n",
        "#         try:\n",
        "#             fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "#             axes = axes.ravel()\n",
        "#             cmap_vel = self.plotter_config.CMAP_VELOCITY; cmap_p = self.plotter_config.CMAP_PRESSURE; cmap_turb = self.plotter_config.CMAP_TURBULENCE\n",
        "#             plot_data_list = [\n",
        "#                 (self.u_pred, 'PINN Streamwise Velocity (u)', 'u (m/s)', cmap_vel),\n",
        "#                 (self.v_pred, 'PINN Transverse Velocity (v)', 'v (m/s)', cmap_vel),\n",
        "#                 (self.p_pred, \"PINN Kinematic Pressure (p)\", r'$p/\\rho$ ($m^2/s^2$)', cmap_p),\n",
        "#                 (self.k_pred, 'PINN TKE (k)', r'$k$ ($m^2/s^2$)', cmap_turb),\n",
        "#                 (self.eps_pred, 'PINN Dissipation ($\\epsilon$)', r'$\\epsilon$ ($m^2/s^3$)', cmap_turb),\n",
        "#                 (self.nu_t_pred / self.config.NU, 'PINN Eddy Viscosity Ratio', r'$\\nu_t / \\nu$', cmap_turb, True)\n",
        "#             ]\n",
        "#             for i, (data, title, label, cmap, *log_flag) in enumerate(plot_data_list):\n",
        "#                 ax = axes[i]; plot_values = data; cbar_label = label; levels = 50\n",
        "#                 use_log = log_flag[0] if log_flag else False\n",
        "#                 if use_log and np.nanmin(data) > self.config.EPS_SMALL:\n",
        "#                     try:\n",
        "#                         min_positive = np.nanmin(data[data > self.config.EPS_SMALL*10])\n",
        "#                         plot_values = np.log10(np.maximum(data, min_positive * 0.01))\n",
        "#                         cbar_label = f'log10({label})'\n",
        "#                         levels = np.logspace(np.log10(min_positive*0.01), np.log10(np.nanmax(data)), levels)\n",
        "#                         logging.debug(f\"Using log scale for {title}\")\n",
        "#                     except Exception as log_err:\n",
        "#                          logging.warning(f\"Could not apply log scale for {title}: {log_err}. Using linear scale.\")\n",
        "#                          use_log = False; plot_values = data\n",
        "#                 try:\n",
        "#                     if use_log: cf = ax.contourf(self.X_grid, self.Y_grid, plot_values, levels=levels, cmap=cmap, extend='both', locator=plt.LogLocator())\n",
        "#                     else: cf = ax.contourf(self.X_grid, self.Y_grid, plot_values, levels=levels, cmap=cmap, extend='both')\n",
        "#                     fig.colorbar(cf, ax=ax, label=cbar_label)\n",
        "#                     ax.set_title(title); ax.set_xlabel('x (m)'); ax.set_ylabel('y (m)')\n",
        "#                     ax.set_aspect('equal', adjustable='box')\n",
        "#                 except ValueError as ve: logging.error(f\"ValueError during contour plot for {title}: {ve}\")\n",
        "#                 except Exception as e: logging.error(f\"Error plotting contour for {title}: {e}\")\n",
        "#             for j in range(i + 1, len(axes)): fig.delaxes(axes[j])\n",
        "#             plt.tight_layout()\n",
        "#             save_path = os.path.join(self.plots_dir, \"pinn_field_contours.png\")\n",
        "#             plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "#             plt.close(fig)\n",
        "#             logging.info(f\"PINN contour field plots saved to {os.path.basename(save_path)}\")\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Failed to generate or save contour plots: {e}\", exc_info=True)\n",
        "#             if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
        "\n",
        "#     def _estimate_utau(self, data_source='pinn', x_slice_loc=None):\n",
        "#         if x_slice_loc is None: x_slice_loc = self.config.L * 0.8\n",
        "#         h = self.config.CHANNEL_HALF_HEIGHT; nu = self.config.NU; rho = self.config.RHO\n",
        "#         y_dist_1 = 0.001 * h; y_dist_2 = 0.01 * h\n",
        "#         y_eval_top_1 = h - y_dist_1; y_eval_top_2 = h - y_dist_2\n",
        "#         eval_points_top = np.array([[x_slice_loc, y_eval_top_1], [x_slice_loc, y_eval_top_2]])\n",
        "#         y_eval_bot_1 = -h + y_dist_1; y_eval_bot_2 = -h + y_dist_2\n",
        "#         eval_points_bot = np.array([[x_slice_loc, y_eval_bot_1], [x_slice_loc, y_eval_bot_2]])\n",
        "#         u1_top, k1_top, eps1_top, u2_top, k2_top, eps2_top = [None]*6\n",
        "#         u1_bot, k1_bot, eps1_bot, u2_bot, k2_bot, eps2_bot = [None]*6\n",
        "#         try:\n",
        "#             interp_method = 'linear'\n",
        "#             if data_source == 'pinn':\n",
        "#                 if self.model is None: return None\n",
        "#                 eval_points_all = np.vstack((eval_points_top, eval_points_bot))\n",
        "#                 pred_raw = self.model.predict(eval_points_all)\n",
        "#                 if pred_raw is None or pred_raw.shape[0] < 4:\n",
        "#                      logging.error(f\"PINN prediction failed or returned insufficient points for u_tau estimate at x={x_slice_loc:.2f}\")\n",
        "#                      return None\n",
        "#                 u_all = pred_raw[:, 0]; k_raw_all = pred_raw[:, 3]; eps_raw_all = pred_raw[:, 4]\n",
        "#                 k_all = np.exp(k_raw_all) + self.config.EPS_SMALL; eps_all = np.exp(eps_raw_all) + self.config.EPS_SMALL\n",
        "#                 u1_top, u2_top, u1_bot, u2_bot = u_all\n",
        "#                 k1_top, k2_top, k1_bot, k2_bot = k_all\n",
        "#                 eps1_top, eps2_top, eps1_bot, eps2_bot = eps_all\n",
        "#             elif data_source == 'reference' and self.has_ref_data:\n",
        "#                 if self.ref_data is None: return None\n",
        "#                 points_ref = self.ref_data[['x', 'y']].values\n",
        "#                 req_cols = ['u_ref', 'k_ref', 'eps_ref']\n",
        "#                 if not all(col in self.ref_data.columns for col in req_cols):\n",
        "#                     logging.warning(f\"Reference data missing required columns {req_cols} for u_tau estimation from gradient.\")\n",
        "#                     return None\n",
        "#                 values_to_interp = {col: self.ref_data[col].values for col in req_cols}\n",
        "#                 interp_results = {}\n",
        "#                 eval_points_all = np.vstack((eval_points_top, eval_points_bot))\n",
        "#                 for col, values in values_to_interp.items():\n",
        "#                     interp_vals = griddata(points_ref, values, eval_points_all, method=interp_method)\n",
        "#                     nan_mask = np.isnan(interp_vals)\n",
        "#                     if np.any(nan_mask):\n",
        "#                         logging.debug(f\"Linear interpolation failed for '{col}' ({data_source}) at x={x_slice_loc:.2f}. Trying 'nearest'.\")\n",
        "#                         interp_nearest = griddata(points_ref, values, eval_points_all[nan_mask], method='nearest')\n",
        "#                         interp_vals[nan_mask] = interp_nearest\n",
        "#                         if np.any(np.isnan(interp_vals)):\n",
        "#                              logging.error(f\"Interpolation (linear & nearest) failed for '{col}' ({data_source}) at x={x_slice_loc:.2f}. Cannot estimate u_tau.\")\n",
        "#                              return None\n",
        "#                     interp_results[col] = interp_vals\n",
        "#                 u1_top, u2_top, u1_bot, u2_bot = interp_results['u_ref']\n",
        "#                 k1_top, k2_top, k1_bot, k2_bot = interp_results['k_ref']\n",
        "#                 eps1_top, eps2_top, eps1_bot, eps2_bot = interp_results['eps_ref']\n",
        "#             else:\n",
        "#                 logging.warning(f\"Invalid data_source '{data_source}' or missing data for u_tau estimation.\")\n",
        "#                 return None\n",
        "#             du_dy_top = (u2_top - u1_top) / (y_eval_top_2 - y_eval_top_1)\n",
        "#             k_avg_top = (k1_top + k2_top) / 2.0; eps_avg_top = (eps1_top + eps2_top) / 2.0\n",
        "#             nu_t_avg_top = self.config.CMU * k_avg_top**2 / max(eps_avg_top, self.config.EPS_SMALL**2)\n",
        "#             nu_eff_avg_top = nu + nu_t_avg_top\n",
        "#             tau_w_top = rho * nu_eff_avg_top * abs(du_dy_top)\n",
        "#             u_tau_top = np.sqrt(max(tau_w_top / rho, self.config.EPS_SMALL))\n",
        "#             du_dy_bot = (u2_bot - u1_bot) / (y_eval_bot_2 - y_eval_bot_1)\n",
        "#             k_avg_bot = (k1_bot + k2_bot) / 2.0; eps_avg_bot = (eps1_bot + eps2_bot) / 2.0\n",
        "#             nu_t_avg_bot = self.config.CMU * k_avg_bot**2 / max(eps_avg_bot, self.config.EPS_SMALL**2)\n",
        "#             nu_eff_avg_bot = nu + nu_t_avg_bot\n",
        "#             tau_w_bot = rho * nu_eff_avg_bot * abs(du_dy_bot)\n",
        "#             u_tau_bot = np.sqrt(max(tau_w_bot / rho, self.config.EPS_SMALL))\n",
        "#             u_tau_estimated = (u_tau_top + u_tau_bot) / 2.0\n",
        "#             logging.info(f\"Estimated u_tau ({data_source}) at x={x_slice_loc:.2f} m: {u_tau_estimated:.4f} m/s (avg of top/bottom grad estimates)\")\n",
        "#             return u_tau_estimated\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error estimating u_tau for {data_source} at x={x_slice_loc:.2f}: {e}\", exc_info=True)\n",
        "#             return None\n",
        "\n",
        "#     def plot_profile_comparison(self):\n",
        "#         if self.u_pred is None: return\n",
        "#         if not self.has_ref_data: return\n",
        "#         logging.info(\"Generating profile comparison plots...\")\n",
        "#         x_slice_loc = self.config.L * 0.8; ny_pinn = self.plotter_config.NY_PRED\n",
        "#         y_coords_pinn = self.Y_grid[:, 0]; x_coords_pinn = self.X_grid[0, :]\n",
        "#         try: x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc)); actual_x_pinn = x_coords_pinn[x_slice_idx_pinn]\n",
        "#         except IndexError: logging.error(\"PINN grid coords invalid.\"); return\n",
        "#         pinn_slice = {'y': y_coords_pinn, 'u': self.u_pred[:, x_slice_idx_pinn], 'v': self.v_pred[:, x_slice_idx_pinn],\n",
        "#                       'p': self.p_pred[:, x_slice_idx_pinn], 'k': self.k_pred[:, x_slice_idx_pinn],\n",
        "#                       'eps': self.eps_pred[:, x_slice_idx_pinn], 'nut': self.nu_t_pred[:, x_slice_idx_pinn]}\n",
        "#         ref_slice = {'y': y_coords_pinn}; interpolation_successful = False\n",
        "#         try:\n",
        "#             if self.ref_data is None: raise ValueError(\"Ref data is None.\")\n",
        "#             points_ref = self.ref_data[['x', 'y']].values\n",
        "#             target_points = np.vstack((np.full(ny_pinn, actual_x_pinn), y_coords_pinn)).T\n",
        "#             logging.info(f\"Interpolating ref data at x={actual_x_pinn:.3f}...\")\n",
        "#             variables_to_interpolate = [('u_ref', 'u'), ('v_ref', 'v'), ('p_ref', 'p'), ('k_ref', 'k'), ('eps_ref', 'eps'), ('nut_ref', 'nut')]\n",
        "#             missing_ref_vars = []\n",
        "#             for var_ref, var_pinn in variables_to_interpolate:\n",
        "#                 if var_ref in self.ref_data.columns:\n",
        "#                     values_ref = self.ref_data[var_ref].values\n",
        "#                     interp_values = griddata(points_ref, values_ref, target_points, method='linear')\n",
        "#                     nan_mask = np.isnan(interp_values)\n",
        "#                     if np.any(nan_mask):\n",
        "#                         interp_nearest = griddata(points_ref, values_ref, target_points[nan_mask], method='nearest')\n",
        "#                         interp_values[nan_mask] = interp_nearest\n",
        "#                         if np.any(np.isnan(interp_values)): logging.warning(f\"Interp failed for '{var_ref}'.\")\n",
        "#                     ref_slice[var_pinn] = interp_values\n",
        "#                 else:\n",
        "#                     ref_slice[var_pinn] = np.full(ny_pinn, np.nan); missing_ref_vars.append(var_ref)\n",
        "#             interpolation_successful = True\n",
        "#             if missing_ref_vars: logging.warning(f\"Could not interp ref vars: {missing_ref_vars}\")\n",
        "#         except ValueError as ve: logging.error(f\"ValueError during ref interp: {ve}\")\n",
        "#         except Exception as e: logging.error(f\"Error interpolating ref data: {e}\", exc_info=True); interpolation_successful = False\n",
        "#         try:\n",
        "#             fig, axes = plt.subplots(3, 2, figsize=(12, 15)); axes = axes.ravel(); plot_idx = 0; h = self.config.CHANNEL_HALF_HEIGHT\n",
        "#             plot_vars = [('u', 'Velocity u', 'm/s'), ('v', 'Velocity v', 'm/s'), ('p', 'Kinematic Pressure p', r'$m^2/s^2$'),\n",
        "#                          ('k', 'TKE k', r'$m^2/s^2$'), ('eps', 'Dissipation eps', r'$m^2/s^3$'), ('nut', 'Eddy Viscosity nu_t', r'$m^2/s$')]\n",
        "#             for key, name, unit in plot_vars:\n",
        "#                 if plot_idx >= len(axes): break\n",
        "#                 ax = axes[plot_idx]\n",
        "#                 ax.plot(pinn_slice[key], pinn_slice['y'] / h, 'r-', linewidth=2, label='PINN')\n",
        "#                 if interpolation_successful and key in ref_slice and not np.all(np.isnan(ref_slice[key])):\n",
        "#                     ax.plot(ref_slice[key], ref_slice['y'] / h, 'b--', linewidth=1.5, label='Reference (CSV)')\n",
        "#                 elif not interpolation_successful and key != 'y': ax.plot([], [], 'b--', label='Reference (Failed)')\n",
        "#                 ax.set_xlabel(f'{name} ({unit})'); ax.set_ylabel('y/h'); ax.set_title(f'{name} Profile')\n",
        "#                 ax.legend(fontsize=8); ax.grid(True, linestyle=':')\n",
        "#                 if key in ['k', 'eps', 'nut']:\n",
        "#                      try:\n",
        "#                          min_val_for_log = self.config.EPS_SMALL; pinn_valid = np.nanmin(pinn_slice[key]) > min_val_for_log; ref_valid = False\n",
        "#                          if interpolation_successful and key in ref_slice and not np.all(np.isnan(ref_slice[key])): ref_valid = np.nanmin(ref_slice[key]) > min_val_for_log\n",
        "#                          if pinn_valid and (ref_valid or not interpolation_successful):\n",
        "#                               ax.set_xscale('log'); ax.grid(True, which='both', linestyle=':')\n",
        "#                      except ValueError: logging.warning(f\"Could not apply log scale for {key}.\")\n",
        "#                      except Exception as log_e: logging.warning(f\"Error log scale for {key}: {log_e}\")\n",
        "#                 plot_idx += 1\n",
        "#             for j in range(plot_idx, len(axes)): fig.delaxes(axes[j])\n",
        "#             plt.suptitle(f'Profile Comparison at x ≈ {actual_x_pinn:.3f} m', fontsize=16)\n",
        "#             plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "#             save_path = os.path.join(self.plots_dir, \"profile_comparison_pinn_vs_csv.png\")\n",
        "#             plt.savefig(save_path, dpi=200, bbox_inches='tight'); plt.close(fig)\n",
        "#             logging.info(f\"Profile comparison plot saved to {os.path.basename(save_path)}\")\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Failed to generate profile comparison plot: {e}\", exc_info=True)\n",
        "#             if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
        "\n",
        "#     def plot_wall_unit_comparison(self):\n",
        "#         if self.u_pred is None: return\n",
        "#         logging.info(\"Generating wall unit comparison plots...\")\n",
        "#         x_slice_loc_utau = self.config.L * 0.8\n",
        "#         self.pinn_data_utau = self._estimate_utau(data_source='pinn', x_slice_loc=x_slice_loc_utau)\n",
        "#         if self.has_ref_data: self.ref_data_utau = self._estimate_utau(data_source='reference', x_slice_loc=x_slice_loc_utau)\n",
        "#         else: self.ref_data_utau = None\n",
        "#         if not self.pinn_data_utau: logging.error(\"Could not estimate PINN u_tau.\"); return\n",
        "#         if self.has_ref_data and not self.ref_data_utau: logging.warning(\"Could not estimate ref u_tau.\")\n",
        "#         nu = self.config.NU; h = self.config.CHANNEL_HALF_HEIGHT; kappa = self.config.KAPPA; B_const = 5.2\n",
        "#         x_slice_loc_plot = x_slice_loc_utau\n",
        "#         y_coords_pinn = self.Y_grid[:, 0]; x_coords_pinn = self.X_grid[0, :]\n",
        "#         try: x_slice_idx_pinn = np.argmin(np.abs(x_coords_pinn - x_slice_loc_plot)); actual_x_pinn = x_coords_pinn[x_slice_idx_pinn]\n",
        "#         except IndexError: logging.error(\"PINN grid coords invalid.\"); return\n",
        "#         wall_indices_pinn = y_coords_pinn >= -self.config.EPS_SMALL; y_wall_pinn = y_coords_pinn[wall_indices_pinn]\n",
        "#         y_dist_wall_pinn = np.maximum(h - y_wall_pinn, self.config.EPS_SMALL * h)\n",
        "#         u_wall_pinn = self.u_pred[wall_indices_pinn, x_slice_idx_pinn]; k_wall_pinn = self.k_pred[wall_indices_pinn, x_slice_idx_pinn]; eps_wall_pinn = self.eps_pred[wall_indices_pinn, x_slice_idx_pinn]\n",
        "#         utau_pinn_safe = max(self.pinn_data_utau, self.config.EPS_SMALL)\n",
        "#         y_plus_pinn = y_dist_wall_pinn * utau_pinn_safe / nu; u_plus_pinn = u_wall_pinn / utau_pinn_safe\n",
        "#         k_plus_pinn = k_wall_pinn / max(utau_pinn_safe**2, self.config.EPS_SMALL**2); eps_plus_pinn = eps_wall_pinn * nu / max(utau_pinn_safe**4, self.config.EPS_SMALL**4)\n",
        "#         sort_idx_pinn = np.argsort(y_plus_pinn)\n",
        "#         y_plus_pinn = y_plus_pinn[sort_idx_pinn]; u_plus_pinn = u_plus_pinn[sort_idx_pinn]; k_plus_pinn = k_plus_pinn[sort_idx_pinn]; eps_plus_pinn = eps_plus_pinn[sort_idx_pinn]\n",
        "#         y_plus_ref, u_plus_ref, k_plus_ref, eps_plus_ref = None, None, None, None; ref_processed = False\n",
        "#         if self.has_ref_data and self.ref_data_utau:\n",
        "#             try:\n",
        "#                 ref_wall_data = self.ref_data[(np.isclose(self.ref_data['x'], actual_x_pinn, rtol=0.05, atol=0.1*self.config.L)) & (self.ref_data['y'] >= -self.config.EPS_SMALL)].copy()\n",
        "#                 if not ref_wall_data.empty:\n",
        "#                     y_wall_ref = ref_wall_data['y'].values; y_dist_wall_ref = np.maximum(h - y_wall_ref, self.config.EPS_SMALL * h)\n",
        "#                     utau_ref_safe = max(self.ref_data_utau, self.config.EPS_SMALL); y_plus_ref = y_dist_wall_ref * utau_ref_safe / nu\n",
        "#                     if 'u_ref' in ref_wall_data.columns: u_plus_ref = ref_wall_data['u_ref'].values / utau_ref_safe\n",
        "#                     if 'k_ref' in ref_wall_data.columns: k_plus_ref = ref_wall_data['k_ref'].values / max(utau_ref_safe**2, self.config.EPS_SMALL**2)\n",
        "#                     if 'eps_ref' in ref_wall_data.columns: eps_plus_ref = ref_wall_data['eps_ref'].values * nu / max(utau_ref_safe**4, self.config.EPS_SMALL**4)\n",
        "#                     sort_idx_ref = np.argsort(y_plus_ref); y_plus_ref = y_plus_ref[sort_idx_ref]\n",
        "#                     if u_plus_ref is not None: u_plus_ref = u_plus_ref[sort_idx_ref]\n",
        "#                     if k_plus_ref is not None: k_plus_ref = k_plus_ref[sort_idx_ref]\n",
        "#                     if eps_plus_ref is not None: eps_plus_ref = eps_plus_ref[sort_idx_ref]\n",
        "#                     logging.info(f\"Processed {len(y_plus_ref)} reference points for wall units.\")\n",
        "#                     ref_processed = True\n",
        "#                 else: logging.warning(f\"No ref data found near x={actual_x_pinn:.3f}, y>=0 for wall units.\")\n",
        "#             except KeyError as ke: logging.error(f\"Missing column in ref data for wall units: {ke}\")\n",
        "#             except Exception as e: logging.error(f\"Error processing ref data for wall units: {e}\", exc_info=True)\n",
        "#         try:\n",
        "#             fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
        "#             y_plus_max_pinn = np.max(y_plus_pinn) if len(y_plus_pinn) > 0 else 100; y_plus_max_ref = np.max(y_plus_ref) if ref_processed and y_plus_ref is not None and len(y_plus_ref) > 0 else y_plus_max_pinn\n",
        "#             y_plus_max_plot = 1.1 * max(y_plus_max_pinn, y_plus_max_ref, self.config.YP_PLUS_TARGET * 1.5)\n",
        "#             u_plus_max_pinn = np.max(u_plus_pinn) if len(u_plus_pinn) > 0 else 25; u_plus_max_ref = np.max(u_plus_ref) if ref_processed and u_plus_ref is not None and len(u_plus_ref) > 0 else u_plus_max_pinn\n",
        "#             u_plus_max_plot = 1.1 * max(u_plus_max_pinn, u_plus_max_ref)\n",
        "#             k_plus_max_pinn = np.max(k_plus_pinn) if len(k_plus_pinn) > 0 else 5; k_plus_max_ref = np.max(k_plus_ref) if ref_processed and k_plus_ref is not None and len(k_plus_ref) > 0 else k_plus_max_pinn\n",
        "#             k_plus_max_plot = 1.1 * max(k_plus_max_pinn, k_plus_max_ref)\n",
        "#             ax = axes[0]; ax.semilogx(y_plus_pinn, u_plus_pinn, 'r.', ms=4, label=f'PINN ($u_\\\\tau \\\\approx {self.pinn_data_utau:.3f}$)')\n",
        "#             if ref_processed and y_plus_ref is not None and u_plus_ref is not None: ax.semilogx(y_plus_ref, u_plus_ref, 'bo', mfc='none', ms=5, label=f'Ref ($u_\\\\tau \\\\approx {self.ref_data_utau:.3f}$)' if self.ref_data_utau else 'Ref (u_tau N/A)')\n",
        "#             y_plus_log_min = 11; y_plus_theory_log = np.logspace(np.log10(max(y_plus_log_min, 1)), np.log10(y_plus_max_plot*1.1), 100)\n",
        "#             u_plus_loglaw = (1 / kappa) * np.log(y_plus_theory_log) + B_const; y_plus_theory_vis = np.linspace(0.1, 30, 50); u_plus_viscous = y_plus_theory_vis\n",
        "#             ax.semilogx(y_plus_theory_log, u_plus_loglaw, 'k:', lw=1.5, label=f'Log Law ($\\\\kappa={kappa}, B={B_const}$)'); ax.semilogx(y_plus_theory_vis, u_plus_viscous, 'k--', lw=1.5, label='Viscous ($U^+=y^+$)')\n",
        "#             ax.set_xlabel('$y^+$'); ax.set_ylabel('$U^+$'); ax.set_title(f'$U^+$ vs $y^+$ Profile'); ax.legend(fontsize=9); ax.grid(True, which='both', ls=':'); ax.set_ylim(bottom=0, top=u_plus_max_plot); ax.set_xlim(left=0.1, right=y_plus_max_plot)\n",
        "#             ax = axes[1]; ax.semilogx(y_plus_pinn, k_plus_pinn, 'r.', ms=4, label='PINN')\n",
        "#             if ref_processed and y_plus_ref is not None and k_plus_ref is not None: ax.semilogx(y_plus_ref, k_plus_ref, 'bo', mfc='none', ms=5, label='Reference')\n",
        "#             if hasattr(self.config, 'YP_PLUS_TARGET'): ax.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'WF $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
        "#             ax.set_xlabel('$y^+$'); ax.set_ylabel('$k^+$'); ax.set_title('$k^+$ vs $y^+$ Profile'); ax.legend(fontsize=9); ax.grid(True, which='both', ls=':'); ax.set_ylim(bottom=0, top=k_plus_max_plot); ax.set_xlim(left=0.1, right=y_plus_max_plot)\n",
        "#             ax = axes[2]; valid_idx_pinn = eps_plus_pinn > self.config.EPS_SMALL**2\n",
        "#             if np.any(valid_idx_pinn): ax.loglog(y_plus_pinn[valid_idx_pinn], eps_plus_pinn[valid_idx_pinn], 'r.', ms=4, label='PINN')\n",
        "#             else: ax.plot([],[], 'r.', label='PINN (No positive data)')\n",
        "#             if ref_processed and y_plus_ref is not None and eps_plus_ref is not None:\n",
        "#                  valid_idx_ref = eps_plus_ref > self.config.EPS_SMALL**2\n",
        "#                  if np.any(valid_idx_ref): ax.loglog(y_plus_ref[valid_idx_ref], eps_plus_ref[valid_idx_ref], 'bo', mfc='none', ms=5, label='Reference')\n",
        "#                  else: ax.plot([],[], 'bo', mfc='none', label='Reference (No positive data)')\n",
        "#             y_plus_theory_eps = np.logspace(np.log10(max(1, 0.1)), np.log10(y_plus_max_plot*1.1), 100); eps_plus_target_theory = 1.0 / (kappa * y_plus_theory_eps)\n",
        "#             ax.loglog(y_plus_theory_eps, eps_plus_target_theory, 'k:', lw=1.5, label='$\\\\epsilon^+ \\\\propto 1/y^+$')\n",
        "#             if hasattr(self.config, 'YP_PLUS_TARGET'): ax.axvline(self.config.YP_PLUS_TARGET, color='g', ls='-.', lw=1, label=f'WF $y_p^+ \\\\approx {self.config.YP_PLUS_TARGET:.1f}$')\n",
        "#             ax.set_xlabel('$y^+$'); ax.set_ylabel('$\\\\epsilon^+$'); ax.set_title('$\\\\epsilon^+$ vs $y^+$ Profile (log-log)'); ax.legend(fontsize=9); ax.grid(True, which='both', ls=':')\n",
        "#             min_eps_plus_data = np.min(eps_plus_pinn[valid_idx_pinn]) if np.any(valid_idx_pinn) else 1e-5; max_eps_plus_data = np.max(eps_plus_pinn[valid_idx_pinn]) if np.any(valid_idx_pinn) else 1\n",
        "#             if ref_processed and eps_plus_ref is not None and np.any(valid_idx_ref): min_eps_plus_data = min(min_eps_plus_data, np.min(eps_plus_ref[valid_idx_ref])); max_eps_plus_data = max(max_eps_plus_data, np.max(eps_plus_ref[valid_idx_ref]))\n",
        "#             ax.set_ylim(bottom=max(min_eps_plus_data * 0.1, 1e-6), top=max_eps_plus_data * 10); ax.set_xlim(left=0.1, right=y_plus_max_plot)\n",
        "#             plt.suptitle(f'Wall Unit Profiles (Top Wall, x ≈ {actual_x_pinn:.3f}m)', fontsize=16)\n",
        "#             plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "#             save_path = os.path.join(self.plots_dir, \"profile_comparison_wall_units.png\")\n",
        "#             plt.savefig(save_path, dpi=200, bbox_inches='tight'); plt.close(fig)\n",
        "#             logging.info(f\"Wall unit comparison plots saved to {os.path.basename(save_path)}\")\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Failed to generate wall unit comparison plot: {e}\", exc_info=True)\n",
        "#             if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
        "\n",
        "#     def plot_pressure_gradient_comparison(self):\n",
        "#         if self.p_pred is None: return\n",
        "#         logging.info(\"Generating centerline pressure gradient comparison plot...\")\n",
        "#         try:\n",
        "#             x_coords_pinn = self.X_grid[0, :]; y_coords_pinn = self.Y_grid[:, 0]\n",
        "#             center_idx_pinn = np.argmin(np.abs(y_coords_pinn - 0.0)); actual_y_center = y_coords_pinn[center_idx_pinn]\n",
        "#             p_centerline_pinn = self.p_pred[center_idx_pinn, :]; dp_dx_pinn = np.gradient(p_centerline_pinn, x_coords_pinn)\n",
        "#             dp_dx_ref = None; x_coords_ref = None; ref_grad_calculated = False\n",
        "#             if self.has_ref_data and 'p_ref' in self.ref_data.columns:\n",
        "#                 try:\n",
        "#                     centerline_tol = 0.05 * self.config.CHANNEL_HALF_HEIGHT; ref_centerline = self.ref_data[np.abs(self.ref_data['y']) <= centerline_tol].copy()\n",
        "#                     if not ref_centerline.empty:\n",
        "#                         centerline_grouped = ref_centerline.groupby('x')['p_ref'].mean().sort_index()\n",
        "#                         if len(centerline_grouped) > 5:\n",
        "#                             x_coords_ref = centerline_grouped.index.values; p_centerline_ref = centerline_grouped.values\n",
        "#                             if len(x_coords_ref) > 1: dp_dx_ref = np.gradient(p_centerline_ref, x_coords_ref); ref_grad_calculated = True\n",
        "#                             else: logging.warning(\"Not enough unique x for ref grad.\")\n",
        "#                         else: logging.warning(f\"Not enough grouped x ({len(centerline_grouped)}) for ref grad.\")\n",
        "#                     else: logging.warning(\"No points near centerline in ref data.\")\n",
        "#                 except KeyError as ke: logging.warning(f\"Missing ref col for pressure grad: {ke}\")\n",
        "#                 except Exception as e: logging.warning(f\"Could not calc ref pressure grad: {e}\")\n",
        "#             elif self.has_ref_data: logging.warning(\"Ref data loaded, but 'p_ref' missing.\")\n",
        "#             fig, ax = plt.subplots(figsize=(10, 6))\n",
        "#             ax.plot(x_coords_pinn, dp_dx_pinn, 'r-', lw=2, label='PINN $dp/dx$')\n",
        "#             if ref_grad_calculated and dp_dx_ref is not None and x_coords_ref is not None: ax.plot(x_coords_ref, dp_dx_ref, 'b--', lw=1.5, label='Reference $dp/dx$ (CSV)')\n",
        "#             elif self.has_ref_data: ax.plot([], [], 'b--', label='Reference $dp/dx$ (Failed)')\n",
        "#             ax.set_xlabel('x / L'); ax.set_ylabel(r'$dp/dx$ $(m/s^2)$'); ax.set_title(f'Streamwise Kinematic Pressure Gradient along Centerline (y ≈ {actual_y_center:.3f}m)')\n",
        "#             ax.legend(); ax.grid(True, ls=':'); ax.set_xlim(0, 1)\n",
        "#             try:\n",
        "#                focus_start_idx = len(dp_dx_pinn) // 2; focus_end_idx = int(len(dp_dx_pinn) * 0.95)\n",
        "#                if focus_start_idx < focus_end_idx :\n",
        "#                     focus_region_pinn = dp_dx_pinn[focus_start_idx:focus_end_idx]\n",
        "#                     if len(focus_region_pinn) > 0:\n",
        "#                          mean_dpdx = np.mean(focus_region_pinn); std_dpdx = np.std(focus_region_pinn)\n",
        "#                          pad = 5 * max(std_dpdx, abs(mean_dpdx)*0.1, 1e-4)\n",
        "#                          ax.set_ylim(mean_dpdx - pad, mean_dpdx + pad)\n",
        "#             except Exception as ylim_e: logging.warning(f\"Could not set y-limits for pressure grad plot: {ylim_e}\")\n",
        "#             ax.set_xticks(np.linspace(0, self.config.L, 6)); ax.set_xticklabels([f\"{x/self.config.L:.1f}\" for x in np.linspace(0, self.config.L, 6)]); ax.set_xlabel('x / L')\n",
        "#             plt.tight_layout(); save_path = os.path.join(self.plots_dir, \"pressure_gradient_comparison.png\")\n",
        "#             plt.savefig(save_path, dpi=200); plt.close(fig)\n",
        "#             logging.info(f\"Pressure gradient comparison plot saved to {os.path.basename(save_path)}\")\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Failed to generate pressure grad plot: {e}\", exc_info=True)\n",
        "#             if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
        "\n",
        "#     def run_post_processing(self):\n",
        "#         \"\"\"Runs the full post-processing sequence (plotting only).\"\"\"\n",
        "#         if self.model is None:\n",
        "#              logging.error(\"Model object is None. Cannot run post-processing.\")\n",
        "#              return\n",
        "#         logging.info(\"--- Starting Full Post-Processing (Plotting Only) ---\")\n",
        "#         self.plot_loss_history() # Will be skipped if history is None\n",
        "#         prediction_successful = self.predict_pinn_fields()\n",
        "#         if prediction_successful:\n",
        "#             self.plot_contour_fields()\n",
        "#             self.load_reference_data()\n",
        "#             if self.has_ref_data:\n",
        "#                 logging.info(\"Proceeding with PINN vs Reference CSV comparisons...\")\n",
        "#                 self.plot_profile_comparison()\n",
        "#                 self.plot_wall_unit_comparison()\n",
        "#                 self.plot_pressure_gradient_comparison()\n",
        "#             else:\n",
        "#                 logging.warning(\"Skipping comparison plots as reference data is unavailable or failed to load.\")\n",
        "#         else:\n",
        "#             logging.error(\"PINN field prediction failed. Aborting further post-processing that depends on predictions.\")\n",
        "#         logging.info(\"--- Post-Processing Finished ---\")\n",
        "# # --- End Plotter Class ---\n",
        "\n",
        "\n",
        "# # =============================\n",
        "# # ===== Main Execution Block =====\n",
        "# # =============================\n",
        "# if __name__ == \"__main__\":\n",
        "#     main_start_time = time.time()\n",
        "\n",
        "#     # --- 1. Initial Setup ---\n",
        "#     main_cfg = Config()\n",
        "#     main_plot_cfg = PlotterConfig()\n",
        "#     mount_drive(main_cfg.DRIVE_MOUNT_POINT) # Mount drive first\n",
        "#     setup_output_directories(main_cfg)    # Setup dirs based on potentially updated path\n",
        "#     setup_logging(main_cfg.LOG_FILE)       # Setup logging to the correct file\n",
        "\n",
        "#     logging.info(\"=\"*60); logging.info(\" PINN RANS k-epsilon PLOTTING Start \"); logging.info(\"=\"*60)\n",
        "#     log_configuration(main_cfg, main_plot_cfg) # Log config\n",
        "\n",
        "#     # --- 2. Define Boundaries (Still needed for model structure) ---\n",
        "#     try:\n",
        "#         bcs, anchor_points = get_boundary_conditions(main_cfg)\n",
        "#         logging.info(f\"Defined {len(bcs)} boundary conditions for model structure.\")\n",
        "#         if anchor_points is None: anchor_points = [] # Ensure it's iterable\n",
        "#     except Exception as e:\n",
        "#         logging.error(f\"Failed to define boundary conditions: {e}\", exc_info=True)\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     # --- 3. Build Model Structure and Load Checkpoint ---\n",
        "#     model_loaded = None\n",
        "#     load_successful = False\n",
        "#     try:\n",
        "#         # Instantiate Trainer just to use its build_model method\n",
        "#         builder = Trainer(main_cfg)\n",
        "#         model_loaded = builder.build_model(bcs, anchor_points)\n",
        "\n",
        "#         if model_loaded is None:\n",
        "#              raise RuntimeError(\"Model building failed.\")\n",
        "\n",
        "#         # --- Find the latest checkpoint ---\n",
        "#         filepath_base = os.path.join(main_cfg.MODEL_DIR, f\"{main_cfg.CHECKPOINT_FILENAME_BASE}-\")\n",
        "#         latest_checkpoint = None\n",
        "#         restored_step = 0\n",
        "#         if os.path.exists(main_cfg.MODEL_DIR):\n",
        "#             filename_pattern = re.compile(rf\"^{re.escape(os.path.basename(filepath_base))}(\\d+)\\.pt$\")\n",
        "#             checkpoint_files = []\n",
        "#             logging.info(f\"Searching for checkpoints in: {main_cfg.MODEL_DIR} with pattern {filename_pattern.pattern}\")\n",
        "#             for f in os.listdir(main_cfg.MODEL_DIR):\n",
        "#                 full_path = os.path.join(main_cfg.MODEL_DIR, f)\n",
        "#                 if os.path.isfile(full_path):\n",
        "#                     match = filename_pattern.match(f)\n",
        "#                     if match:\n",
        "#                         step_num = int(match.group(1))\n",
        "#                         checkpoint_files.append((step_num, full_path))\n",
        "#             if checkpoint_files:\n",
        "#                 checkpoint_files.sort(key=lambda item: item[0], reverse=True)\n",
        "#                 restored_step, latest_checkpoint = checkpoint_files[0]\n",
        "#                 logging.info(f\"Found latest checkpoint to load: {latest_checkpoint} at step {restored_step}\")\n",
        "#             else:\n",
        "#                 logging.error(\"No valid checkpoints found matching the pattern. Cannot proceed with plotting.\")\n",
        "#                 sys.exit(1) # Exit if no checkpoint found\n",
        "#         else:\n",
        "#              logging.error(f\"Model directory not found: {main_cfg.MODEL_DIR}. Cannot load model.\")\n",
        "#              sys.exit(1)\n",
        "\n",
        "#         restore_path = latest_checkpoint\n",
        "\n",
        "#         # --- Compile model (Still recommended for DeepXDE structure before prediction) ---\n",
        "#         # Use a dummy optimizer/lr as it won't be used for training\n",
        "#         logging.info(\"Compiling model structure (recommended before prediction)...\")\n",
        "#         # Ensure the loss_weights argument is not passed if it's None in Config\n",
        "#         compile_args = {\"optimizer\": \"adam\", \"lr\": 1e-4}\n",
        "#         if main_cfg.LOSS_WEIGHTS is not None:\n",
        "#              compile_args[\"loss_weights\"] = main_cfg.LOSS_WEIGHTS\n",
        "#         else:\n",
        "#              logging.debug(\"Compiling without loss_weights as it's None in Config.\")\n",
        "#         model_loaded.compile(**compile_args) # Optimizer/LR choice is arbitrary here\n",
        "#         logging.info(\"Model compiled.\")\n",
        "\n",
        "#         # ===> KEY CHANGE: Manual Weight Loading <===\n",
        "#         logging.info(f\"Manually loading network weights from: {restore_path}\")\n",
        "#         # Determine device to load onto (CPU or GPU if available)\n",
        "#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#         logging.info(f\"Loading checkpoint onto device: {device}\")\n",
        "#         # Load the entire checkpoint dictionary\n",
        "#         checkpoint = torch.load(restore_path, map_location=device)\n",
        "\n",
        "#         # Check if 'model_state_dict' key exists (standard PyTorch/DeepXDE practice)\n",
        "#         if 'model_state_dict' in checkpoint:\n",
        "#             # Access the underlying network (assuming it's stored in model.net)\n",
        "#             if hasattr(model_loaded, 'net') and isinstance(model_loaded.net, torch.nn.Module):\n",
        "#                 # Load only the model's parameters\n",
        "#                 model_loaded.net.load_state_dict(checkpoint['model_state_dict'])\n",
        "#                 logging.info(f\"Model weights restored manually from step {restored_step}.\")\n",
        "#                 load_successful = True\n",
        "#             else:\n",
        "#                 logging.error(\"Model object does not have a 'net' attribute or it's not a torch.nn.Module. Cannot load state dict.\")\n",
        "#                 load_successful = False\n",
        "#         # Fallback: Check for 'state_dict' (might be used in some cases)\n",
        "#         elif 'state_dict' in checkpoint:\n",
        "#              logging.warning(\"Checkpoint using 'state_dict' key instead of 'model_state_dict'. Attempting load.\")\n",
        "#              if hasattr(model_loaded, 'net') and isinstance(model_loaded.net, torch.nn.Module):\n",
        "#                 model_loaded.net.load_state_dict(checkpoint['state_dict'])\n",
        "#                 logging.info(f\"Model weights restored manually from step {restored_step} using 'state_dict'.\")\n",
        "#                 load_successful = True\n",
        "#              else:\n",
        "#                 logging.error(\"Model object does not have a 'net' attribute or it's not a torch.nn.Module. Cannot load state dict.\")\n",
        "#                 load_successful = False\n",
        "#         else:\n",
        "#             logging.error(\"Checkpoint file does not contain 'model_state_dict' or 'state_dict'. Cannot load weights.\")\n",
        "#             load_successful = False\n",
        "#         # ===> END OF KEY CHANGE <===\n",
        "\n",
        "#     except Exception as e:\n",
        "#          logging.error(f\"A critical error occurred during model build or load: {e}\", exc_info=True)\n",
        "#          load_successful = False\n",
        "\n",
        "#     # --- 4. Post-processing and Plotting Phase ---\n",
        "#     if load_successful and model_loaded is not None:\n",
        "#         logging.info(\"Proceeding to post-processing (plotting).\")\n",
        "#         try:\n",
        "#             # Instantiate Plotter with the loaded model\n",
        "#             # Pass None for history and state as they weren't generated\n",
        "#             plotter = Plotter(main_cfg, main_plot_cfg, model_loaded, None, None)\n",
        "#             plotter.run_post_processing()\n",
        "#         except Exception as e:\n",
        "#              logging.error(f\"An error occurred during post-processing/plotting: {e}\", exc_info=True)\n",
        "#     else:\n",
        "#         logging.error(\"Model loading failed or model is invalid. Skipping post-processing.\")\n",
        "\n",
        "#     main_end_time = time.time()\n",
        "#     logging.info(\"=\"*60); logging.info(f\" Script Execution (Plotting Only) Finished in {main_end_time - main_start_time:.2f} seconds\"); logging.info(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NR77MoSDHwE",
        "outputId": "9d3cee0e-b222-4a2c-f6e9-62d1a0a1fc9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not explicitly set backend via dde.config (likely older DeepXDE version). Relied on environment variable DDE_BACKEND=pytorch.\n",
            "DeepXDE Backend requested: pytorch\n",
            "DeepXDE Backend actual: pytorch\n",
            "CUDA available.\n",
            "PyTorch CUDA device detected by DDE: 0 (Tesla T4)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "Number of GPUs: 1\n",
            "PyTorch default dtype: torch.float32\n",
            "2025-04-14 20:50:54 [INFO] Google Drive already mounted.\n",
            "2025-04-14 20:50:54 [INFO] Output paths point to Google Drive: /content/drive/MyDrive/PINN_RANS_ChannelFlow\n",
            "2025-04-14 20:50:54 [INFO] Setting up output directories...\n",
            "2025-04-14 20:50:54 [INFO] Output directories verified/created.\n",
            "2025-04-14 20:50:54 [INFO] Logging configured.\n",
            "2025-04-14 20:50:54 [INFO] ============================================================\n",
            "2025-04-14 20:50:54 [INFO]  PINN RANS k-epsilon PLOTTING Start \n",
            "2025-04-14 20:50:54 [INFO] ============================================================\n",
            "2025-04-14 20:50:54 [INFO] ==================================================\n",
            "2025-04-14 20:50:54 [INFO] Plotting Configuration:\n",
            "2025-04-14 20:50:54 [INFO]   Output Directory: /content/drive/MyDrive/PINN_RANS_ChannelFlow\n",
            "2025-04-14 20:50:54 [INFO]   Model Directory: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints\n",
            "2025-04-14 20:50:54 [INFO]   Plot Directory: /content/drive/MyDrive/PINN_RANS_ChannelFlow/plots\n",
            "2025-04-14 20:50:54 [INFO]   Ref Data File: /content/drive/MyDrive/PINN_RANS_ChannelFlow/data/reference_output_data.csv\n",
            "2025-04-14 20:50:54 [INFO]   Network Expected: 8 layers, 64 neurons\n",
            "2025-04-14 20:50:54 [INFO]   Prediction Grid Nx: 200, Ny: 100\n",
            "2025-04-14 20:50:54 [INFO] ==================================================\n",
            "2025-04-14 20:50:54 [INFO] Generated 400 anchor points for wall functions (needed for BC setup).\n",
            "2025-04-14 20:50:54 [INFO] Defined 10 boundary conditions for model structure.\n",
            "2025-04-14 20:50:54 [INFO] Building the PINN model structure...\n",
            "Warning: 1 points required, but 3 points sampled.\n",
            "2025-04-14 20:50:54 [INFO] Using 400 anchor points in Data object.\n",
            "2025-04-14 20:50:54 [INFO] Model structure built successfully.\n",
            "2025-04-14 20:50:54 [INFO] Searching for checkpoints in: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints with pattern ^rans_channel_wf\\-(\\d+)\\.pt$\n",
            "2025-04-14 20:50:54 [INFO] Found latest checkpoint to load: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-69000.pt at step 69000\n",
            "2025-04-14 20:50:54 [INFO] Compiling model structure (recommended before prediction)...\n",
            "Compiling model...\n",
            "'compile' took 0.000362 s\n",
            "\n",
            "2025-04-14 20:50:54 [INFO] Model compiled.\n",
            "2025-04-14 20:50:54 [INFO] Manually loading network weights from: /content/drive/MyDrive/PINN_RANS_ChannelFlow/model_checkpoints/rans_channel_wf-69000.pt\n",
            "2025-04-14 20:50:54 [INFO] Loading checkpoint onto device: cuda\n",
            "2025-04-14 20:50:55 [INFO] Model weights restored manually from step 69000.\n",
            "2025-04-14 20:50:55 [INFO] Proceeding to post-processing (plotting).\n",
            "2025-04-14 20:50:55 [INFO] Plotter initialized. Plots will be saved in: /content/drive/MyDrive/PINN_RANS_ChannelFlow/plots\n",
            "2025-04-14 20:50:55 [INFO] Reference CSV data path found: /content/drive/MyDrive/PINN_RANS_ChannelFlow/data/reference_output_data.csv\n",
            "2025-04-14 20:50:55 [INFO] --- Starting Full Post-Processing (Plotting Only) ---\n",
            "2025-04-14 20:50:55 [INFO] Loss history or train state not available (likely loaded model). Skipping loss plot.\n",
            "2025-04-14 20:50:55 [INFO] Predicting PINN flow fields on evaluation grid...\n",
            "2025-04-14 20:50:55 [INFO] PINN field prediction and processing complete.\n",
            "2025-04-14 20:50:55 [INFO] Generating PINN contour plots...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-73636d76ccef>:686: UserWarning: Log scale: values of z <= 0 have been masked\n",
            "  if use_log: cf = ax.contourf(self.X_grid, self.Y_grid, plot_values, levels=levels, cmap=cmap, extend='both', locator=plt.LogLocator())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-14 20:50:59 [INFO] PINN contour field plots saved to pinn_field_contours.png\n",
            "2025-04-14 20:50:59 [INFO] Loading reference data from: /content/drive/MyDrive/PINN_RANS_ChannelFlow/data/reference_output_data.csv\n",
            "2025-04-14 20:50:59 [INFO] Loaded reference data: 414060 rows, 12 cols.\n",
            "2025-04-14 20:50:59 [INFO] Filtered for latest time/step (Time=1000): 41406 rows remaining.\n",
            "2025-04-14 20:50:59 [INFO] Identified coordinate columns: x='Points:0', y='Points:1', z='Points:2'\n",
            "2025-04-14 20:50:59 [INFO] Filtered for z-plane near 0.0 (actual: 0.0000): 20703 rows remaining.\n",
            "2025-04-14 20:50:59 [INFO] Renamed reference columns based on mapping. New columns: ['TimeStep', 'Time', 'x', 'y', 'z', 'u_ref', 'v_ref', 'U:2', 'eps_ref', 'k_ref', 'nut_ref', 'p_ref']\n",
            "2025-04-14 20:50:59 [INFO] Successfully loaded and preprocessed reference CSV data. Final columns: ['y', 'u_ref', 'eps_ref', 'p_ref', 'k_ref', 'nut_ref', 'z', 'v_ref', 'x']\n",
            "2025-04-14 20:50:59 [INFO] Proceeding with PINN vs Reference CSV comparisons...\n",
            "2025-04-14 20:50:59 [INFO] Generating profile comparison plots...\n",
            "2025-04-14 20:50:59 [INFO] Interpolating ref data at x=7.990...\n",
            "2025-04-14 20:51:06 [INFO] Profile comparison plot saved to profile_comparison_pinn_vs_csv.png\n",
            "2025-04-14 20:51:06 [INFO] Generating wall unit comparison plots...\n",
            "2025-04-14 20:51:06 [INFO] Estimated u_tau (pinn) at x=8.00 m: 0.0878 m/s (avg of top/bottom grad estimates)\n",
            "2025-04-14 20:51:08 [INFO] Estimated u_tau (reference) at x=8.00 m: 0.0825 m/s (avg of top/bottom grad estimates)\n",
            "2025-04-14 20:51:08 [INFO] Processed 2912 reference points for wall units.\n",
            "2025-04-14 20:51:10 [INFO] Wall unit comparison plots saved to profile_comparison_wall_units.png\n",
            "2025-04-14 20:51:10 [INFO] Generating centerline pressure gradient comparison plot...\n",
            "2025-04-14 20:51:10 [INFO] Pressure gradient comparison plot saved to pressure_gradient_comparison.png\n",
            "2025-04-14 20:51:10 [INFO] --- Post-Processing Finished ---\n",
            "2025-04-14 20:51:10 [INFO] ============================================================\n",
            "2025-04-14 20:51:10 [INFO]  Script Execution (Plotting Only) Finished in 15.84 seconds\n",
            "2025-04-14 20:51:10 [INFO] ============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3psya9EDHtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46Kjz5EGDHrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZC0WKW4DHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4ejbnv8DHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6exLdkNhDHkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gooPhhRVDHiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}